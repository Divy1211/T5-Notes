{"config":{"lang":["en"],"separator":"[\\s\\-\\.\\_]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Term 5 Notes 2022","text":""},{"location":"courses/cse/deadlines/","title":"Important Deadlines","text":""},{"location":"courses/cse/to_know/","title":"Important Information","text":""},{"location":"courses/cse/notes/w1/res_alloc/","title":"Resource Allocation","text":""},{"location":"courses/cse/notes/w1/res_alloc/#what-is-an-os","title":"What is an OS?","text":"<p>An intermediate program between the users of a computer and its hardware. It does:</p> <ol> <li>Resource allocation and coordination: manages I/O requests, interrupts, hardware</li> <li>Controls program execution: storage hierarchy manager, process manager</li> <li>Limits program execution: prevents illegal access to hardware and improper hardware usage and ensures security</li> </ol> <p>An OS can be broadly divided into three parts:</p> <ol> <li>Kernel</li> <li>System programs</li> <li>User programs</li> </ol> <p></p>"},{"location":"courses/cse/notes/w1/res_alloc/#booting-nt","title":"Booting (NT)","text":"<ol> <li>Hardware initiated, user presses a physical button to load the BIOS (located in a dedicated ROM that comes built in to a computer)</li> <li>BIOS loads the Master Boot Record, which contains info about disk partitions and bootstrap code.</li> <li>Bootstrap code points to the bootloader, which is in the bootsector of the disk, which in turn points to the entry point of the OS code. If there are multiple OS, the bootloader asks the user to choose</li> <li>OS starts</li> </ol>"},{"location":"courses/cse/notes/w1/res_alloc/#io-controllers","title":"I/O Controllers","text":"<p>Each I/O device is managed by an autonomous hardware entity called the device controllers as shown in the figure below:</p> <p></p> <p>These controller devices are asynchronous and can execute instructions independently of the CPU</p> <p>The following components make up a device controller:</p> <ol> <li>Registers \u2014 contains instructions that can be read by an appropriate device driver program at the CPU</li> <li>Local memory buffer \u2014 contains instructions and data that will be fetched by the CPU when executing the device driver program, and ultimately loaded into the RAM.</li> <li>A simple program to communicate with the device driver</li> </ol> <p>An I/O operation happens data is transfered between the local memory buffer of the device controller and the device itself.</p> <ol> <li>Input: Device -&gt; Buffer -&gt; RAM</li> <li>Output: RAM -&gt; Buffer -&gt; Device</li> </ol>"},{"location":"courses/cse/notes/w1/res_alloc/#drivers","title":"Drivers","text":"<p>A system must have device drivers installed for each type of device. This is a special program to interpret the behaviour of each type of device so that the OS can meaningfully communicate with it.</p> <p>Device drivers can run in both kernel and user mode. Running in user mode is slower due to frequent sys calls, but safer if poorly written. Poorly written kernel mode drivers can wreak havoc on a computer.</p>"},{"location":"courses/cse/notes/w1/res_alloc/#the-kernel","title":"The Kernel","text":"<p>The kernel is the heart of the OS. It operates on the physical space - it has full knowledge of all PA instead of VA and has complete priviledge over all hardware of the computer system</p> <p>The only way to access the kernel code is when a process runs in kernel mode: controlled entry points. Hardware supported dual mode</p> <p>The kernel can</p> <ol> <li>Interrupt other user programs</li> <li>Receive and manage I/O requests</li> <li>Manage other user program locations on the RAM, the MMU, and schedule user program executions</li> </ol> <p>It consists of the following components:</p> <ol> <li>Vectored IRQ</li> <li>Prep Instructions</li> <li>Interrupt Handlers</li> <li>Scheduler (ProcTbl + Scheduler)</li> <li>Drivers</li> </ol> <p>The kernel performs the following tasks:</p>"},{"location":"courses/cse/notes/w1/res_alloc/#resource-allocation-and-coordination","title":"Resource Allocation and Coordination","text":"<p>The kernel controls and coordinates the use of hardware and I/O devices.</p>"},{"location":"courses/cse/notes/w1/res_alloc/#interrupt-driven-io-operation","title":"Interrupt-Driven I/O Operation","text":""},{"location":"courses/cse/notes/w1/res_alloc/#hardware-interrupt","title":"Hardware Interrupt","text":"<p>Triggered from I/O sources like mouse/keyboard</p> <p>IRQ -&gt; save registers to process table (for restoring) (the context) Transfer control to the appropriate interrupt service routine (JMP)</p> <p>The <code>Xaddr</code> in the beta can be muxed to directly provide the correct address of the correct I/O handler. The <code>X</code> is a variable object!</p>"},{"location":"courses/cse/notes/w1/res_alloc/#vectored-interrupt-system","title":"Vectored Interrupt System","text":"<p>In a VIS, the interrupt signal includes an address, which serves as an offset to a table called the interrupt vector. This table holds the addresses of the routines needed to process specific interrutps.</p> <p>This is useful when there are sparse I/O requests, but complex to implement</p>"},{"location":"courses/cse/notes/w1/res_alloc/#polled-interrupt-system","title":"Polled Interrupt System","text":"<p>CPU scans devices to see if an interrupt request was made. Unlike vectored interrupt, there is no signal including the identity of the device sending the signal. This is done at periodically/at a fixed interval.</p> <p>This is bad when there are sparse I/o requests, but simple to implement</p> <p>In case of multiple interrupts, the kernel scheduler decides which requests to service first.</p> <p></p>"},{"location":"courses/cse/notes/w1/res_alloc/#overview-of-an-io-interrupt","title":"Overview of an I/O Interrupt","text":"<ol> <li>I/O Device receives physical input</li> <li>Device controller triggers an IRQ</li> <li>CPU senses the IRQ and jumps to the interrupt handler</li> <li>Handler saves the current process' state and determines the source of the interrupt (vectored/polling) and performs the necessary handling</li> <li>This involves transferring the data from the device controller buffer to the RAM</li> <li>The handler clears the IRQ signal</li> <li>Restores the state of the process which it interrupted and performs a jump to the exception pointer</li> <li>They are async</li> </ol>"},{"location":"courses/cse/notes/w1/res_alloc/#software-interrupts-traps-sync","title":"Software Interrupts (Traps, Sync)","text":"<p>System calls and traps triggered by instructions themselves. Some software traps can be blocking. Lower prio than hardware interrupts</p> <p>An SVC and an IRQ can be combined:</p> <ol> <li>SVC made to load data from disk</li> <li>After some delay, the SVC is served and the SVC handler asks disk controller to load data and returns to the user process</li> <li>After more delay and the disk controller has loaded the file, it triggers an IRQ</li> <li>After some more delay, this IRQ is handled, and the loaded file is transferred from the disk buffers to the RAM</li> </ol>"},{"location":"courses/cse/notes/w1/res_alloc/#reentrancy","title":"Reentrancy","text":"<p>A re-entrant kernel allows multiple processes to make SVCs simultaneously without leadnig to consistency issues in the kernel data structures. A non re-entrant kernel only serves an interrupt if the current process is in user mode.</p> <p>An SVC is a voluntary interrupt</p> <p>Any processes making an SVC is therefore halted in a non-reentrant kernel if another process is already in kernel mode. Interrupts are disabled during the time a context switch or state saves or restores are being peformed</p>"},{"location":"courses/cse/notes/w1/res_alloc/#preemption","title":"Preemption","text":"<p>A pre-emptive kernel allows the scheduler to interrupt processes in kernel mode to execute the highest priority task first (strong preemptive scheduling policy)</p> <p>In a non preemtive kernel, a process cannot interrupt another process forcibly</p> <p>A re-entrant non pre-emptive kernel will not allow a process in KM to be interrupted!</p> <p>A non re-entrant pre-emptive kernel will allow a process in KM to be interrupted, but won't allow any other process to go into KM because the first process is blocking</p>"},{"location":"courses/cse/notes/w1/res_alloc/#timed-hardware-interrupts","title":"Timed hardware interrupts","text":"<p>A user process cannot be allowed to take control of the CPU indefinitely, thus a hardware scheduler invokes IRQ at set periods so that the OS (kernel) can take the control of the CPU back.</p> <p>There is a counter that is initially set by the kernel itself, and it is continuously decremented whenever the clock ticks, and when it reaches 0, an interrupt is triggered.</p>"},{"location":"courses/cse/notes/w1/res_alloc/#exceptions","title":"Exceptions","text":"<p>Exceptions are software interrupts. The CPU may use a pre built event vector table to handle exceptions (catch blocks?)</p> <p>Each exception has an ID and a vector address associated with its handler's entry point. They also have a prioerity (lower priority number = more important)</p>"},{"location":"courses/cse/notes/w10/net_sec/","title":"Network Security","text":""},{"location":"courses/cse/notes/w10/net_sec/#security-conditions","title":"Security Conditions","text":"<ol> <li>Confidentiality: only the sender - receiver know the contents, encryption</li> <li>Authentication: sender - receiver confirm the identity of each other</li> <li>Message Integrity: Ensuring that the message has not been altered in transit or afterwards</li> <li>Access &amp; Availability: Services must be accessible and available users</li> </ol>"},{"location":"courses/cse/notes/w10/net_sec/#what-can-a-malicious-person-do","title":"What can a malicious person do","text":"<ol> <li>Eavesdrop</li> <li>Insert/Delete/Modify Messages</li> <li>Impersonate (spoof/phishing)</li> <li>Hijacking: Taking over</li> <li>DoS</li> </ol>"},{"location":"courses/cse/notes/w10/net_sec/#encryption-algorithm","title":"Encryption Algorithm","text":"<ol> <li>Security by secrecy</li> <li>Security by obscurity</li> </ol> <p>Symmetric (both same)/Asymmetric (different enc/dec) keys</p> <ol> <li>Sub cipher, key: mapping</li> </ol>"},{"location":"courses/cse/notes/w10/net_sec/#data-encryption-standard-des","title":"Data Encryption Standard (DES)","text":"<ul> <li>56 bit symmetric key, 64 bit plaintext input (padding if needed)</li> <li>Block cipher w/ block cipher chaining</li> <li>DES 56 bit key encrypted phrase is decrypted in less than a day</li> <li>no known analytic attack</li> <li>DES more secure<ul> <li>3DES: encrypt 3 times with 3 different keys</li> </ul> </li> <li>Operation:<ul> <li>16 identical rounds</li> <li>choose 48 different bits of key Ki</li> <li>64 bit input - permute -&gt; L1 R1 -&gt; L2, R2 = f(L1, R1, K1) repeat ... L17, R17 - permute -&gt; 64 bit output</li> </ul> </li> </ul>"},{"location":"courses/cse/notes/w10/net_sec/#advanced-encryption-standard-aes","title":"Advanced Encryption Standard (AES)","text":"<ul> <li>128, 192 or 256 bit keys, 128 bit data blocks</li> <li>brute force that takes 1s on DES will take 149T years on AES</li> </ul>"},{"location":"courses/cse/notes/w10/net_sec/#symmetric-key-enc","title":"Symmetric Key Enc","text":"<ol> <li>Secret shared key</li> <li>Diffie Hellman key exchange</li> </ol>"},{"location":"courses/cse/notes/w10/net_sec/#public-key-asymmetric-enc","title":"Public Key (Asymmetric) Enc","text":"<ol> <li>Diffie Hellman76, RSA78</li> <li>No secret key shared</li> <li>Public enc key known to all</li> <li>Private (aka Secret) dec key known only to receiver</li> </ol> <p>How does it work?</p> <ol> <li>PK can enc SK can dec.</li> <li>You generate a pair and give out the PK</li> <li>People send you a message by enc with your PK so only you can dec with your SK</li> </ol>"},{"location":"courses/cse/notes/w10/net_sec/#rsa","title":"RSA","text":"<p>Generating a PK SK Pair:</p> <ol> <li>Two large primes = \\(p, q\\) (1024 bits)</li> <li>\\(n = pq\\) and \\(z = (p-1)(q-1)\\)</li> <li>pick \\(e\\) : \\(e &lt; n\\) and \\(gcd(e, z) = 1\\)</li> <li>pick \\(d\\) : \\(ed = 1 (\\mod z)\\)</li> <li>PK = \\((n, e)\\) &amp; SK = \\((n, d)\\)</li> <li>\\(c = m^e (\\mod n)\\)</li> <li>\\(m = c^d (\\mod n)\\)</li> <li>This works because \\((m^e (\\mod n))^d = m (mod n)\\). Why?</li> </ol> <p>This is secure if \\(n\\) is large because to figure out \\(d\\) from \\((n, e)\\) you need to find the prime factors of \\(n\\) to compute \\(z\\)</p> <ol> <li>PK(SK(m)) = SK(PK(m)) = m</li> </ol> <p>This means that if a sender encrypts a message with their SK, only their PK can decrypt it =&gt; authentication! Thus, a sender's SK acts as their Digital Signature</p> <p>RSA is slow, because the numbers involved are big, and exponentiation on big numbers is expensive. Thus, RSA is used to establish a shared (session key) Ks to be used with DES or another algo. Enter Diffie Hellman</p>"},{"location":"courses/cse/notes/w10/net_sec/#diffie-hellman-key-exchange","title":"Diffie Hellman Key Exchange","text":""},{"location":"courses/cse/notes/w10/net_sec/#message-digests-digital-fingerprints","title":"Message Digests, Digital Fingerprints","text":"<p>Computationally expensive to do an SK encryption of the entire message everytime a message is sent. Solution: Fixed length hashing (collisions possible but rare, also computationally infeasable to find an m such that x = H(x)). Sender can now send this encrypted hash value as a fingerprint.</p>"},{"location":"courses/cse/notes/w10/net_sec/#susceptibility","title":"Susceptibility:","text":"<p>Man in the middle attack. Solution: Certification Authorities. CAs are trusted 3<sup>rd</sup> parties which sign a sender's PK with their digital fingerprint (known to everyone by default, comes preinstalled/comes with browser). To get someone's PK, ask CA for their certificate and use the CA's PK to decrypt it!</p>"},{"location":"courses/cse/notes/w11/dns/","title":"Domain Name System","text":"<p>Translate from name to IPv4 (32 bits). Hierarchical, short prefixes are used only to decide the next hop - much smaller routing table</p> <p>Internet hosts, routers are identified by their 32 bit IPv4 address. Their names \"www.google.com\" are only used by humans.</p>"},{"location":"courses/cse/notes/w11/dns/#dns","title":"DNS","text":"<ol> <li>Distributed database of IPs - implemented in hierarchy of many name servers</li> <li>Application layer protocol - hosts, name servers communicate to resolve names (address/name translation). This is a core interenet function, implemented as app layer protocol. Complexity at network's edge</li> </ol>"},{"location":"courses/cse/notes/w11/dns/#functions-of-dns","title":"Functions of DNS:","text":"<ol> <li>name to IP (Protect domains, strong modularity, fault isolation. cf. processes), allows servers to move w/o affecting end users (late binding)</li> <li>Distributed (hierarchical) servers: scalable</li> <li>mail server aliasing</li> <li>host aliasing - many names corresp. one IP (many to one mapping)</li> <li>load distribution - replicated web services, many IPs corresp. to one name (one to many mapping)</li> <li>Cost of indirection: Delay, potential poisoning</li> </ol>"},{"location":"courses/cse/notes/w11/dns/#why-not-centralised","title":"Why not centralised?","text":"<ol> <li>DOES NOT SCALE</li> <li>Bottleneck</li> <li>Single pt of failure</li> <li>distant central database (long delay for lookup)</li> <li>Maintenance (modifying translations)</li> </ol>"},{"location":"courses/cse/notes/w11/dns/#hierarchy","title":"Hierarchy","text":"<ol> <li>Root DNS Server</li> <li>Top Level Domains, (TLD) <code>com</code>, <code>org</code>, <code>edu</code>, <code>net</code>... etc. and <code>sg</code>, <code>in</code>, <code>jp</code>... etc.</li> <li>Authoritative DNS: resolving names w/i the organisation.</li> </ol> <p>Local DNS name servers - One for each ISP. When hosts make DNS queries, it is sent to the local DNS server. It maintains a cache of name-address pairs, acts as proxy, queries are forwarded up the heirarchy</p> <p>query root -&gt; query one of these -&gt; get IP</p> <p>13 root name \"servers\" (logical, aka named auth). Worldwide, over 400 physical servers.</p> <ol> <li>Iterated queries: Queried server replies with \"IDK, but ask this server\"</li> <li>Recursive queries: Queried server must return a resolved name, so now IT needs to ask other servers for name-IP map.</li> </ol> <p>Qns:</p> <ul> <li>Who decides (client/server) iter/recur query?</li> <li>Are servers willing to support recur queries?</li> </ul> <p>Once a DNS server learns the name-IP map, it caches. TLD servers are often cached in local servers - root servers not visited. Caches disappear after some time (TTL). Who sets TTLs? If the IP changes, it won't be known internet wide until all TTLs expire</p> <p>Update/Notify mechanisms proposed IETF standards - RFC 2136</p>"},{"location":"courses/cse/notes/w11/dns/#dns-resource-records-rr","title":"DNS Resource Records (RR)","text":""},{"location":"courses/cse/notes/w11/dns/#format","title":"Format","text":"<p><code>&lt;name, val, type, ttl&gt;</code></p> <ol> <li>type = A, name - hostname, val - IP</li> <li>type = CNAME, name - alias for canonical name, val - canonical name</li> <li>type = NS, name - domain, val - hostname of auth name server for this domain</li> <li>type = MX, val - mail server associated with name</li> </ol> <p></p> <p></p>"},{"location":"courses/cse/notes/w11/dns/#inserting","title":"Inserting","text":"<ol> <li>Register name at DNS registrar, provide name, ip, auth name server (primary/secondary)</li> <li>Add two RRs to TLD, (NS (for auth DNS name server) and A type) and also a type MX RR.</li> </ol>"},{"location":"courses/cse/notes/w11/dns/#attacking","title":"Attacking","text":""},{"location":"courses/cse/notes/w11/dns/#ddos","title":"DDoS","text":"<ol> <li>bombard root w/traffic. Ineffective (filtering, local caches allow root bypass)</li> <li>bombard TLD. Potentially more dangerous</li> <li>Send queries with spoofed source address, targete IP. Requires amplification</li> </ol>"},{"location":"courses/cse/notes/w11/dns/#redirect","title":"Redirect","text":"<ol> <li>MitM</li> <li>DNS poisoning</li> </ol>"},{"location":"courses/cse/notes/w11/dns/#naming-application-endpoints","title":"Naming Application Endpoints","text":"<p>IP not enough to talk to a specific app on a machine.</p> <p>Is the IP and pid enough to communicate with a specific process? No, pid too volatile.</p> <ul> <li>Sockets: door between application process and e2e transport protocol</li> <li>Network access: I/O, sockets: special file descriptors</li> <li>IP + Port</li> <li>Mux at sender (add transport header to data from multiple sockets)</li> <li>Demux at receiver (use header to deliver correct data to different sockets)</li> </ul>"},{"location":"courses/cse/notes/w11/dns/#udp","title":"UDP","text":"<ul> <li>No handshake (no \"connection\")</li> <li>IP + Port on each packet</li> <li>Lossy/Out of order</li> <li>Unreliable, but real time</li> </ul>"},{"location":"courses/cse/notes/w11/dns/#tcp","title":"TCP","text":"<ul> <li>Socket: 4-tuple (source.ip, source.port, dest.ip, dest.port)</li> <li>Demux: All 4 values used to determine destination</li> <li>Concurrent TCP sockets</li> <li>Connection oriented</li> <li>handshake/setup, reliability, flow control, congestion control.</li> <li>Client contacts server, server must be listening for connections</li> <li>clients contact server by creating TCP socket specifying IP/Port of server. Client TCP establishes connection to server TCP</li> <li>When contacted by client, create new TCP socket to talk to client. Multiple clients support, source ip/port used to demux</li> <li>Reliable, in-order byte stream (pipe)</li> </ul>"},{"location":"courses/cse/notes/w11/dns/#web-and-http-tcp-servers","title":"Web and HTTP: TCP servers","text":"<p>HTTP - web's application layer protocol</p> <ul> <li>Client initiates TCP connection to server, server accepts</li> <li>HTTP messages exchanged b/n client and server</li> <li>HTTP is stateless. No need to maintain history/cleanup</li> <li> <p>Cookies can be added to keep state</p> </li> <li> <p>Non-persistent HTTP</p> <ul> <li>At most one object sent over TCP connection, connection then closed</li> <li>Multiple objects = multiple connections</li> <li>client: connection tcp init (syn)</li> <li>server: tcp connection accept, notify (syn + ack)</li> <li>client: request message (ack + piggy backed req)</li> <li>server: response message, close connection (ack + piggy backed res)</li> <li>client: receives response. repeat process for any sub-ref-ed objs</li> <li>Time: 2*RTT + file transmission time </li> </ul> </li> <li> <p>Persistent HTTP</p> <ul> <li>Server leaves connection open</li> <li>Multiple objects can be sent over one connection</li> </ul> </li> </ul>"},{"location":"courses/cse/notes/w2/mem_man/","title":"Memory Management","text":"<p>The kernel manages all the memory devices (RAM, Disk, Cache) so that they can be shared among many processes. The hierarchical storage structure requires a concrete form of memory management, since the same data may appear in different levels of the storage system</p>"},{"location":"courses/cse/notes/w2/mem_man/#virtual-memory","title":"Virtual Memory","text":"<p>The kernel implements virtual memory. It must support:</p> <ol> <li>Demand paging</li> <li>Keep track of which memory is used by what</li> <li>Decide which processes and data to move in and out of RAM</li> <li>Mapping files into a process address space</li> <li>Allocate and Deallocate memory as needed</li> <li>if RAM is full, start using swap space</li> <li>Manage the pagetable and any operations associated with it</li> </ol> <p>Note: CPU Caches are managed entirely by hardware (replacement, hit, miss, etc), however the Kernel may do some initial setup (policy)</p>"},{"location":"courses/cse/notes/w2/mem_man/#mmu","title":"MMU","text":"<p>The MMU is a hardware component that handles the translation of VA to PA. It uses the pagetable, which is set up by the kernel who also determines the rules for address mapping. The kernel only programs the MMU to perform VA -&gt; PA mapping</p>"},{"location":"courses/cse/notes/w2/mem_man/#cache-performance","title":"Cache performance","text":"<p>Hit ratio \\(\\color{yellow}\\alpha\\)</p> <p>Hit access time \\(\\color{yellow}\\tau\\)</p> <p>Miss access time \\(\\color{yellow}\\epsilon\\)</p> <p>Effective access time: \\(\\alpha \\tau + (1-\\alpha)\\epsilon\\)</p>"},{"location":"courses/cse/notes/w2/mem_man/#process-management","title":"Process Management","text":"<p>The kernel is also responsible for process management, multiprogramming and time sharing</p>"},{"location":"courses/cse/notes/w2/mem_man/#multiprogramming","title":"Multiprogramming","text":"<p>Multiprogramming means that the CPU is always being utilised by some process or the other. It is the kernel's job to ensure that the CPU never goes idle. It improves efficiency.</p> <p>The reason for need of multiprogramming is that:</p> <ol> <li>Single users must not hog the CPU/IO devices</li> <li>CLK speed of modern comps vroom (Ghz)</li> <li>Too fast for just one process</li> <li>CPU goes idle if only one process uses it (since the process may be I/O, or disk loading, etc.)</li> <li>A subset of the total jobs in the system is kept in the memory + swap space</li> <li>One job per CPU is selected to be run by the scheduler</li> <li>When a particular job has to wait, a context switch is performed</li> </ol>"},{"location":"courses/cse/notes/w2/mem_man/#timesharing","title":"Timesharing","text":"<p>Multiprocessing also allows for timesharing.</p> <p>Time sharing refers to context switches that are performed so fast that the users observe the system as interactive and seemingly capable of running a lot of processes simultaneously despite only having a limited number of CPUs.</p> <p>Timesharing is the logical extension of multiprogramming. It results in an interactive computer system, which provides direct communication between the user and the system</p>"},{"location":"courses/cse/notes/w2/mem_man/#process-vs-program","title":"Process vs Program","text":"<p>Process - active entity, has a context, state changing over time. Needs resources (CPU, RAM, I/O) to run.</p> <p>Program - passive entity, lines of instructions. Does not require resources</p> <p>The kernel allocates the resources required for the process when it is started, and also cleans them up after the process when it terminates.</p> <p>Kernel is not a process. There may be kernel threads. For instance, an I/O handler is a program that is written JUST to handle certain events.</p> <p>The kernel is special instructions that user processes jump to, via SVCs and Interrupts. It is part of every user process (piggybacks) and not one by itself!</p>"},{"location":"courses/cse/notes/w2/mem_man/#process-manager","title":"Process Manager","text":"<p>Part of kernel scheduler. Manages and keeps track of system wide process table.</p> <p>Responsible for:</p> <ol> <li>Creating/Terminating both user and system processes</li> <li>Pausing and resuming processes in the case of interrupts</li> <li>Sync-ing processes and providing interprocess communication</li> <li>Provides mechanisms to handle dead locks</li> </ol>"},{"location":"courses/cse/notes/w2/mem_man/#security-and-protection","title":"Security and Protection","text":"<p>The kernel provides a mechanism for controlling access of a process to resources defined by the OS</p> <ol> <li>User IDs</li> <li>Group IDs</li> </ol> <p>are associated with each process and file that determine its \"priviledge\" level. Priviledge Escalation is an event by which a user can change their ID to another effective ID with more rights.</p> <p>Kernel also provides defensive security measures to protect the computer against internal and external attacks, via firewall, encryption, etc. Network security issues</p>"},{"location":"courses/cse/notes/w3/os_struct/","title":"OS Structure","text":"<p>Aside from Kernel and UI, an OS also comes with system programs (utilities) and application programs</p>"},{"location":"courses/cse/notes/w3/os_struct/#system-programs","title":"System Programs","text":"<ol> <li>Basic tools for low level activities</li> <li>Generic, can be considered part of the system</li> </ol> <p>They run in user mode</p> <p>Following are common system programs:</p>"},{"location":"courses/cse/notes/w3/os_struct/#package-managers","title":"Package Managers","text":"<p><code>ls, rm, mkdir</code> etc: file management</p> <p><code>brew</code> - MacOS</p> <p><code>apt</code> - Linux</p> <p><code>nano</code> - file modification</p>"},{"location":"courses/cse/notes/w3/os_struct/#status-info","title":"Status Info","text":"<p><code>top, ls, df</code></p>"},{"location":"courses/cse/notes/w3/os_struct/#programming-language-support","title":"Programming Language Support","text":"<p>Compilers, Assemblers, Debuggers for common languages, and respective pkg managers <code>npm, pip</code></p>"},{"location":"courses/cse/notes/w3/os_struct/#program-loading-and-execution","title":"Program Loading and Execution","text":"<p>The system may provide absolute loaders, relocatable loaders, linkage editors, and overlay loaders</p>"},{"location":"courses/cse/notes/w3/os_struct/#communication","title":"Communication","text":"<p>Programs for virtual connections between processes, users and computers. <code>ssh, pip (|)</code></p>"},{"location":"courses/cse/notes/w3/os_struct/#background-services","title":"Background Services","text":"<p>launching certain system-program processes at boot time, network related system programs, device drivers.</p> <p>Constantly running system program processes are known as services, subsystems or daemon</p> <p>Examples of these:</p> <ol> <li><code>init</code> process (systemd: linux launchd: MacOS)</li> <li>Schedulers</li> <li>Error Monitoring</li> <li>Print Servers</li> </ol>"},{"location":"courses/cse/notes/w3/os_struct/#application-programs","title":"Application Programs","text":"<p>User programs:</p> <ol> <li>Specific user related tasks</li> <li>Installed on demand</li> <li>Require user interaction</li> </ol> <p>System programs:</p> <ol> <li>Used for operating hardware and common system usages</li> <li>Preinstalled with OS</li> <li>Run in the background, minimal to no user interaction</li> </ol>"},{"location":"courses/cse/notes/w3/os_struct/#user-and-system-goals","title":"User and System Goals","text":"<ol> <li>User goals: OS should be convenient to use, easy to learn, reliable, safe, fast</li> <li>System goals: The system should be easy to design, implement, and maintain; and it should be flexible, reliable, error free, and efficient.</li> </ol>"},{"location":"courses/cse/notes/w3/os_struct/#policy-and-mechanism-separation","title":"Policy and Mechanism Separation","text":"<ol> <li>Policy: determines what will be done</li> <li>Mechanism: determines how to do something</li> </ol> <p>The separation of policy and mechanism is important for flexibility:</p> <p>Policies are likely to change across places or over time. In the worst case, each change in policy would require a change in the underlying mechanism.</p> <p>A general mechanism that is insensitive to policy changes is better. A change in policy would just require tweaking the parameters of the mechanism.</p>"},{"location":"courses/cse/notes/w3/os_struct/#os-structures","title":"OS Structures","text":""},{"location":"courses/cse/notes/w3/os_struct/#monolithic-structure","title":"Monolithic Structure","text":"<p>A monolithic kernen is an OSA where the entire OS is working in the kernel space. It can operate with or without dual mode</p>"},{"location":"courses/cse/notes/w3/os_struct/#without-dual-mode","title":"Without dual mode","text":"<ol> <li>The interfaces and levels of functionality are not well separated (all programs can access hardware directly) The architecture (intel 8088) that MS-DOS was written for did not support dual mode</li> <li>Application programs were able to use basic I/O routines to write directly to the display and disk drivers</li> <li>Thus, vunerable to errant (or malicious) programs, causing entire system crashes when a user program fails</li> </ol> <p>MS-DOS:</p> <p></p>"},{"location":"courses/cse/notes/w3/os_struct/#with-dual-mode","title":"With dual mode","text":"<p>The early Unix OS was layered to a minimal extent with very simple structuring.</p> <p></p> <p>The kernel provides FS management, CPU scheduling, memory management, and other OS functions through SVCs, all crammed into one level.</p> <ol> <li>Pros: distinct performance advantage due to little overhead in SVC/communication within the kernel.</li> <li>Cons: difficult to implement and maintain.</li> </ol> <p>Other monolithic dual mode OS: BSD and Solaris</p>"},{"location":"courses/cse/notes/w3/os_struct/#layered-approach","title":"Layered Approach","text":"<p>OS broken into layers, layer 0 is hardware layer and layer N is user interface layer.</p> <p></p> <p>Pros:</p> <ol> <li>Simple to make and debug</li> <li>Each layer is implemented with only the lower layer's operations - maintainability, abstraction</li> </ol> <p>Cons:</p> <ol> <li>Appropriately defining the layers and planning</li> <li>Assume that the layer below our's works correctly</li> <li>Assume that the bug is in our code, not a compiler</li> <li>Sometimes, 2. and 3. are not true, specially with growing size of OS.</li> <li>Longer SVC times</li> </ol> <p>Eg: Windows NT</p>"},{"location":"courses/cse/notes/w3/os_struct/#microkernel","title":"Microkernel","text":"<p>very small kernel that provides minimal proc and mem man, in addition to communication functionality. The idea is to keep all non-essential code out of the kernel and implement them as system programs. Thus, we get a smaller kernel that only does:</p> <ol> <li>IPC</li> <li>Mem Man</li> <li>Scheduling</li> </ol> <p>Pros:</p> <ol> <li>Extending the OS is easier as all new services are added to the user space and do not require constant modification of the kernel.</li> </ol> <p>Cons:</p> <ol> <li>Overhead due to frequent context switches. Example: Mach, Windows NT (first release was a microkernel).</li> </ol>"},{"location":"courses/cse/notes/w3/os_struct/#hybrid-approach","title":"Hybrid Approach","text":"<p>Combine micro and monolithic kernels</p> <p>Example: MacOS</p> <ol> <li>Mach provides: IPC, scheduling, memory management</li> <li>BSD provides: CLI, file system management, networking support, POSIX APIs implementations    </li> </ol> <p></p>"},{"location":"courses/cse/notes/w3/os_struct/#java-operating-system-jx","title":"Java Operating System (JX)","text":"<p>The JX OS is written almost entirely in Java. Such a system is known as language-based extensible system and runs in single address space (no virtualisation, or MMU). Thus it has difficulty maintaining memory protection</p> <p>Language-based systems instead rely on type-safety features of the language. As a result, language-based systems are desirable on small hardware devices, which may lack hardware features that provide memory protection.</p> <p>Since Java is a type-safe language, JX is able to provide isolation between running Java applications without hardware memory protection. This is called language based protection, where system calls and IPC in JX does not require an address-space switch.</p> <p>JX organises its system according to domains:</p> <p></p> <p>Each domain is its own JVM</p> <ol> <li>An abstract VM that can run on any OS</li> <li>One JVM instance per application</li> <li>Portable execution env. for java based apps</li> <li>It maintains a heap used for allocating memory during object creation and threads within itself, as well as for garbage collection.</li> </ol> <p>Domain zero is a microkernel responsible for low-level details, such as system initialisation and saving and restoring the state of the CPU. Written in C and assembly language. All other domains are written entirely in Java. Communication between domains occurs through a specific mechanism called portals. Protection within and between domains relies on the type safety of the Java language. However, since domain zero is not written in Java, it must be considered trusted (built by trusted sources)</p>"},{"location":"courses/cse/notes/w3/services/","title":"OS Services","text":"<p>The OS provides services for user programs. The goal of an OS is to allow the users of a computer to use it in a more efficient manner</p>"},{"location":"courses/cse/notes/w3/services/#basic-support","title":"Basic Support","text":"<ol> <li>Program Execution: load into memory and run the program on request. Program can end its own execution normally or via an error</li> <li>I/O Operations:</li> <li>File System: read/write/create/delete/search for files and directories, see file's meta info. Permission management</li> <li>Interprocess Communication: Shared Memory/Message Passing - packets moved between process by the OS. This includes a communication protocol to connect to the internet where processes in different physical computers can communicate</li> <li>Error Detection: The OS should be aware of potential errors. CPU errors, memory errors (power failures, etc.) in I/O devices. For each error, the OS must know how to appropriately deal with it.</li> </ol>"},{"location":"courses/cse/notes/w3/services/#sharing-resources","title":"Sharing Resources","text":"<p>Diagnostics reports and computer sharing feature:</p> <ol> <li>Resource sharing: Multiple users or multiple jobs running at the same time. Resources handled: CPU, memory, file storage, I/O</li> <li>Resource accounting: Record keeping that can be used for accounting (billing users for usage) or accumulating usage stats</li> </ol>"},{"location":"courses/cse/notes/w3/services/#network-security","title":"Network Security","text":"<p>Protection and security against external threads</p> <ol> <li>Access to system resources is controlled</li> <li>Defend against invalid requests coming from external hardware/IO devices</li> <li>Record network traffic and connections for detecting break ins</li> </ol>"},{"location":"courses/cse/notes/w3/services/#ui","title":"UI","text":"<ol> <li>GUI or CLI</li> <li>Write instructions and make SVC</li> </ol>"},{"location":"courses/cse/notes/w3/services/#gui","title":"GUI","text":"<p>The GUI or Desktop Env is what we usually call home screen or desktop. It characterises the look and feel of the OS.</p>"},{"location":"courses/cse/notes/w3/services/#cli","title":"CLI","text":"<p>Command Line Interpreter is what we usually call the terminal or command prompt. The user issues successive commands to the program in the form of text. Commands in a shell can be built in or invocations for system programs</p>"},{"location":"courses/cse/notes/w3/svcs/","title":"System Calls","text":"<p>System calls are programming interfaces provided by the OS Kernel for users to access kernel services</p> <p></p> <p>Kernel is divided into two spaces, logical and virtual (aka lowmem and vmalloc). In <code>lowmem</code>, the VA are mapped to PA with just an offset (<code>X+C</code>). This mapping is determined during a boot, and never changes. The kernel virtual address area (<code>vmalloc</code>) is used for non contiguous PA so that it is easier to allocate them</p> <ol> <li>dynamic, on demand</li> <li>On each allocation, a series of locations of PP are found for the corresponding kernel VP, and the pagetable is modified to create the mapping</li> <li>This makes it unsuitable for DMA</li> </ol> <p>API is an interface that provides a way to interact with the underlying library2 that makes the system calls, often named the same as the system calls they invoke.</p> <p>Parameters to syscalls can be passed through:</p> <ol> <li>Registers, (fast, limited by # of regs).</li> <li>Stack (slower, not limited by # of regs)</li> <li>Block: pointer to physical contiguous block of memory with all the args (same as stack)</li> </ol>"},{"location":"courses/cse/notes/w3/svcs/#types-of-system-calls","title":"Types of System Calls","text":"<ol> <li>Process Control: load, create, start, pause, terminate processes, get and set process attributes, wait for time, wait for event, signal event, aloocate and free memory</li> <li>File Manipulation: create, delete, rename, open, close, read, write, copy, move</li> <li>Device Manipulation: request and release device, read, write, move device, get and set device attributes, logically attach or detach attributes</li> <li>Information Maintenance: get or set time and date, system data, process, file, device attributes</li> <li>Communication: create/delete pipes, send/receive packets through network, transfer status information, attach/detach remote devices</li> <li>Protection: set network enc protocs</li> </ol>"},{"location":"courses/cse/notes/w3/svcs/#blocking-vs-non-blocking-calls","title":"Blocking vs Non Blocking Calls","text":"<p>A system call is blocking if the callee process must wait for the system call to return before it can continue its execution</p> <p><code>input()</code> is a blocking system call. If no input is ready when the call is made, then the process <code>yields</code> and other processes are scheduled.</p> <p>A non blocking system call is one which can return immediately without completing I/O (<code>async_load</code>)</p>"},{"location":"courses/cse/notes/w3/svcs/#single-tasking-system","title":"Single Tasking System","text":"<p>MS-DOS</p> <ol> <li>Overwrites itself</li> <li>Runs program</li> <li>Rewrites itself from disk</li> </ol>"},{"location":"courses/cse/notes/w3/svcs/#multi-tasking-system","title":"Multi Tasking System","text":"<p>FreeBSD</p> <ol> <li>Uses <code>fork()</code> and <code>exec()</code> syscalls</li> <li>Kernel resp for context switching and timesharing</li> </ol> <p></p>"},{"location":"courses/cse/notes/w4/ipc/","title":"InterProcess Communication","text":"<p>Processes need to cooperate for the following reasons:</p> <ol> <li>Information sharing</li> <li>Speeding up communication</li> <li>Modularity and convenience</li> </ol> <p>Thus, IPC mechanisms are supported by the kernel:</p> <ol> <li>Shared memory</li> <li>Message passing (sockets)</li> </ol>"},{"location":"courses/cse/notes/w4/ipc/#shared-memory","title":"Shared Memory","text":"<p>Created using a system call. The kernel allocates and establishes a region in the memory and returns it to the calling process. Once established, all accesses to it are treated like normal user memory access and no assistance from the kernel is required.</p> <p>Using the <code>shmget</code> system call. <code>SHM_KEY</code> is a unique integer that must be known to both the processes in order to use the same shared memory.</p> <pre><code>int shmid = shmget(SHM_KEY, 1024, 0666 | IPC_CREAT);\n</code></pre> <p>Using <code>shmat</code> attaches the shared memory into the process' own address space:</p> <pre><code>void* str = shmat(shmid, NULL, 0);\n</code></pre> <p></p> <p>The shared memory can be detached from the virtual address space of the process using:</p> <pre><code>shmdt(str);\n</code></pre> <p>Finally, the shared memory control function can be used to free/destroy the memory. This must be done by the last process reading from it.</p> <pre><code>shmctl(shmid, IPC_RMID, NULL);\n</code></pre> <p>synchronisation issues.</p>"},{"location":"courses/cse/notes/w4/ipc/#message-passing","title":"Message Passing","text":"<p>Message passing allows two processes to communicate synchronously, usnig svcs.</p>"},{"location":"courses/cse/notes/w4/ipc/#socket","title":"Socket","text":"<p>A socket is one endpoint of a two-way communication link between two programs running on the network with the help of the kernel:</p> <ol> <li>It is a concatenation of an IP address, e.g: 127.0.0.1 for localhost</li> <li> <p>And TCP (connection-oriented) or UDP (connectionless) port, e.g: 8080.</p> </li> <li> <p>When concatenated together, they form a socket, e.g: 127.0.0.1:8080. All socket connections between two communicating processes must be unique!</p> </li> </ol> <p>When on the same machine, two processes can communicate using the IP <code>localhost</code> and an  unused port number. <code>read()</code> and <code>write()</code> data using svc</p> <p></p> <p></p> <p>One process uses the <code>write()</code> svc to copy data from its own space to kernel space, then the other process uses the <code>read()</code> svc to copy the data from kernel space into its space</p>"},{"location":"courses/cse/notes/w4/ipc/#message-queue","title":"Message Queue","text":"<p>Another interface for message passing. Uses svcs: <code>ftok</code>, <code>msgget()</code>, <code>msgsnd()</code>, <code>msgrcv()</code> (can be both blocking/non blocking)</p> <p></p> <p>Kernel maintains the message queue - any process can read from/write to it</p>"},{"location":"courses/cse/notes/w4/ipc/#differences","title":"Differences:","text":"Message Passing Shared Memory One svc per msg, quicker for low # of msgs <code>shmget</code> and <code>shmat</code> are expensive svcs but subsequent uses of <code>shm</code> are fast 1-1 Many processes can use the same block of shared memory Faster for smaller amounts of data (svc overhead) Faster for larger amounts of data (<code>shmget</code> and <code>shmat</code> overhead) No synchronisaiton needed (thx kernel) Synchronisation needed (need dev) Nothing needs to be freed Needs to be freed or will exist even after process terminates"},{"location":"courses/cse/notes/w4/proc_ops/","title":"Process Operations","text":"<p>We can perform the following operations on processes:</p> <ol> <li>spawning child processes</li> <li>terminate the process</li> <li>set up inter-process communication channels</li> <li>change the process priority</li> </ol> <p>And many more. All of these operations require an SVC.</p>"},{"location":"courses/cse/notes/w4/proc_ops/#process-creation","title":"Process Creation","text":"<p>the <code>fork()</code> SVC can be used to create a child process from a parent process. Each child may furhter create more chilren processes, creating a process tree:</p> <p></p> <p>Process ID: each process is identified by a unique (on the system) process ID</p> <p><code>ps</code> on linux shows a list of processes with their pid.</p>"},{"location":"courses/cse/notes/w4/proc_ops/#child-vs-parent-process","title":"Child vs Parent Process","text":"<p>The new child process creates a duplicate of its parent's address space at the exact time the fork occurs. Parent and child processes execute in different address spaces (in isolation).</p> <p><code>fork()</code> returns 0 in the child process, but returns the child's process ID in the parent process. This can be used to make separate executions for the two processes.</p> <p><code>execlp()</code> SVC can be used to replace the address space of one process with another (basically start a new process in its place)</p> <p>the <code>wait()</code> or <code>waitpid()</code> SVC can be used by a parent process to wait (blocking system call) for the child processes to finish execution. <code>wait()</code> blocks the parent process until one of its children processes have exited. It returns a <code>&lt; 0</code> value if all child processes have exited</p> <p>Once the child has finished execution and the parent process resumes, we say that the child has been reaped. It is necessary for every child process to be reaped by the parent using <code>wait()</code> for its PCB and scheduler releated resources to be cleaned up. (RAM is cleaned when child process exits either way)</p> <p><code>kill(pid, SIGKILL)</code> SVC kills a process, or it may terminate itself by <code>exit()</code></p>"},{"location":"courses/cse/notes/w4/proc_ops/#orphaned-processes","title":"Orphaned Processes","text":"<p>Active child processes whoes parent process has been terminated. These can either be:</p> <ol> <li>Terminated</li> <li>Adopted by the <code>init/launchd</code> process</li> </ol> <p>The <code>init</code> process on linux is one of the two first processes to run.</p>"},{"location":"courses/cse/notes/w4/proc_ops/#zombie-processes","title":"Zombie Processes","text":"<p>Terminated child processes who were not reaped by their parent process. The RAM and other resources of these processes are freed by the OS, but not the PCB entry. Zombie processes are terminated by the OS if their parent process dies.</p>"},{"location":"courses/cse/notes/w4/proc_scd/","title":"Process Scheduling","text":"<p>A process is an active-dynamic entity, it changes its state overtime, which a program is a static entity</p>"},{"location":"courses/cse/notes/w4/proc_scd/#context","title":"Context","text":"<ol> <li>Contents of all the process' registers</li> <li>PC</li> <li>instruction (text section)</li> <li>Dedicated address space</li> <li>stack</li> <li>data</li> <li>heap</li> </ol> <p>A program becomes a process when instructions from an executable are loaded into memory. Concurrency and protection</p>"},{"location":"courses/cse/notes/w4/proc_scd/#process-scheduling-states","title":"Process Scheduling States","text":"<ol> <li>New: The process is being created</li> <li>Running: The process is running</li> <li>Waiting: The process is waiting for some event to occur (I/O)</li> <li>Ready: The process is ready to be assigned to a processor to start execution</li> <li>Terminated: The process has finished execution</li> </ol>"},{"location":"courses/cse/notes/w4/proc_scd/#process-table-and-process-control-block","title":"Process Table and Process Control Block","text":"<p>The system wide process table is a data struct that is maintained by the kernel to facilitate context switching and scheduling. Each process' metadata is stored by the kernel in a Process Control Block (PCB, aka Task Control Block). A process table is made of an array of PCBs, containing info about the current processes in the system</p> <p>The PCB contains data such as:</p> <ol> <li>Process state</li> <li>PC</li> <li>Regs</li> <li>Scheduling Info: priority, pointers to scheduling queues and any other scheduling programs</li> <li>Memory Management Info: Page tables, MMU-related info, memory limits</li> <li>Accouting Info: CPU and real time used, time limits, account numbers, process id (pid)</li> <li>I/O status Info: The list of I/O devices allocated to the process, a list of open files</li> </ol> <p>In the linux kernel, the proctable is a doubly linked list whoes nodes are made up of <code>task_struct</code>s implemented in C. The kernel maintains a <code>current_pointer</code> to the process currently running in the CPU</p>"},{"location":"courses/cse/notes/w4/proc_scd/#context-switching","title":"Context Switching","text":"<p>When a CPU switches execution between processes, the kernel has to store all the process' metadata (the updated <code>task_struct</code>) in the corresponding PCB and then load the new process' information from its PCB</p> <p>Context switch: The mechanism of saving the states of the current process and restoring (loading) the state of a different process when switching the CPU to execute another process.</p> <p>The advantages of rapid context switching is that it gives the illusion of concurrency in a single processor system</p> <ol> <li>Improved responsiveness, multiple apps at ocne</li> <li>Support for multiprogramming: optimise CPU usage, we cannot allow one single process to hog the CPU specially if the process blocks execution when waiting for I/O calls.</li> </ol> <p>The drawbacks of context switches is pure overhead related to saving and restoring a context. Context switch times are highly dependent on hardware support, some hardware support rapid context switching out of the box, and circumvent the CPU entirely by having a dedicated unit for it.</p>"},{"location":"courses/cse/notes/w4/proc_scd/#mode-switch","title":"Mode Switch","text":"<ol> <li>The privilege of a process changes. Simply escalates privilege from user mode to kernel mode to access kernel services.</li> <li>Done by either: hardware interrupts, system calls (traps, software interrupt), exception, or reset</li> <li>Mode switch may not always lead to context switch. Depending on implementation, Kernel code decides whether or not it is necessary.</li> </ol>"},{"location":"courses/cse/notes/w4/proc_scd/#process-scheduling-queues","title":"Process Scheduling Queues","text":"<p>There are several scheduling queues that are maintained by the kernel scheduler:</p> <ol> <li>Job Queue: Set of all processes in the system (may be in swap space or RAM)</li> <li>Ready Queue: Set of all processes residing in the main memory which are ready to be executed by the CPU (queuing for CPU time)</li> <li>Device Queues: Set of processes that are waiting for some form of I/O from a device. One queue is maintained per device</li> </ol> <p></p> <p></p>"},{"location":"courses/cse/notes/w4/proc_scd/#long-term-and-short-term-schedulers","title":"Long term and Short term Schedulers","text":""},{"location":"courses/cse/notes/w4/threads/","title":"Threads","text":"<p>A thread is defined as a segment of a process.</p> <p>A process can have multiple threads, and we can define thread as a basic unit of CPU utilization; it comprises of:</p> <ol> <li>Thread ID,</li> <li>PC</li> <li>Registers</li> <li>Stack</li> </ol> <p>Since threads of one process operate in the same address space, heaps are shared.</p>"},{"location":"courses/cse/notes/w4/threads/#benefits-of-mt","title":"Benefits of MT","text":"<ol> <li>Responsiveness. If one thread of the program is blocked, other threads can continue to provide user interaction.</li> <li>Multiprocessor architecture =&gt; parallel programming</li> <li>Easy resource sharing and communication</li> <li>Cheaper than multiprocessing (context switch between threads is more light-weight)</li> </ol> Multiprocessing Multithreading concurrency and protection Only concurrency IPC is expensive (need svc and context switch) Thread switching cheaper since same addr space parallel process exec is always available on a multicore architecture Parallel thread execution depends on the type of threads and may not be available synchronisaiton by svc and kernel Synchronisation by dev Managed by the kernel scheduler (full ctxs + svc + flush mmu) Managed by thread scheduler (ctxs light, only regs and stack, may not need svc)"},{"location":"courses/cse/notes/w4/threads/#types-of-threads","title":"Types of threads","text":""},{"location":"courses/cse/notes/w4/threads/#user-level-thread","title":"User Level Thread","text":"<p>green threads</p> <ol> <li>scheduled by runtime libs or venv, not kernel. Kernel is unaware of their existance!</li> <li>managed in user space, cheaper to (de)alloc</li> <li>can run on any system, take up thread data structure</li> <li>simple management done in user space, no svc, efficient</li> <li>scheduling may be bad</li> <li>what is used in Java/C (pthreads)</li> </ol>"},{"location":"courses/cse/notes/w4/threads/#kernel-level-thread","title":"Kernel Level Thread","text":"<ol> <li>scheduled by kernel, shares data with kernel but has own stack/regs. lightweight proc</li> <li>managed in kernel space, take up more res to (de)alloc</li> <li>need kernel support, take up kernel data structure</li> <li>significant overhead and kernel complexity, ctxs as expensive as svc.</li> <li>scheduling is efficiently done by kernel itself</li> <li>may not be associated with a processes</li> <li>implement bkg tasks in kerel, (async handling or waiting for event)</li> <li>kernel can use them to service multiple SVCs concurrently</li> <li>Need a TCB</li> </ol>"},{"location":"courses/cse/notes/w4/threads/#thread-mapping","title":"Thread Mapping","text":"<ol> <li>kernel level thread: virtual processor</li> <li>user level thread: thread</li> </ol>"},{"location":"courses/cse/notes/w4/threads/#many-to-one","title":"Many to One","text":"<p>Many user threads are mapped to one kernel thread</p> <p>Pros:</p> <ol> <li>Thread man done by lib in user mode =&gt; efficient</li> <li>As many threads as devs want</li> </ol> <p>Cons:</p> <ol> <li>EVERY user thread is blocked if ANY thread makes a blocking svc since kernel is unaware of user level threads</li> <li>Multiple threads unable to access the kernel at the same time, not actually concurrent even on multicore architectures</li> </ol>"},{"location":"courses/cse/notes/w4/threads/#one-to-one","title":"One to One","text":"<p>One user thread mapped to one kernel thread</p> <p>Pros:</p> <ol> <li>Blocking svc on one thread doesn't affect the others</li> <li>True concurrency</li> </ol> <p>Cons:</p> <ol> <li>Overhead related to creating a kernel thread</li> <li>Number of threads limited</li> </ol>"},{"location":"courses/cse/notes/w4/threads/#many-to-many","title":"Many to Many","text":"<p>Best of both worlds, multiplexed many user level threads to equal or lesser number of kernel threads.</p> <ol> <li>Users can create as many threads as they like</li> <li>As many concurrent threads as virtual cores</li> </ol> <p>A variation of many to many is a two level model, where some threads may be mapped to one-one with kernel threads. Kernel may allocate more kernel threads to a single process on demand.</p>"},{"location":"courses/cse/notes/w4/threads/#hyper-threading","title":"Hyper Threading","text":"<ol> <li>A single CPU appears as two (or more) logical CPUs to the kernel.</li> <li>physical cores split into multiple virtual cores</li> <li>increases efficiency as the kernel assumes that there are two independent CPUs for utilisation</li> <li> <p>The idea is similar to pipelining/context switching on the hardware level</p> </li> <li> <p>Data parallelism: Perform the same operation on different cores on subsets of the same dataset.</p> </li> <li>Task parallelism: Delegation, may or may not be operating on the same dataset</li> </ol>"},{"location":"courses/cse/notes/w4/threads/#amdahls-law","title":"Amdahl's Law","text":"<p>if \\(\\alpha\\) is the fraction of the program that must be executed serially, then the maximum speedup that can be gained when running this program with \\(N\\) processors is:</p> \\[\\cfrac{1}{\\alpha + \\cfrac{1-\\alpha}{N}}\\]"},{"location":"courses/cse/notes/w4/threads/#appendix","title":"Appendix","text":""},{"location":"courses/cse/notes/w4/threads/#daemons","title":"Daemons","text":"<ul> <li>Bkg process that performs a specific function or system task.</li> <li>User mode.</li> <li> <p>System programs that are kept out of the kernel</p> </li> <li> <p>Generally persistent</p> </li> <li>No controlling terminal, (controlling <code>tty</code> process group (<code>tpgid</code>) <code>-1</code>)</li> <li>Parent init</li> <li>Own process group <code>id</code> and session <code>id</code></li> </ul>"},{"location":"courses/cse/notes/w4/threads/#linux-startup","title":"Linux Startup","text":"<ol> <li>BIOS -&gt; Bootloader -&gt; OS -&gt; setup system functions that are crucial for hardware and memory paging, perform the majority of system setups pertaining to interrupts, memory management, device, and driver initialization</li> <li><code>idle</code> and <code>init</code> user processes, <code>init</code> starts more daemons</li> <li>Display Manager (explorer.exe) is a daemon -&gt; Lock screen -&gt; session (set of programs, UI elements, etc which form a complete desktop env)</li> </ol>"},{"location":"courses/cse/notes/w5/deadlock/","title":"Deadlocks","text":""},{"location":"courses/cse/notes/w5/deadlock/#necessary-conditions-for-deadlock","title":"Necessary conditions for deadlock","text":"<p>When these conditions are fulfilled, there is a possibility of deadlocks. If any one is violated, deadlock cannot happen</p> <ol> <li>Mutex</li> <li>Hold and wait</li> <li>No pre-emption: a process cannot be forced to release a resource</li> <li>Circular wait: A cycle in the resource allocation graph</li> </ol>"},{"location":"courses/cse/notes/w5/deadlock/#resource-allocation-graph","title":"Resource Allocation Graph","text":"<p>Table:</p> <p></p> <p></p> <p>If no cycle, no deadlock</p> <p>If cycle, and cycle has a resource with single instance, deadlock</p> <p>If cycle, but all res have multiple instances then deadlock may happen. Need to analyse</p>"},{"location":"courses/cse/notes/w5/deadlock/#deadlock-prevention","title":"Deadlock Prevention","text":""},{"location":"courses/cse/notes/w5/deadlock/#mutex-prevention","title":"Mutex prevention","text":"<p>Not possible for certain types of res</p>"},{"location":"courses/cse/notes/w5/deadlock/#hold-and-wait-prevention","title":"Hold and wait prevention","text":"<p>Don't hold on to a resource while requesting/waiting on another resource</p> <p>Protocol 1: acquire everything before starting execution (suffers from idle utilisation) Protocol 2: Only request for new resources when no resources are already acquired (suffers from acquire/release overhead)</p>"},{"location":"courses/cse/notes/w5/deadlock/#allowed-pre-emption","title":"Allowed Pre-emption","text":"<p>This is commonly used with resources whose states can be easily saved/restored, like CPU regs.</p> <p>Protocol 1: forcefully release and acquire resources held by a process implicitly Protocol 2: release and acquire resources only if the process which holds them is waiting</p>"},{"location":"courses/cse/notes/w5/deadlock/#circular-wait-prevention","title":"Circular Wait Prevention","text":"<p>Ensure that the resources are requested in a particular order. If not, earlier acquired resources must be released first.</p>"},{"location":"courses/cse/notes/w5/deadlock/#deadlock-avoidance","title":"Deadlock Avoidance","text":""},{"location":"courses/cse/notes/w5/deadlock/#bankers-algorithm","title":"Banker's Algorithm","text":"<p>let there me <code>M</code> resources and <code>N</code> processes</p> <ol> <li>Saftey Algorithm</li> <li>Resource Allocation Algorithm</li> </ol> <p>State variables:</p> <ol> <li><code>available[i]</code> # of res <code>i</code> available.</li> <li><code>max[i][j]</code> max # of simultaneous requests for resource <code>j</code> by proc <code>i</code></li> <li><code>allocation[i][j]</code> current # of resource <code>j</code> alloc-ed to proc <code>i</code></li> <li><code>need[i][j]</code> how many more # of resource <code>j</code> are needed by proc <code>i</code></li> </ol> <p><code>max[i][j] = need[i][j] + allocation[i][j]</code></p>"},{"location":"courses/cse/notes/w5/deadlock/#resource-allocation-algorithm","title":"Resource Allocation Algorithm","text":"<p>returns: boolean to indicate request accepted or rejected</p> <ol> <li>if <code>any(request[i] &gt; need[i])</code> return <code>False</code> (reject)</li> <li>if <code>any(request[i] &gt; available[i])</code> return <code>False</code></li> <li>deepcopy <code>available</code>, <code>allocation</code>, <code>need</code> and pass to saftey check algo.</li> <li>If the outcome of the saftey check is<ol> <li><code>True</code> then<ol> <li><code>available -= request[i]</code></li> <li><code>need[i] -= request[i]</code></li> <li><code>allocation[i] += request[i]</code></li> </ol> </li> <li><code>False</code> then</li> <li>Try again</li> </ol> </li> </ol>"},{"location":"courses/cse/notes/w5/deadlock/#safety-algorithm","title":"Safety Algorithm","text":"<ol> <li>create an array <code>finish = [False]*N</code>.</li> <li>Hypothetically grant the request:<ol> <li><code>available -= request[i]</code></li> <li><code>need[i] -= request[i]</code></li> <li><code>allocation[i] += request[i]</code></li> </ol> </li> <li>find a process <code>i</code> for which <code>!finish[i]</code></li> <li>if <code>all(need[i] &lt; available[i])</code> (proc can finish even after req granted)<ol> <li><code>available += allocation[i]</code></li> <li><code>finish[i] = True</code></li> <li>repeat</li> </ol> </li> <li>if <code>all(finish)</code> return <code>True</code> else <code>False</code></li> </ol> <p>\\(\\mathcal{O}(MN^2)\\) time complexity</p>"},{"location":"courses/cse/notes/w5/deadlock/#deadlock-detection","title":"Deadlock Detection","text":"<p>Same as safety algorithm but performed on actual system state, not on a hypothetical state.</p> <p>How often to check for deadlock?</p> <ol> <li>depends on how often it can occur and</li> <li>how many procs can be affected by it. it's an expensive algo!</li> </ol> <p>Resolution options:</p> <ol> <li>Release all held resources from all deadlocked processes</li> <li>Release all held resources from deadlocked processes one at a time.</li> </ol> <p>Questions:</p> <ol> <li>Order of pre-emption</li> <li>Cost. Which process will have longest rollback?</li> <li>Starvation should not occur</li> </ol>"},{"location":"courses/cse/notes/w5/java_sync/","title":"Java Sync","text":"<p>synchronised methods and statements.</p> <p>Every java object has an associated binary lock.</p> <p>Lock is acquired/released by invoking/exiting a synchronised method/block.</p> <p>Synchronised member functions block all other synchronised methods from execution as well, but only for that particular instance. Other instances may call the same method without issues.</p> <p>A call to a static synchronised function blocks other static synchronised functions only.</p> <p>Threads waiting to acquire a lock wait in the entry set. Threads listening for an event wait in the wait set.</p> <p>Every java object has a wait and entry set.</p> <p><code>wait();</code> and <code>notify();</code></p> <pre><code>public synchronized void doWork(int id) {\n   while (turn != id) { // turn is a shared variable\n       try {\n           wait(); \n       } catch (InterruptedException e) {}\n   }\n   // CRITICAL SECTION\n   // ...  \n   turn = (turn + 1) % N;\n   notify();\n}\n</code></pre> <p>when calling <code>wait();</code> the mutex must be held by the caller. This is to ensure a TOCTOU condition doesn't happen.</p> <p><code>notify();</code> picks an arbitrary threat in the wait set and moves it into the entry set. The state of the thread is changed from locked to runnable.</p> <p><code>notify();</code> might not wake up the right thread whose <code>turn == id</code>. We can thus use <code>notifyAll();</code></p> <p>it is important to call <code>wait();</code> in a loop because</p> <ol> <li>Suprious wakeup: without signal</li> <li>Extraneous wakeup: when <code>turn != id</code> for you</li> </ol> <p>A re-entrant lock is a lock that can re-acquire itself.</p> <p>A non re-entrant will suffer from re-entrance lockout if we use recursion on it.</p> <pre><code>// Create arrays of condition\nLock lock = new ReentrantLock();\nArrayList&lt;Condition&gt; condVars = new ArrayList&lt;Condition&gt;();\nfor(int i = 0; i&lt;N; i++) condVars.add(lock.newCondition()); // create N conditions, one for each Thread\n</code></pre> <pre><code>// the function\npublic void doWork(int id)\n{\n   lock.lock();\n   while (turn != myNumber)\n   {\n       try\n       {\n             condVars[id].await(); \n    }\n       catch (InterruptedException e){}\n   }\n   // CS\n   // assume there's some work to be done here...\n   turn = (turn + 1) % 5;\n   condVars[turn].signal(); \n   lock.unlock();\n}\n</code></pre>"},{"location":"courses/cse/notes/w5/race/","title":"Race Conditions","text":""},{"location":"courses/cse/notes/w5/race/#producer-consumer-problem","title":"Producer Consumer Problem","text":"<p>let <code>counter</code> and <code>buffer</code> of size <code>N</code> be shared variables between the producer and the consumer. <code>counter</code> keeps track how many items are in the <code>buffer</code>.</p> <pre><code>while (true) {\n    /* produce an item in next produced */\n    while (counter == BUFFERSIZE); /* do nothing */\n    buffer[in] = nextproduced; \n    in = (in + 1) % BUFFERSIZE; \n    counter++;\n}\n</code></pre> <p>And the following consumer program:</p> <pre><code>while (true) {\n    while (counter == 0); /* do nothing */\n    next consumed = buffer[out]; \n    out = (out + 1) % BUFFERSIZE; \n    counter--;\n    /* consume the item in next consumed */\n}\n</code></pre> <p>On their own, the two programs are correct, but when executed concurrently, they are incorrect. This is due to the presence of race conditions.</p> <p>If the execution of <code>counter++</code> and <code>counter--</code> are interleaved, then the final value that we get after the execution of these two programs may be incorrect.</p> <p>A race condition is where multiple processes access and manipulate the same data concurrently, and the outcome of the execution depends on the order in which the accesses take place.</p>"},{"location":"courses/cse/notes/w5/race/#critical-section","title":"Critical Section","text":"<p>The parts of a program where atomicity needs to be guaranteed for its correct functioning are known as its critical sections.</p> <p>In the above consumer producer code, the critical section is <code>counter++</code> and <code>counter--</code> parts.</p> <ol> <li>The mutual exclusion: mutex locks</li> <li>Condition synchronisation: Synchronise the execution of a process in a CS based on certain conditions</li> </ol>"},{"location":"courses/cse/notes/w5/race/#requirements-for-a-solution-to-css","title":"Requirements for a solution to CSs","text":"<ol> <li>Mutual Exclusion</li> <li>Progress: If there is no process in the CS and another process wishes to enter the CS, it should be allowed to enter in some finite time</li> <li>Bounded Waiting If a process has requested to enter the CS, there exists a bound on the number of times other processes are allowed to enter the CS before A. This also implies that the CS is of finite length (no infinite loops).</li> </ol> <p>When a solution guarantees all three of these, we have two properties of that solution as a result:</p> <ol> <li>Safety: no race conditions</li> <li>Liveness: A program with a proper CS solution will not hang forever</li> </ol> <p>The protocol to approach a CS:</p> <ol> <li>Request</li> <li>Execute</li> <li>Exit</li> </ol>"},{"location":"courses/cse/notes/w5/sol/","title":"Solutions","text":"<p>CS solutions can be classified as follows:</p> <ol> <li>Software Mutex<ul> <li>Algo</li> <li>Software</li> <li>Busy waits</li> <li>Constrained</li> </ul> </li> <li>Hardware Spinlocks:<ul> <li>Busy waits</li> </ul> </li> <li>Software spinlocks and mutex locks</li> <li>Semaphores<ul> <li>No busy waiting</li> <li>Generalisation of mutex locks</li> <li>Able to protect two or more identical shared resources</li> </ul> </li> <li>Conditional Variables<ul> <li>No busy waiting</li> <li>Wake up condition</li> </ul> </li> </ol>"},{"location":"courses/cse/notes/w5/sol/#software-mutex","title":"Software Mutex","text":""},{"location":"courses/cse/notes/w5/sol/#petersons-solution","title":"Peterson's Solution","text":"<p>Conditions:</p> <ol> <li>Two processes only (can be extended to N processes with proper data struct)</li> <li>Practical only for single processor system</li> <li><code>LD/ST</code> MUST be atomic. An atomic operation completes \"in a single step\" w.r.t. other threads. An atomic <code>ST</code> means that no other process can read/write to a half written value.</li> </ol> <p>let <code>int turn</code> and <code>bool flag[2]</code> be two shared global variables</p> <p><code>flag = {false, false}</code> and <code>turn = arbitrary value</code></p> <p>The code for process <code>Pi</code>:</p> <pre><code>do {\n   flag[i] = true;\n   turn = j;\n\n   while(flag[j] &amp;&amp; turn == j);\n   // CS\n   // ...\n   flag[i] = false;\n   // RS\n   // ...\n\n} while(true);\n</code></pre> <p>The algorithm above is the solution for process <code>Pi</code>. The code for <code>Pj</code> is equivalent, just swap all the <code>i</code> to <code>j</code> and vice versa. </p> <p>Let's consider a few variations of this algorithm that one might think of:</p>"},{"location":"courses/cse/notes/w5/sol/#why-do-we-need-the-flag","title":"Why do we need the flag?","text":"<pre><code>do {\n   turn = j;\n\n   while(turn == j);\n   // CS\n   // ...\n   turn = j;\n   // RS\n   // ...\n\n} while(true);\n</code></pre> <p>This does not work, because imagine that this same process wants to re-enter the critical section, but the other process has left its critical section and will not re-enter it at all. In this case, the other process won't set the <code>turn = i</code> and this process will never be able to re-enter the CS. It violates progress!</p>"},{"location":"courses/cse/notes/w5/sol/#why-do-we-need-the-turn","title":"Why do we need the turn?","text":"<pre><code>do {\n   flag[i] = true;\n\n   while(flag[j]);\n   // CS\n   // ...\n   flag[i] = false;\n   // RS\n   // ...\n\n} while(true);\n</code></pre> <p>This does not work, because imagine that both processes want to enter the CS. process <code>i</code> set its <code>flag[i] = true</code> then gets interrupted, and process <code>j</code> is scheduled. Now process <code>j</code> sets <code>flag[j] = true</code>. Now we have a deadlock. This violates progress as well!</p> <p>Thus, we need both <code>turn</code> and <code>flag</code> for the algorithm to work correctly.</p>"},{"location":"courses/cse/notes/w5/sol/#a-few-variations-that-do-work","title":"A few variations that do work:","text":"<p>This is nothing but the same thing expressed in different \"words\" <pre><code>do {\n   flag[i] = true;\n   turn = j;\n\n   while(flag[j] &amp;&amp; turn != i);\n   // CS\n   // ...\n   flag[i] = false;\n   // RS\n   // ...\n\n} while(true);\n</code></pre></p> <p>Here <code>turn</code> is not a good name for the variable: <pre><code>do {\n   flag[i] = true;\n   turn = i;\n\n   while(flag[j] &amp;&amp; turn == i);\n   // CS\n   // ...\n   flag[i] = false;\n   // RS\n   // ...\n\n} while(true);\n</code></pre></p>"},{"location":"courses/cse/notes/w5/sol/#to-check-if-a-solution-is-correct","title":"To check if a solution is correct?","text":"<ol> <li>Check for mutex: can another process enter CS while one is already in CS?</li> <li>Check for progress: can another process enter CS if the other process has left the CS and won't try to re-enter it</li> <li>Check for bounded waiting: can a process re-enter the CS after just leaving it?</li> </ol>"},{"location":"courses/cse/notes/w5/sol/#synchronisation-hardware","title":"Synchronisation hardware","text":"<p>Number of hardware locks may be physically limited</p>"},{"location":"courses/cse/notes/w5/sol/#preventing-interrupts","title":"Preventing interrupts","text":"<p>We can intentionally prevent interrupts from occurring while a shared variable is being modified. This only works in a single processor system and is not feasible on a multiprocessor architecture, because:</p> <ol> <li>Time consuming, need to pass a message to all other processors</li> <li>Clock synchronisation between processors</li> <li>Decreases efficiency, defeats the purpose of multiprocessing</li> </ol>"},{"location":"courses/cse/notes/w5/sol/#atomic-instructions","title":"Atomic instructions","text":"<p>Atomic instructions can be implemented by locking the memory bus. mutex is trivial. Typically used for synchronisation in SMP systems. Common instructions are:</p> <ol> <li><code>compare_and_swap()</code></li> <li><code>test_and_set()</code></li> <li><code>read</code> and <code>write</code></li> <li><code>swap</code></li> <li><code>fetch_and_add</code></li> <li><code>load_link</code> / <code>store_conditional</code></li> </ol>"},{"location":"courses/cse/notes/w5/sol/#software-spinlocks-and-mutex-locks","title":"Software Spinlocks and Mutex locks","text":"<p>This uses hardware synchronisation instructions like the ones mentioned above. Although, this requires more memory than a hardware spinlock </p>"},{"location":"courses/cse/notes/w5/sol/#spinlocks","title":"Spinlocks","text":"<p>A spinlock provides mutex. It is simply a variable that is repeatedly checked for availability using atomic instructions:</p> <pre><code>acquire() {\n   // this is done using the test and set instruction, atomically!\n   while(!available);\n   available = false;\n}\n\nrelease() {\n   available = true;\n}\n</code></pre>"},{"location":"courses/cse/notes/w5/sol/#busy-waiting","title":"Busy Waiting","text":"<p>wastes CPU cycles. In a single processor environment, this is wasteful as no other process is able to run if there is busy wait.</p> <p>spinlock is useful in a multiprocessor system when the anticipated waiting time is shorter than a quantum. The advantage over mutex lock here is that there is a context switch with mutex locks, since this thread must be put to sleep first, and if the waiting time is short, this is more wasteful than spinning.</p>"},{"location":"courses/cse/notes/w5/sol/#mutex-lock","title":"Mutex Lock","text":"<p>Requires integration with the kernel scheduler since it puts the waiting process to sleep to reduce busy wait. Context switch is involved, and thus this is an expensive operation to perform. should only be done when the waiting time is much longer than the time used for context switching</p>"},{"location":"courses/cse/notes/w5/sol/#semaphores","title":"Semaphores","text":"<p>Generalised Mutex Lock, implemented at the kernel level. This is a high level software solution that relies on synchronisation hardware and is considered more robust than mutex locks. These do not cause busy waiting either.</p> <p>A semaphore is defined as:</p> <ol> <li><code>int numAvailableResources</code> whcih is defined in the kernel.</li> <li>accesible only via <code>signal()</code> and <code>wait()</code></li> </ol> <p>Semaphores can be used in two ways:</p> <ol> <li>Binary semaphore (0 or 1)</li> <li>Counting semaphore (1 to N)</li> </ol> <p>How they work: Semaphores integrate with the kernel scheduler to avoid busy waiting by putting the threads to sleep if there are no available resources.</p> <pre><code>wait(semaphore* S) {\n\n   // reduce the number of available resources\n   --s-&gt;value;\n\n   if(s-&gt;value &lt; 0) // if the number of available resources was 0, it is now negative, hence wait.\n      add this process to s-&gt;list and call block();\n}\n\n// The magnitude of the negative s-&gt;value indicates how many processes are waiting on the resource\n\nsignal(semaphore* S) {\n   ++s-&gt;value;\n\n    // if s-&gt; value was &lt; 0 it means that there was a process waiting. after ++, the value may be = 0.\n   if(s-&gt;value &lt;= 0)\n      remove a process P from s-&gt;list and call wakeup(P);\n}\n</code></pre> <p>note: <code>wait()</code> and <code>signal()</code> are critical sections of the semaphore algorithm. A lock must be used to ensure that there is mutex, and there is busy waiting when <code>wait()</code> and <code>signal()</code> are called. Semaphores do not completely eliminate busy waiting, however the busy waiting involved with the <code>wait(</code>) and <code>signal()</code> functions is way less than the busy waiting involved with using a spinlock.</p>"},{"location":"courses/cse/notes/w5/sol/#applying-semaphores-to-the-mpc-problem","title":"Applying semaphores to the MPC problem","text":"<p>shared vars:</p> <ul> <li>write index <code>int in</code></li> <li>read index <code>int out</code></li> <li><code>char buf[N]</code></li> <li>one binary and one counting semaphore</li> </ul> <pre><code>char buf[N];\nint in = 0; int out = 0;\n\nsemaphore chars = 0; \nsemaphore space = N;\nsemaphore mutex_p = 1; \nsemaphore mutex_c = 1;\n</code></pre> <p>Producer program:</p> <pre><code>void send (char c){\n   wait(space);\n   wait(mutex_p);\n\n   buf[in] = c;\n   in = (in + 1)%N;\n\n   signal(mutex_p);\n   signal(chars);\n}\n</code></pre> <pre><code>char rcv(){\n   char c;\n   wait(chars);\n   wait(mutex_c);\n\n   c = buf[out];\n   out = (out+1)%N;\n\n   signal(mutex_c);\n   signal(space);\n}\n</code></pre>"},{"location":"courses/cse/notes/w5/sol/#conditional-variables","title":"Conditional Variables","text":"<p>Conditional variables can be used to ensure that certain code is executed in order by multiple processes. A process can wait for the completion of a given event on  a particular object. It is used to communicate between processes or threads when certain conditions become <code>true</code>.</p> <p>An event is a change in the state of some condition or object that a process can listen to. It can wait to be awakened later by a signalling process which actually changes the condition/object.</p> <pre><code>pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;\npthread_cond_t p_cond_x = PTHREAD_COND_INITIALIZER;\n</code></pre> <p>Process 1: <pre><code>pthread_mutex_lock(&amp;mutex);\n// CRITICAL SECTION\n// ...\ncond_x = true;\npthread_cond_signal(&amp;p_cond_x);\npthread_mutex_unlock(&amp;mutex);\n</code></pre></p> <p>Process 2: <pre><code>pthread_mutex_lock(&amp;mutex);\nwhile(!cond_x)\n   pthread_cond_wait(&amp;p_cond_x, &amp;mutex);  // yields mutex, and goes to sleep\n\n// CRITICAL SECTION, can only be executed if cond_x == true\n// ...\npthread_mutex_unlock(&amp;mutex);\n</code></pre></p> <p>process 2 automatically reacquires the lock when it is awoken after process 1 sets <code>cond_x = true</code> and signals process 2. If another process has the mutex lock, process 2 won't be awoken. After being awoken, it will check if the <code>cond_x</code> is true before proceeding to the CS. This is important because process 1 may not have executed at all in the time that process 2 was sleeping</p> <p>Conditional Variables allow us to block a process out of the CS based on an arbitrary condition, even when the CS is empty. Mutex locks cannot do this alone.</p>"},{"location":"courses/cse/notes/w6/fs/","title":"File System","text":"<p>A file is a uniquely named collection of related information that is stored on the secondary storage. It acts as a logical storage unit in the computer, defined by the OS.</p> <p>Two types of entities in a file system:</p> <ol> <li>Directories - has a structured namespace</li> <li>Files</li> </ol> <p>A file consists of metadata and attributes (stored in the inode) such as name, size, datetime of creation, userId, etc. Its content is a group of data bytes</p> <p>A file can have a file extension which (usually) indicates the type of file that it is.</p>"},{"location":"courses/cse/notes/w6/fs/#file-interpreters","title":"File Interpreters","text":"<p>NTFS, FAT, EXT4</p> <ol> <li>OS Kernels:<ol> <li>unix based OS implement directories as special files.</li> <li>it knows the difference between directories and regular files\\</li> <li>supports executable files</li> <li>Doesn't understand any other formats</li> </ol> </li> <li>System programs<ol> <li>Compilers (.c) , Linkers and Loaders (elf)</li> </ol> </li> <li>User applications<ol> <li>VLC (mp3, mp4)</li> <li>MS Word (.docx) etc.</li> </ol> </li> </ol>"},{"location":"courses/cse/notes/w6/fs/#file-attrs","title":"File Attrs","text":"<p>A file has the following attrs:</p> <ol> <li>Name</li> <li>Identifier: usually a number (stored in inode) that identifies the file within the file system. It is the non-human-readable name for the file</li> <li>Type/Format</li> <li>Location (pointer)</li> <li>Size</li> <li>Protection and Access control information</li> <li>Time and date of creation/modification, owner</li> </ol> <p><code>ls -ali</code> shows the files with their permissions and identifiers</p>"},{"location":"courses/cse/notes/w6/fs/#file-interface","title":"File Interface","text":"<ol> <li>create</li> <li>read</li> <li>write</li> <li>delete</li> <li>reposition</li> <li>truncate (<code>file.length = 0</code>)</li> </ol>"},{"location":"courses/cse/notes/w6/fs/#creation","title":"Creation","text":"<ol> <li>free space is available</li> <li>An entry for the new file must be made in the directory (OS)</li> </ol>"},{"location":"courses/cse/notes/w6/fs/#readwrite","title":"Read/Write","text":"<ol> <li><code>open()</code> (must have perms)</li> <li><code>read()/write()</code> on file pointer (byte by bye)</li> <li><code>close()</code></li> </ol>"},{"location":"courses/cse/notes/w6/fs/#file-deletion","title":"File Deletion","text":"<ol> <li>free the space occupied by the file</li> <li>Remove its entry from the directory</li> </ol>"},{"location":"courses/cse/notes/w6/fs/#truncation","title":"Truncation","text":"<ol> <li>We keep the file's attributes, but its lengnth is set to 0</li> <li>The space for the file is released</li> </ol>"},{"location":"courses/cse/notes/w6/fs/#reposition","title":"Reposition","text":"<ol> <li><code>open()</code></li> <li>move the file pointer to another portion of the file (say byte <code>N</code>) <code>lseek()</code></li> </ol>"},{"location":"courses/cse/notes/w6/fs/#complex-operations","title":"Complex operations","text":"<p><code>copy</code> can be performed by <code>create</code>ing a new file, <code>opening</code> and <code>read</code>ing the old file and <code>write</code>ing to the new file</p>"},{"location":"courses/cse/notes/w6/fs/#the-file-system","title":"The File System","text":"<p>Maintains and organises a physical secondary storage</p> <p>A file system controls how data is stored and retrieved in a system.</p> <p>It is a set of rules (and features) used to determine the way data is stored.</p> <p>FS logically separates the segments of data on a disk and gives each piece a unique name. Each group of data is the contents of a file, and file attributes may be stored elsewhere. The structural and logical rules used to manage these groups of data are called a \"file system\".</p> <p>FSs may vary between OSs, but there are a few that are widely used:</p> <ol> <li>File Allocation Table (FAT) (Win)</li> <li>NTFS (default on Win)</li> <li>ext4 (linux)</li> <li>Universal Disk Format UDF (DVDs)</li> <li>Hierarchical File System (HFS) (macOS)</li> <li>Apple File System (AFS) (macOS)</li> </ol>"},{"location":"courses/cse/notes/w6/fs/#unix-file-system-data-structures","title":"Unix file system data structures","text":"<ol> <li>File Descriptor Table: FD table per process -&gt; Contains file descriptors which are numbers that uniquely (per proc) identify an open file in a computer's OS. Each FD is associated with a file pointer that points to the SWOFT.</li> <li>System Wide Open File Table (SWOFT): Contains a list of all opened files, sockets, devices (anything that uses <code>open()</code>)<ul> <li><code>cp</code>: current pointer (pointer to the specific byte of the file being read)</li> <li><code>Access status</code>: such as read/write/append/execute etc.</li> <li><code>Open count</code>: How many FD table entries point to it. We cannot remove open file entries unless their ref count reaches 0</li> <li><code>Inode pointer</code>: A pointer to a Unix inode table.</li> </ul> </li> <li>Inode Table: short for Index Node Table or File Contorl Block (FCB): database of all file attributes and the locations of their contents<ul> <li>Does not store filename</li> </ul> </li> </ol>"},{"location":"courses/cse/notes/w6/fs/#file-system-mapping","title":"File System Mapping","text":"<ol> <li> <p>Multiple FD table entries can point to the same swoft</p> <ol> <li>FD pass to another proc by sockets</li> <li>FD inherited from parent after <code>fork()</code></li> <li>Single process can have two or more FDs that ref the same file. <code>dup()</code> or <code>dup2()</code>. Two FDs created this way affect each other</li> </ol> </li> <li> <p>Multiple swoft entries can point to the same inode. This happens when multiple files call <code>open()</code> on the same file. Two FDs do not affect each other this way.</p> </li> </ol>"},{"location":"courses/cse/notes/w6/fs/#appendix","title":"Appendix","text":""},{"location":"courses/cse/notes/w6/fs/#inode","title":"Inode","text":""},{"location":"courses/cse/notes/w6/fs/#the-physical-fs","title":"The physical fs","text":"<p>logical fs is the interface. this is what happens on bare metal</p> <ol> <li>Boot Control Block per Volume. Contains the information needed by the system to boot an OS from that volume</li> <li>Volume Control Block. contains volume details, such as number of blocks, block size, free block count, free block pointers, free FCB count and FCB pointers.</li> <li>Directory Structure. used to organise files</li> <li>FCB. one entry per file, contains attrs, has a unique id and associated with a directory entry.</li> </ol> <p>Volume vs Partition:</p> <ol> <li>A single volume can span multiple disks, a partition is created on a single disk.</li> <li>Volumes have names (C:) partitions are only identified by numbers.</li> <li>Partitions are more sutited for individual devices, volumes are more suited for network attached storages</li> </ol>"},{"location":"courses/cse/notes/w6/fs/#mounting-fs-during-boot","title":"Mounting fs during boot","text":"<p>root partition which contain OS kernel and sometimes other system files, is mounted at boot time. The system has an in-memory mount table that contains information about each mounted volume. Other volumes can be automatically mounted at boot time or later, depending on the operating system.</p> <p>The mount procedure:</p> <ol> <li>The OS is given the name of the device and the mount point - the location w/i the file structure where the fs is to be attached</li> <li>Typically, the mount point is an empty directory.</li> </ol> <p>In memory information about the fs is loaded at mount time and there are three data structs related to this:</p> <ol> <li>Mount Table: </li> <li>Inode Table: contains directory structure and file pointers to the actual data on secondary storage or cached memory. </li> <li>swoft</li> <li>FDT </li> </ol>"},{"location":"courses/cse/notes/w6/fs/#searching-for-a-file","title":"Searching for a file","text":"<p>Since a directory is also a file, it has its own inode number. Its contents are other filenames (can also be dirs, subdirs) and their inode numbers.</p> <p>Each volume/partition also contains information about its own fs. The volume table of contents is a list of entries in the device directory that have info like filenames, size, etc.</p>"},{"location":"courses/cse/notes/w6/fs/#purpose","title":"Purpose","text":"<ol> <li>Locate: a (group of) files quickly</li> <li>Naming:<ul> <li>same name for different files in different directories</li> <li>same name for different extensions</li> <li>same file can have different names (links)</li> </ul> </li> <li>Organisation</li> </ol>"},{"location":"courses/cse/notes/w6/fs/#folder","title":"Folder","text":"<p>A folder is a GUI concept, because of the icons. The term directory is more broad and refers to a general collection of files</p>"},{"location":"courses/cse/notes/w6/fs/#directory-operations","title":"Directory operations","text":"<p>All of these involve modifying the dir:</p> <ol> <li><code>create/delete</code>: create/delete a file or subdi</li> <li><code>list</code></li> <li><code>rename</code>: a file</li> <li><code>search</code>: for a file</li> <li><code>traverse</code>: <code>cd</code></li> </ol>"},{"location":"courses/cse/notes/w6/fs/#directory-structure","title":"Directory structure","text":""},{"location":"courses/cse/notes/w6/fs/#single-level","title":"Single level","text":"<p>All files in one folder</p>"},{"location":"courses/cse/notes/w6/fs/#two-level","title":"Two level","text":"<p>Separate folder per user</p>"},{"location":"courses/cse/notes/w6/fs/#tree-level","title":"Tree level","text":"<p>Unique path to reach file (no links)</p>"},{"location":"courses/cse/notes/w6/fs/#file-links","title":"File Links","text":"<p>Different filenames may point to the same inode entry. These are formally called links</p>"},{"location":"courses/cse/notes/w6/fs/#hard-link","title":"Hard Link","text":"<p>Extra names mapped to the same inode entry. Multiple \"files\" can point to the same content. Each hard link increases the ref count of the file in the inode entry.</p> <p><code>.</code> and <code>..</code> are hard links!</p> <p>In unix, directories cannot be hard linked, this is done to keep the directory structure acyclic. Symbolic links should be used instead</p>"},{"location":"courses/cse/notes/w6/fs/#symlinks","title":"Symlinks","text":"<p>Symbolic Links are simply files whose content is a text string (reference to another file/dir) which is automatically interpreted and followed by the OS as a path to another file/dir.</p> <p>Symlinks aka soft links. dirs can be linked with symlinks.</p> <p>Symlinks do not increase the ref count. Will break if the original file that it is pointing to is deleted (although this is not a consequence of ref count not being <code>++</code>). The link will work again if the deleted file is recreated</p>"},{"location":"courses/cse/notes/w6/fs/#graph-directory-structure","title":"Graph Directory Structure","text":""},{"location":"courses/cse/notes/w6/fs/#acyclic","title":"Acyclic","text":"<p>multiple paths can reach the same file (links)</p>"},{"location":"courses/cse/notes/w6/fs/#general","title":"General","text":"<p>Self referencing can cause infinite loops. Reference counting does not work if there are self references. Performance issues with searching and traversal</p>"},{"location":"courses/cse/notes/w8/networks/","title":"Networks","text":"<p>f## Components of a network</p> <ol> <li>End systems, clients running network apps</li> <li>Communication links: fiber optics, radio</li> <li>Packet switches: forward packets. routers and switches</li> <li>Protocols: TCP, ID, UDP, HTTP, etc.</li> </ol>"},{"location":"courses/cse/notes/w8/networks/#the-internet","title":"The internet","text":"<p>A network of networks - Interconnected ISP</p>"},{"location":"courses/cse/notes/w8/networks/#componenets-of-the-internet","title":"Componenets of the Internet","text":"<ol> <li>Standards (open, free)<ul> <li>RFC: Request For Comments</li> <li>IETF: Internet Engineering Task Force</li> </ul> </li> <li>Infrastructure that provides services to applications<ul> <li>Web, VoIP, email, games</li> </ul> </li> <li>Programming interface to apps<ul> <li>hooks that allow sending and receiving app programs to connect to the internet</li> <li>provides service options, analogus to postal service</li> <li>Socket APIs</li> </ul> </li> </ol>"},{"location":"courses/cse/notes/w8/networks/#important-properties-for-the-internet","title":"Important Properties for the Internet","text":"<ol> <li>Interoperability: standard communication protocols (HTTP, TCP, UDP, IP, ARP, whatever-P), conventions.<ul> <li>Each <code>P</code> deals with a  unit of data called a packet. Each packet has a H: header and a P: payload.</li> <li>protocols define format, order of msgs sent and received among network entities, and actions taken on msg transmission and receipt</li> </ul> </li> <li>Shared resources: Internet is a shared resource.<ul> <li>Time division multiplexing (TDM, eg: RCS)</li> <li>Frequency division multiplexing (FDM, eg: multiprocessors)</li> <li>Data traffic is bursty. Packet switching &amp; statistical multiplexing:<ul> <li>1 Mbps link.</li> <li>Each user 100 Kbps when active.</li> <li>Active 10% of the time.</li> <li>Packet occupies the link on demand only</li> <li>FDM or TDM - fixed dedicated fraction of link</li> </ul> </li> </ul> </li> <li>Complex interacting components<ul> <li>Many pieces to a network: naming, routing, reliability, lots of apps</li> <li>Emergent Behaviour</li> <li>Layering reduces interactions from \\(\\mathcal{O}(N^2)\\) to \\(\\mathcal{O}(N)\\) where \\(N\\) is the number of modules</li> <li>Internet protocol stack:<ul> <li>Application (FTP, HTTP)</li> <li>Transport (TCP, UDP)</li> <li>Network (IP, routing)</li> <li>Link: data transfer b/n neighbouring network elements (Ethernet, Wifi)</li> <li>Physical: Hardware</li> <li>Cascaded headers</li> </ul> </li> </ul> </li> <li>Scalability</li> </ol>"},{"location":"courses/cse/notes/w9/net_perf/","title":"Network Performance","text":""},{"location":"courses/cse/notes/w9/net_perf/#packet-delay-loss","title":"Packet Delay &amp; Loss","text":"<p>If the packet arrival rate &gt; output link rate, packets queue in a packet buffer to wait to be linked. If there is no space in the buffer, the packets are dropped. Priority packet dropping. Dropped packets may or may not be re-transmitted by previous node. Loss is random - each link has an average probability for loss. Packets may also be dropped due to errors (wifi)</p> <p>Four sources of packet delay:</p> <ol> <li>Nodal Processing: checksum/multiplexing (typically &lt; ms)</li> <li>Queuing: depends on utilisation</li> <li>Transmission: Time to push packet from router buffers to link ((bandwidth) bits/s, <code>size/speed</code>) link technology</li> <li>Propagation: Time for packet to move from beginning to end of link (<code>distance/(2/3*c)</code>)</li> </ol> <p>R: link bandwidth L: packet size (bits) a: avg packet arrival rate</p> <p>La/R average link utilisation.</p> <ol> <li><code>~0</code>: small queue delays</li> <li><code>~1</code>: large queue delays</li> <li><code>&gt;1</code>: packets dropped, more work arriving, queue delay infinite!</li> </ol> <p>Queue delay is a convex function of utilisation, assuming random and bursty packet arrival (normal for internet)</p> <p>In real networks, the full delay will be composed of multiple routers. The average loss rate is the composition of loss rates for multiple routers as well</p> <p><code>Traceroute</code> is a program that measures the round trip time of 3 packets</p>"},{"location":"courses/cse/notes/w9/net_perf/#traceroute","title":"Traceroute","text":"<p>Source sends a series of UDP segments to destination with an unlikely port number</p> <p>The first set of 3 packets has TTL=1 The next set of 3 packets has TTL=2, and so on.</p> <p>If the TTL expires before reaching the destination, an ICMP message (type 11 code 0) is sent back to the source by the router where the TTL expired.</p> <p>Traceroute stops when the UDP segment reaches the host and it replies ICMP \"port unreachable\" (type 3 code 3), and then the source stops.</p>"},{"location":"courses/cse/notes/w9/net_perf/#icmp","title":"ICMP","text":"<p><code>ping</code> is another useful program.</p> <ol> <li><code>Traceroute</code> uses ICMP TTL exceeded and ICMP port unreacahble messages</li> <li><code>Ping</code> uses ICMP echo request/reply messages</li> </ol> <p>These are not critical for normal operation, thus it is possible for them to be disabled (hence the <code>*</code> in some output lines) or misconfigured or bugged</p>"},{"location":"courses/cse/notes/w9/net_perf/#throughput","title":"Throughput","text":"<p>Rate at which bits can be transferred between the sender and the receiver.</p> <p>ONE link between all the routers in the connection is the bottleneck for the speed</p>"},{"location":"courses/cse/notes/w9/net_perf/#space-time-diagram","title":"Space Time Diagram","text":"<p>If the diagram is blue part dominated, it is throughput limited (use faster link!). If it is white dominated, it is delay limited (not much scope for improvement)</p>"},{"location":"courses/esc/deadlines/","title":"Important Deadlines","text":""},{"location":"courses/esc/to_know/","title":"Important Information","text":""},{"location":"courses/esc/notes/w1/swdev_methods/","title":"SW Dev Methods","text":""},{"location":"courses/esc/notes/w1/swdev_methods/#swdev-life-cycle","title":"SWDev Life Cycle","text":"<p>Analysis Diagram </p> <ol> <li>Requirements<ol> <li>List candidate requirements</li> <li>Understand the system context through domain modelling and business modelling</li> <li>Capturing functional as well as non functional requirements         1. Functional requirement: A booking system must make the booking if the user has paid for the booking         2. Non functional requirement: must book w/i 30s</li> </ol> </li> <li>Analysis<ol> <li>Understand how requirements interact and what it means for the system</li> <li>Client might miss out on specifications in the requirement - you need to read between the lines</li> <li>Developing an internal view of the system</li> <li>Identifying the analysis classes and their collaborators - place holders of functionality</li> </ol> </li> <li>Design<ol> <li>Deciding the collaboration between the components lies at the heart of SW Design</li> <li>A component fulfils its own responsibility through the code it contians</li> <li>Components exhcange information through methods</li> <li>Tech stack</li> <li>Dividing the system into implementation units</li> </ol> </li> <li>Implementation<ol> <li>Programming</li> <li>Unit testing</li> <li>Integration testing</li> <li>Deployment model</li> </ol> </li> <li>Testing<ol> <li>Manaul &amp; automated testing</li> <li>Continuous integration and testing</li> </ol> </li> </ol>"},{"location":"courses/esc/notes/w1/swdev_methods/#swdev-methods","title":"SWDev Methods","text":""},{"location":"courses/esc/notes/w1/swdev_methods/#waterfall-model","title":"Waterfall Model","text":"<p>Linear sequential model of SDLC</p> <ul> <li>Clearly defined stages, well understood milestones</li> <li>Hardly adjustable to new requirements</li> </ul> <pre><code>Requirements\n    Analysis\n        Design\n            Implementation\n                Test\n                    Roll out\n</code></pre> <p>Work products from each stage are inputs to the next page</p> <p>Each stage of the life cycle is separate from the others</p> <p>If the requirements are not stable, this can be a disadvantage, because it means restarting the life cycle if there is a change in the product requirements</p>"},{"location":"courses/esc/notes/w1/swdev_methods/#rapid-prototyping","title":"Rapid Prototyping","text":"<p>Make a prototype of your end product ASAP to review w/ customer</p> <ol> <li>building of prototypes to clarify requirements and system scope</li> <li>The prototypes, however, should never become the final system</li> <li>Once the requirements have been sort out, the system is built formally</li> </ol>"},{"location":"courses/esc/notes/w1/swdev_methods/#iterative-and-incremental-development","title":"Iterative and Incremental Development","text":"<p>RADIT, Release, RADIT, Release, ...</p> <p>Sprints, time boxed development cycles</p> <p>iterative incremental model: make small steps at each iteration: wheel comes before car</p>"},{"location":"courses/esc/notes/w1/swdev_methods/#agile-method","title":"Agile Method","text":"<ol> <li>Individual interactions over processes and tools</li> <li>Working software over comprehensive documentation</li> <li>Customer collaboration over contract negotiation</li> <li>Responding to change over following a plan</li> </ol> <p>12 Agile principles</p> <ol> <li>Customer satisfaction by rapid delivery of useful software</li> <li>Welcome changing requirements, even late in development</li> <li>Working software is delivered frequently (weeks rather than months)</li> <li>Close, daily cooperation between business people and developers</li> <li>Projects are built around motivated individuals, who should be trusted</li> <li>Face-to-face conversation is the best form of communication (co-location)</li> <li>Working software is the principal measure of progress (talk is cheap, show me your code)</li> <li>Sustainable development, able to maintain a constant pace</li> <li>Continuous attention to technical excellence and good design</li> <li>Simplicity\u2014the art of maximizing the amount of work not done\u2014is essential</li> <li>Self-organizing teams</li> <li>Regular adaptation to changing circumstance</li> </ol> <p>It has high demand on the quality of devs, and its hard to separate responsibilities. Not recommended for team size &gt; 12</p>"},{"location":"courses/esc/notes/w1/swdev_methods/#abstraction","title":"Abstraction","text":"<p>Black box, the process of hiding the implementation details of something and providing the user with just the functionality</p>"},{"location":"courses/esc/notes/w1/swdev_methods/#hierarchy","title":"Hierarchy","text":"<p>OOP</p>"},{"location":"courses/esc/notes/w1/swdev_methods/#the-software-equation","title":"The Software Equation","text":"<p>\\((B^{1/3} * Size ) / Productivity = Effort^{1/3} * Time^{4/3}\\)</p> <p>\\(Effort = \\left[ \\cfrac{Size}{Productivity \\times Time^{4/3}} \\right]^3 \\times B\\)</p>"},{"location":"courses/esc/notes/w2/uml/","title":"UML","text":""},{"location":"courses/esc/notes/w2/uml/#modeling-languages","title":"Modeling Languages","text":"<ul> <li>For docs and communication</li> <li>Important Part of any software dev process</li> <li>It forms the basis of an iterative design process</li> <li>It forms the basis of communication between different parties</li> </ul>"},{"location":"courses/esc/notes/w2/uml/#unified-modelling-language-uml","title":"Unified Modelling Language (UML)","text":"<ol> <li>Use case diagrams</li> <li>Sequence diagrams</li> <li>Class diagrams</li> <li>State diagrams</li> </ol>"},{"location":"courses/esc/notes/w2/uml/#use-case-diagrams","title":"Use Case Diagrams","text":""},{"location":"courses/esc/notes/w2/uml/#roles-functionality","title":"Roles &amp; Functionality","text":"<ol> <li>The user can:</li> <li>li1</li> <li>li2</li> <li>The admin can:</li> <li>li1</li> <li>li2</li> </ol>"},{"location":"courses/esc/notes/w2/uml/#use-case-document","title":"Use Case Document","text":""},{"location":"courses/esc/notes/w2/uml/#format","title":"Format","text":"<p>Name: Name of the use case Objective: Action of the use case Actor: Who carries out the action (primary), who is affected by the action (secondary) Constraints: the formal rules and limitations of a use case, defining what can and cannot be done Flow: Describe the interactions between the actor(s) and the system clearly enough for an outisder to easily understand    - The flow of events should represent what the system does (not how)</p>"},{"location":"courses/esc/notes/w2/uml/#actor","title":"Actor","text":"<p>Each actor has associated use cases that are used to document what the particular actor expects of the system.</p> <p>Identifying the actors of a system should be done early in the life cycle.</p> <p>Actors are captured, including    - Who they repr    - Why they're needed    - What interests the actor has in the system    - Chars of the actor    - Name    - Brief description    - Relationships to use cases</p>"},{"location":"courses/esc/notes/w2/uml/#constraints","title":"Constraints","text":"<ul> <li>Pre conditions: xxx thing has already run before yyy thing runs (Create before Edit). These must be observable states, these are not what triggers a use case.</li> <li>Post conditions: zzz thing is always true after yyy thing runs (Consistent after Edit). Must be true regardless of the flow</li> <li>Invariants: www thing is the same before and after yyy thing runs (ID)</li> </ul>"},{"location":"courses/esc/notes/w2/uml/#flow","title":"Flow","text":"<ul> <li>Describe the interaction between the actors and the system clearly</li> <li> <p>What the system does, not how</p> </li> <li> <p>Basic Flow (Most common path)</p> </li> <li>Alternate Flow (Alternate paths)</li> <li> <p>Exception Flow (When something goes wrong)</p> </li> <li> <p>Describes how the use case starts and ends</p> </li> <li>What data is exchanged b/n actor and use case</li> <li>UI should be left out of the use case</li> <li>Flow over functionality When the actor...</li> <li>Avoid vague terminology</li> <li>Detailed flow (what happens when) - used for test cases</li> </ul> <p>Example:</p> <p></p>"},{"location":"courses/esc/notes/w2/uml/#use-case-diagrams_1","title":"Use Case Diagrams","text":"<p>Shows the set of use cases, actors and their relationships. It contains actors, actions/use case, connection between use case/actions.</p> <p>Use case diagrams represent system functionaltiy, the requirements of the system from the user's perspective</p>"},{"location":"courses/esc/notes/w2/uml/#actors","title":"Actors","text":"<p>Actors are the object that provide or receive information from the system. Primary: proivdes, Secondary: receives</p>"},{"location":"courses/esc/notes/w2/uml/#use-case","title":"Use Case","text":"<p>Is a desc of the set of seq of actions that the system performs that yields an observable value to the actor. Should begin with a verb. Granularity of use cases is important, its not a sequence diagram</p> <p>Use Case diagrams are not supposed to capture misuse cases (absuing system, hacks, security)</p> <p>Example:</p> <p></p>"},{"location":"courses/esc/notes/w2/uml/#to-draw-a-use-case-diagram","title":"To Draw a Use Case Diagram","text":"<ol> <li>Take a look at the requirement description to figure out the roles and functionalities</li> <li>Roles can be seen as the \"actors\" and \"functionalities\" can be seen as the \"use cases\". Example: The users can..., the Admins can...</li> <li>Pre conditions of the initial use cases can also become use cases if they are coarse enough</li> <li>Consider mis use-cases</li> <li>Whatever can prevent a mis use-case becomes another use-case</li> </ol>"},{"location":"courses/esc/notes/w3/seq_diag/","title":"Sequence Diagram","text":"<p>Complements a use case with details of workflow events.</p> <p>A sequence diagram is an interaction diagram that highlights the order in which instructions are executed/messages are exchanged</p> <ul> <li>Component symbols: lifeline/activation box/option loop/alternative</li> <li>Message symbols (arrows): synch/asynchronous message</li> </ul> <p>Instances of the actors are placed at the top of the diagram.</p> <p>Lifelines represent either roles or object instances that participate in the sequence being modeled.</p> <p>Activation boxes represent the time needed for an object to complete a task. The longer a task takes, the longer the activation box.</p> <p>To show interaction, an arrow is drawn from the sending object to the receiving object. A solid arrowheard for sync operations, A stick arrowhead for async operations.</p> <p>On the same lifeline, a higher message precedes a lower message and Message sending precedes message receiving.</p> <p>An alternative can be used to show multiple/branching cases</p> <p>A loop can be used to show repeating things</p> <p></p> <p>A timeout can indicate an action that only repeats after a certain interval has passed</p> <p></p> <p>A parallel can be used to indicate two things that happen simultaneously</p> <p></p>"},{"location":"courses/esc/notes/w3/sm_diag/","title":"State Machine Diagram","text":"<p>Remember Finite State Machines?</p> <p></p> <p>Transitions: from one state to another are denoted by stick arrows with a trigger on them</p> <p>Action: what results in the transition</p> <p>Self Loops: Self loops</p>"},{"location":"courses/esc/notes/w4/conc/","title":"Concurrency","text":""},{"location":"courses/esc/notes/w4/conc/#concurrency","title":"Concurrency","text":"<p>Given a semi prime, your program outs its prime factors within 6 days</p> <ol> <li>Precondition - given a semi prime</li> <li>Post condition - outs its prime factors</li> <li>Non functional requirement - within 6 days</li> </ol>"},{"location":"courses/esc/notes/w4/conc/#testing","title":"Testing","text":"<ol> <li>Correctness Testing</li> <li>Performance Testing</li> </ol> <p>Ideally, with K extra cores we can do K times faster</p> <p>Multithreading programs must be efficient and not have:</p> <ol> <li>race conditions</li> <li>visibility issue</li> <li>execution ordering problem</li> <li>deadlock</li> </ol>"},{"location":"courses/hass/deadlines/","title":"Important Deadlines","text":""},{"location":"courses/hass/to_know/","title":"Important Information","text":""},{"location":"courses/hass/notes/w1/org_be_sim/","title":"Org Sim","text":""},{"location":"courses/hass/notes/w1/org_be_sim/#frank-svp-pm","title":"Frank, svp pm","text":"<ul> <li>How to proceed</li> <li>Don't want a lawsuit</li> <li>Potential causes - misleading instructions</li> </ul>"},{"location":"courses/hass/notes/w1/org_be_sim/#ceo","title":"CEO","text":"<p>Issuing a press release to tell the users that they're aware of the issues and will get to the bottom of it asap</p>"},{"location":"courses/hass/notes/w1/org_be_sim/#publicist","title":"Publicist","text":"<p>respond to medical physicial</p>"},{"location":"courses/hass/notes/w1/org_be_sim/#marketing-people","title":"marketing people","text":"<ul> <li>Images that suggest that the issue lies with instructions on the packaging</li> <li>Report that clinical tests were skewed because younger and more educated people took part in it</li> <li>More vidence suggesting that people dont know the correct instructions</li> <li>Laboratry conditions on diebetic patients reveal that no diff in readings</li> <li>even more evidence to support the above claim</li> </ul>"},{"location":"courses/hass/notes/w1/org_be_sim/#elm-streed-sandy-powers","title":"Elm Streed Sandy Powers","text":"<p>Elderly people and customer misuse suggestion</p> <p>-- Chose Reprint Instructions vs doc campaign --</p> <p>-- 100k/150k split -- translate/doc campaign</p> <p>-- 100k/0k split -- images/doc campaign</p> <p>-- 100k/200k split -- reprint/doc campaign</p> <p>Choice:</p> <p>lay off 400 people with certain financial impact</p> <p>CHOSE: replacement device - 33% chance that there is no layoff, 67% chance - 600 people</p> <p>press release</p> <p>As all of you are aware, there have been more than usual reports over the past few months about inaccurate readings of our GlucoGauge Device. Over the course of this time, we have tried a number of things to help mitigate this issue.</p> <p>Initially, we believed that the issue arose due to consumer misuse, and thus we updated and reprinted the instructions on our packaging. Next, we also provided images with the instructions, provided translated instructions in two additional languages, and also carried out a communication campaign with physicians to help them teach their patients how to use the device correctly.</p> <p>Whilst we hoped that this would solve the issue, this was not the case. We later discovered that there was a possible error in the microcontroller of the device. We addressed this issue by shipping out a new device to all the people who had purchased our GlucoGuage Device. </p> <p>Despite all that has been done, we still continue to closely monitor the situation for developments and hope to fully resolve this issue as soon as possible.</p>"},{"location":"courses/hass/notes/w2/critical/","title":"Critical Thinking","text":""},{"location":"courses/hass/notes/w2/critical/#ct-standards","title":"CT Standards","text":"<ol> <li>Clarity: Truth can be expressed clearly</li> <li>Precsion: Close attention to detial</li> <li>Accuracy: Making sure that the beliefs/information are valid</li> <li>Consistency: Non contradictory claims</li> <li>Relevance (cogent premises are relevant to the claim)</li> <li>Reasoning</li> <li>Completeness: explore the issue fully</li> <li>Fairness: Open mindedly</li> </ol>"},{"location":"courses/hass/notes/w2/critical/#defs","title":"Defs","text":"<ol> <li>Claims: Sentence that can be viewed as T/F. Expresses an opinion or belief</li> <li>Premises: Evidences to support the claim</li> <li>Conclusion: The claim</li> <li>Argument: A group of statements that contain both a premise and a conclusion</li> <li>Issue/Question: What is raised when the claim is called into question</li> </ol>"},{"location":"courses/hass/notes/w4/crit/","title":"Four Aspects","text":"<ol> <li>The critique of authority denies the hegemony of a single legitimate viewpoint or interest, promoting instead the acceptance of a plurality of positions</li> <li>The critique of objectivity denies the assumption of pure, value-free knowledge and introduces the Foucauldian notion of power/knowledge</li> <li></li> </ol>"},{"location":"courses/hass/notes/w9/ans/","title":"Ans","text":"<p>A paradigm is a set of very basic meta theoretical assumptions which form the basis of the frame of reference, the modes of theorising and operation for social theorists who operate within them. Through these fundamental assumptions, paradigms provide the foundational tools for carrying out analysis and deduction of facts about an organisation. Different paradigms allow us to view and analyse organisations from different competing viewpoints.</p> <p>The home paradigm is the philosophical perspective that a person feels the most comfortable with, and the perspective with which they are most likely to view the world. My home paradigm is probably the functionalist, as I try to be as rational and practical as possible in any situation (eg: will save 5 people in the trolley problem), and my approach to any situation is problem driven (find the issue, think about how to fix the issue objectively).</p> <p>The two most significant limitations are:</p> <ol> <li>The subjective-objective labels which are used to distinguish between the four paradigms are socially contrived. In other words, what counts as subjective or objective is a \"subjective\" question in itself. Using them as the basis for \"fundamental\" assumptions and separation of paradigms is contradictory because they no longer remain fundamental but are subject to debate themselves</li> <li>Secondly, the subjective-objective separation leads many to associate objectivity with quantitative analysis and subjectivity with qualitative analysis. Due to this, a lot of people misunderstand the difference between the two, believing that the difference is just different methods of collecting data. This undermines the actual importance of a particular method of data collection which is the ultimate purpose/goal for the research.</li> </ol>"},{"location":"courses/ml/deadlines/","title":"Important Deadlines","text":""},{"location":"courses/ml/rel_linalg/","title":"Relevant Linear Algebra","text":""},{"location":"courses/ml/rel_linalg/#optimisation-problems","title":"Optimisation Problems","text":""},{"location":"courses/ml/rel_linalg/#definitions","title":"Definitions","text":"<p>A positive definite matrix is a symmetric matrix whoes eigen values are all positive. Additionally, this matrix has the property that \\(z^T M z &gt; 0 \\forall z \\neq 0\\). All the principal minors of this matrix are positive</p> <p>A negative definite matrix is a symmetric matrix whoes eigen values are all negative. Additionally, this matrix has the property that \\(z^T M z &lt; 0 \\forall z \\neq 0\\). The minors with an odd number of rows and columns are negative and the minors with an even number of rows and columns are positive</p>"},{"location":"courses/ml/rel_linalg/#unconstrained-optimisation","title":"Unconstrained Optimisation","text":"<p>A critical point where the hessian is a:</p> <ol> <li>positive definite is a local min</li> <li>negative definite is a local max</li> <li>mixed (non 0) eigen values is a saddle point</li> </ol>"},{"location":"courses/ml/rel_linalg/#constrained-optimisation","title":"Constrained Optimisation","text":"<p>Max \\(f(\\mathbf{x})\\) subject to \\(\\mathbf{g}(\\mathbf{x}) \\leq 0\\) and \\(\\mathbf{h}(\\mathbf{x}) = 0\\)</p> <p>\\(L(\\mathbf{x}, \\mathbf{\\mu}, \\mathbf{\\lambda}) = f(\\mathbf{x}) + \\mathbf{\\mu}^T\\mathbf{g}(\\mathbf{x}) + \\mathbf{\\lambda}^T\\mathbf{f}(\\mathbf{x})\\)</p> <p>We then find the critical points of \\(L\\), which satisfy the following conditions:</p> <ol> <li>Primal feasibility \\(\\mathbf{g}(\\mathbf{x^*}) \\leq 0\\) and \\(\\mathbf{h}(\\mathbf{x^*}) = 0\\)</li> <li>Dual feasibility \\(\\mu \\geq 0\\)</li> <li>Complementary slackness \\(\\mu_i g_i(\\mathbf{x^*}) = 0\\)</li> </ol> <p>Given that there are a total of \\(l\\) binding inequality constraints and \\(n\\) equality constraints, we have a total of \\(m\\) effective constraints.</p> <p>A critical point where \\((2m + i)\\) th principal minors \\(\\forall i \\geq 1\\) of the hessian:</p> <ol> <li>Alternate signs, starting at \\((-1)^{m+1}\\) is a local max</li> <li>All have the sign \\((-1)^{m}\\) is a local min</li> </ol>"},{"location":"courses/ml/to_know/","title":"Important Information","text":""},{"location":"courses/ml/notes/w1/intro/","title":"Intro","text":""},{"location":"courses/ml/notes/w1/intro/#what-is-ml","title":"What is ML","text":"<p>A computer program learns from XP (E) wrt task (T) and perf measure (P) if P on T improves with E.</p>"},{"location":"courses/ml/notes/w1/intro/#no-free-lunch-rule","title":"No free lunch rule:","text":"<ol> <li>Training and testing data come from the same distribution</li> <li>Some assumptions and biases</li> </ol>"},{"location":"courses/ml/notes/w1/intro/#factors-affecting-perf","title":"Factors affecting perf","text":"<ol> <li>quality of training data</li> <li>Form and extent of initial background knowledge</li> <li>type of feedback provided</li> <li>learning algo used</li> </ol>"},{"location":"courses/ml/notes/w1/intro/#two-important-factors","title":"Two important factors","text":"<ol> <li>Modelling</li> <li>Optimisation</li> </ol>"},{"location":"courses/ml/notes/w1/intro/#types-of-ml","title":"Types of ML","text":""},{"location":"courses/ml/notes/w1/intro/#based-on-info-available","title":"Based on Info available","text":"<ol> <li>Supervised (\\(\\{x_n \\in \\mathbb{R}^d, y_n \\in \\mathbb{R}\\}^N_{n=1}\\))<ol> <li>classification</li> <li>regression</li> </ol> </li> <li>Unsupervised (\\(\\{x_n \\in \\mathbb{R}^b\\}^N_{n=1}\\))<ol> <li>clustering</li> <li>probability distribution estimation</li> <li>finding associations in features</li> <li>dimension reduction</li> </ol> </li> <li>Semi Supervised</li> <li>Reinforcement<ol> <li>Decision making (robots, games)</li> </ol> </li> </ol>"},{"location":"courses/ml/notes/w1/intro/#based-on-learners-role","title":"Based on learner's role","text":"<ol> <li>Passive - What most ML models are, use data to produce a model</li> <li>Active - Query the environment, perform experiments</li> </ol>"},{"location":"courses/ml/notes/w1/intro/#different-ml-problems","title":"Different ML Problems","text":""},{"location":"courses/ml/notes/w1/intro/#classification-problem","title":"Classification Problem","text":"<p>Tumor classification: Find a good classifier - a mapping (function) from samples to labels normal/tumor</p> <p>A sample is represented using a feature vector. For instance, the coordinates of this vector could be constructed from a measure of activeness of each gene in the tissue cells. The assumption here is that this info is sufficient to determine malignance.</p> <p>It is important to follow the same protocol to construct feature vectors for training samples and any new samples to be classified.</p>"},{"location":"courses/ml/notes/w1/intro/#gender-from-images","title":"Gender from images","text":"<p>How do we represent the feature vectors? We could just concat the pixel values of the high res image but this may not work very well, because this is an overwhelming amount of information that is left to the classifier to figure out. A better first step might be to classify the image based on easier-to-determine features such as skin colour, edge detection, eyes, hair using simpler classifiers. Then, we can use the above as a 2<sup>nd</sup> feature vector to use for a gender classifier.</p>"},{"location":"courses/ml/notes/w1/perceptron/","title":"Perceptron","text":""},{"location":"courses/ml/notes/w1/perceptron/#the-ml-model","title":"The ML Model","text":"<p>A feature (col) vector \\(x = [x_1, ... x_d]^T \\in \\mathbb{R}^d\\). In the context where \\(x\\) is used as the original object, \\(\\phi(x)\\) is the feature vector constructed from it</p> <p>In each example, \\(x\\) is associated with a binary label \\(y \\in \\{-1, 1\\}\\). The training data \\(S_n\\) consists of of pairs \\((x^{(i)}, y^{(i)})\\). A classifier is a mapping from feature vectors to labels: \\(h : \\mathbb{R}^d \\rightarrow \\{-1, 1\\}\\):</p> <ol> <li>Set of classifiers \\(\\mathcal{H}\\). This is also known as the model or the hypothesis class. The larger this set is, the more powerful the ML model is. This is because there are many hypothesis on how the features relate to the labels.</li> <li>Learning algorithm/criterion. The problem of finding an \\(\\hat{h} \\in \\mathcal{H}\\) based on the trainig dataset \\(S_n\\) is solved by the learning algorithm. Many learning algorithms or corresponding estimation criteria might use the same set of classifiers \\(\\mathcal{h}\\) but select different classifiers in response to the training set.</li> <li>Generalisation. The goal of the learning algorithm is to find a classifier \\(\\hat{h} \\in \\mathcal{H}\\) which will work well on unseen examples of \\(S_n\\). A classifier that predicts all the trainig data correctly may not generalise well -&gt; potentially overfitting. The right set of classifiers must neither be too large (contains clearly different functions) or too small (it may not contain the right \\(\\hat{h}\\) at all). This is known as the model selection problem</li> </ol>"},{"location":"courses/ml/notes/w1/perceptron/#linear-classification","title":"Linear Classification","text":"<p>\\(h(x, \\theta) = sign(\\theta^T x) = sign(\\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_d x_d \\geq 0)\\)</p> <p>\\(h(x, \\theta) = +1 \\text{ if } \\theta^T x \\geq 0 \\text{ else } -1\\)</p> <p>\\(h(x, \\theta) = +1 \\text{ if } \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_d x_d \\geq 0 \\text{ else } -1\\)</p> <p>This is called a linear classifier because geometrically, a hyper (\\(d\\)) plane separates the two regions of classification. The decision boundary is the plane itself, where \\(\\theta x = 0\\)</p> <p>\\(\\mathcal{X}^{+}(\\theta) = \\{ x \\in \\mathbb{R}^d | h(x_i, \\theta) = +1\\}\\)</p> <p>\\(\\mathcal{X}^{-}(\\theta) = \\{ x \\in \\mathbb{R}^d | h(x_i, \\theta) = -1\\}\\)</p> <p>Now that we have chosen a set of classifiers (\\(\\mathbb{R}^d\\)), we need to determine a specific one by setting \\(\\theta\\). For linear classification, we can just use the one with the fewest errors over the training data, i.e. the classifier that minimises:</p> <p>\\(\\varepsilon_n (\\theta) = \\cfrac{1}{n}\\sum\\limits_{t=1}^{n}|y^{(t)} - h(x, \\theta)|/2\\)</p> <p>Choosing an algorithm that can minimise \\(\\varepsilon_n\\) is not an easy problem to solve. We consider a special case where there exists a linear classifier (through origin) that achieves 0 training error. This is also known as the realisable case. Realisability depends on the training examples and the set of classifiers that we use. For linear classifiers, the assumption is made that training examples are linearly separable through the origin</p> <p>A set of labled points \\(S_n = \\{ (x^{(t)}, y^{(t)}), t = 1, ..., n\\}\\) are linearly separable through the origin if there exists a parameter vector \\(\\hat{\\theta}\\) such that \\(y^{(t)}(\\hat{\\theta}^T x^(t)) &gt; 0\\) for all \\(t = 1, ..., n\\)</p>"},{"location":"courses/ml/notes/w1/perceptron/#perceptron-algorithm","title":"Perceptron Algorithm","text":"<p>Mistake Driven. Start with a random classifier, say \\(\\hat{\\theta} = 0\\) and successively tries to adjust the parameters, based on each training example, so as to correct any mistakes. The following is known as the update rule</p> <p>if \\(y^{(t)} \\neq h(x^{(t)}, \\theta^{(k)})\\) then</p> <p>\\(\\theta^{(k+1)} = \\theta^{(k)} + y^{(t)}x^{(t)}\\)</p> <p>The reason why this works is because, a mistake means that the sign of the label and the classifier disagree, i.e. \\(y^{(t)} ((\\theta^{(k)})^T x^{(t)}) &lt; 0\\) and lets look at the sign after the update:</p> <p>\\(y^{(t)} ((\\theta^{(k+1)})^T x^{(t)}) = y^{(t)} (\\theta^{(k)} + y^{(t)}x^{(t)})^T x^{(t)}\\)</p> <p>\\(\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; = y^{(t)} ((\\theta^{(k)})^T x^{(t)}) + (y^{(t)})^2 (x^{(t)})^T x^{(t)}\\)</p> <p>\\(\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; = y^{(t)} ((\\theta^{(k)})^T x^{(t)}) + ||x^{(t)}||^2\\)</p> <p>Thus, eventually this drives the parameter \\(\\theta\\) towards the correct value so that this particular point is classified correctly.</p> <p>This algorithm converges in the realisable case (i.e., when the training data is linearly separable through the origin)</p>"},{"location":"courses/ml/notes/w1/perceptron/#linear-classifiers-with-offset","title":"Linear Classifiers with Offset","text":"<p>\\(h(x, \\theta, \\theta_0) = sign(\\theta^T x + \\theta_0)\\)</p> <p>A set of labled points \\(S_n = \\{ (x^{(t)}, y^{(t)}), t = 1, ..., n\\}\\) are linearly separable if there exists a parameter vector \\(\\hat{\\theta}\\) such that \\(y^{(t)}(\\hat{\\theta}^T x^(t) + \\theta_0) &gt; 0\\) for all \\(t = 1, ..., n\\)</p> <p>if \\(y^{(t)} \\neq h(x^{(t)}, \\theta^{(k)}, \\theta^{(k)}_0)\\) then</p> <p>\\(\\theta^{(k+1)} = \\theta^{(k)} + y^{(t)}x^{(t)}\\)</p> <p>\\(\\theta^{(k+1)}_0 = \\theta^{(k)}_0 + y^{(t)}\\)</p> <p>Note: linear classification with an offset is really just linear classification through origin, but just with an extra dimension. each point \\(x^{(t)}\\) can be considered to have an additional coordinate that is always set to 1</p>"},{"location":"courses/ml/notes/w10/ensemble/","title":"Ensemble","text":""},{"location":"courses/ml/notes/w10/ensemble/#overfitting","title":"Overfitting","text":"<ul> <li>Model too complex</li> <li>Fit noise</li> <li>One off pattern</li> <li>Overfit if<ul> <li>another F can be found with more training errors but less test errors</li> </ul> </li> </ul>"},{"location":"courses/ml/notes/w10/ensemble/#underfitting","title":"Underfitting","text":"<ul> <li>Model too simple</li> <li>Does not capture salient patterns</li> <li>Underfit if<ul> <li>another F can be found with less training and test errors</li> </ul> </li> </ul>"},{"location":"courses/ml/notes/w10/ensemble/#generalisation-error","title":"Generalisation Error","text":"<ul> <li>Training error: \\(1/N \\sum e(t)\\)</li> <li>Generalisation error: \\(1/N \\sum e'(t)\\)<ul> <li>Optimistic approach: \\(e'(t) = e(t)\\)</li> <li>Pessimistic approach: \\(e'(t) = e(t) + 0.5\\)</li> <li>Reduced Error Pruning (REP): Use actual test data to estimate generalisation error</li> </ul> </li> </ul> <p>The actual generalisation error can be lower than the optimistic error because its entirely dependent on the test data. The pessimistic error can never be lower than the optimistic error though, since it is literally \\(n/N 0.5\\) + optimistic error (\\(n\\) = errors, \\(N\\) = total)</p>"},{"location":"courses/ml/notes/w10/ensemble/#occams-razor","title":"Occam's Razor","text":"<ul> <li>Given two models of similar generalisation error, choose the simpler one</li> <li>For complex models, there is a higher chance of overfit</li> <li>Model complexity should be included in the metrics for evaluation</li> </ul>"},{"location":"courses/ml/notes/w10/ensemble/#addressing-overfitting","title":"Addressing Overfitting","text":"<ul> <li> <p>Pre-Pruning (Early stopping rule)</p> <ul> <li>Stop the algorithm before it becomes a fully grown tree</li> <li>Typical stopping conditions for a node:<ul> <li>Stop if all instances belong to the same label</li> <li>Stop if all attribute values are the same (user majority for label)</li> <li>Stop if expanding the current node does not improve impurity measures</li> </ul> </li> </ul> </li> <li> <p>Post-Pruning</p> <ul> <li>Grow fully</li> <li>Trim the nodes bottom up, if gen error improves, replace subtree with leaf and use majority for leaf</li> </ul> </li> <li> <p>Lazy solution</p> <ul> <li>Try all depths of tree</li> </ul> </li> </ul>"},{"location":"courses/ml/notes/w10/ensemble/#ensemble-method","title":"Ensemble Method","text":"<ul> <li>Construct a set of classifiers from the training data (odd number)</li> <li>Predict class label by majority prediciton from all the classifiers</li> <li>Split training data into \\(n\\) sets, and use each to generate a DT. Combine the resulting classifiers</li> <li>\\(n\\) can be found from the elbow method? don't make \\(n\\) too big, or divided sets too small and underfitting</li> </ul>"},{"location":"courses/ml/notes/w10/ensemble/#why-does-this-work","title":"Why does this work?","text":"<p>Supposing there are \\(N = 25\\) base classifiers, and the independent error rate \\(\\varepsilon = 0.35\\), the \\(\\mathbb{P}( \\text{misclassification}) = \\mathbb{P}(X &gt; 13)\\) where \\(X\\) is the number of individual misclassification</p> <p>\\(\\mathbb{P}(\\text{misclassification}) = \\sum\\limits_{i = N//2+1}^{N} \\displaystyle \\binom{N}{i} \\varepsilon^i (1-\\varepsilon)^{N-i} = 0.06\\)</p>"},{"location":"courses/ml/notes/w10/ensemble/#bagging","title":"Bagging","text":"<ul> <li>Bootstrap Aggregation (Bagging)</li> <li>The above, but the \\(D_i\\) are generated by random sampling with replacement.</li> <li>Each sample has a prob. of \\(1 - (1-\\cfrac{1}{n})^n\\) of being selected (1 - prob. of it NOT being selected at all)</li> <li>= \\(1- \\cfrac{1}{e}\\) as \\(n \\to \\infty\\)</li> <li>Why is it not done without replacement? If its not done with replacement, there is no difference between the aggregate and the full tree because they see nothing different!</li> <li>Why repetition in the same bag is ok? The path to it created with decision trees will be the same anyway (There is a low chance of multiple records repeating in the same bag if \\(N\\) is large)</li> <li>Why can the model not be different for each bag? It can, but this doesn't fall under bagging approach, if the models are different there is no need to divide the training dataset, just use voting approach with different models on the full dataset is ok! Then why not do the same with same models? No difference in resulting models if we do this with the same model </li> </ul>"},{"location":"courses/ml/notes/w10/ensemble/#random-forest","title":"Random Forest","text":"<ul> <li>Train a lot of DT and use bagging</li> <li>Wisdom of crowd<ul> <li>Uncorrelated trees are preferred</li> </ul> </li> </ul> <p>watch</p>"},{"location":"courses/ml/notes/w10/ensemble/#boosting","title":"Boosting","text":"<ul> <li>An iterative procedure to adaptively change distribution of training data by focusing more on previously misclassified records<ul> <li>Split into \\(n\\) bags sampling randomly with replacement</li> <li>For each bag \\(i\\), Initially all points have the same weight</li> <li>Train the \\(n\\) models and classify points</li> <li>For each model, put the misclassified points into a new bag and fill the remaining with random samples from dataset (subject to boosting algo)</li> <li>Records wrongly classified will have their weight increased</li> <li>Records that are correctly classified with their weights decreased</li> <li>Retrain. New \\(\\text{model} = \\text{model}_1 + \\text{model}_2\\)</li> <li>Why is this not done on the entire dataset? That is definitely overfitting</li> </ul> </li> <li>Tendency To Overfit</li> </ul>"},{"location":"courses/ml/notes/w10/ensemble/#adaboost","title":"AdaBoost","text":"<ul> <li>\\(w(x_i, y_i) = \\cfrac{1}{n}\\)</li> <li>Initialise base classifiers \\(C_i\\)</li> <li> <p>for each classifier \\(C_i\\)</p> <ul> <li>Train \\(C_i\\)</li> <li>\\(\\varepsilon_i = \\cfrac{1}{N} \\sum\\limits_{j = 1}^{N} w_j \\delta(C_i(x_j) \\neq y_j)\\) (\\(\\delta = 1\\) if condition true)</li> <li>Compute the importance of \\(C_i\\):</li> <li>\\(\\alpha_i = \\cfrac{1}{2} ln (\\cfrac{1-\\varepsilon_i}{\\varepsilon_i})\\) (if error rate &gt; 0.5, this is negative)</li> <li>Update     \\(w_i^{(j+1)} = \\cfrac{w_i^{(j)}}{Z_j} \\begin{cases} e^{-\\alpha_i} \\text{ if correct} \\\\ e^{\\alpha_i} \\text{ if incorrect} \\end{cases}\\) (\\(e^{-\\alpha}\\) is low if correctly classified but \\(e^{\\alpha}\\) is higher on misclassification, \\(Z_j\\) is normalisation factor)</li> </ul> </li> <li> <p>Final Classifier: \\(\\arg \\max\\limits_{y} \\sum\\limits_{j = 1}^{N} \\alpha_j \\delta(C_j (x) = y)\\)</p> </li> </ul>"},{"location":"courses/ml/notes/w10/hmm/","title":"Hidden Markov Models","text":""},{"location":"courses/ml/notes/w10/hmm/#sequence-labeling","title":"Sequence Labeling","text":"<p>Noun, Verb, Article, Adjective, Noun</p> <p>eg - Faith is a fine invention</p> <p>One to one mappying between labels and input values</p> <p>\\(\\prod\\limits_{j=0}^{n} \\mathbb{P}(y_{i+1} | y_j) \\times \\prod\\limits_{j=1}^{n} \\mathbb{P}(x_i | y_j)\\)</p> <p>The first term -&gt; Transition Probabilities \\(a_{y_j, y_{j+1}}\\) The second term -&gt; Emission Probabilities \\(b_{y_j} (x_j)\\)</p>"},{"location":"courses/ml/notes/w10/hmm/#hidden-markov-model","title":"Hidden Markov Model","text":"<p>Assumption: Current state only determined by previous state, and nothing else</p> <ul> <li>An HMM is defined by a tuple: \\(\\langle \\mathcal{T}, \\mathcal{O}, \\theta \\rangle\\)<ul> <li>\\(\\mathcal{T}\\): A set of states including START and END</li> <li>\\(\\mathcal{O}\\): A set of observation symbols</li> <li>\\(\\theta\\): Transition and emission parameters \\(a_{u,v}\\) and \\(b_u (o)\\)</li> </ul> </li> </ul> <p>The probability of observing an input \"the dog the\" as a state transition \\(A B A\\) is \\(a_{START,A} b_A(\"the\") a_{A,B} b_B(\"dog\") a_{B,A} b_A(\"the\") a_{A, STOP}\\)</p>"},{"location":"courses/ml/notes/w10/hmm/#estimation","title":"Estimation","text":"<p>From actual labeled data (too computationally expensive)</p> <p>\\(a_{u,v} = \\cfrac{\\text{count}(u,v)}{count(u)}\\)</p> <p>\\(b_u(o) = \\cfrac{\\text{count}(u \\to o)}{count(u)}\\)</p>"},{"location":"courses/ml/notes/w10/hmm/#mle","title":"MLE","text":"<p>likelihood = \\(\\prod\\limits_{u,v} (a_{u, v})^{\\text{count}(u, v)} \\times \\prod\\limits_{u, o} (b_{u} (o))^{\\text{count}(u \\to o)}\\)</p> <p>log likelihood = \\(\\text{count}(u, v) \\sum\\limits_{u,v} (a_{u, v}) + \\text{count}(u \\to o)\\sum\\limits_{u, o} (b_{u} (o))\\)</p> <p>maximise!</p> <p>Answer the qn: Which label sequence is the most probable given the word sequence x?</p> <p>\\(y^* = \\argmax_y \\mathbb{P}(y | x)\\) where \\(y\\) is the label seq and \\(x\\) is the input seq</p> <p>\\(y^* = \\argmax_y \\cfrac{\\mathbb{P}(y \\cap x)}{mathbb{P}(x)}\\)</p> <p>\\(y^* = \\argmax_y \\mathbb{P}(y \\cap x)\\) (denom const wrt \\(y\\))</p> <p>Too complex to brute force: \\(\\mathcal{O}(|\\mathcal{T}^n|)\\) ($\\mathcal{T}# - # states, \\(n\\) - num words in sentence)</p> <p>Better: Finding highest scoring path in transition graph, enter DP (Viterbi Algo)</p> <p>\\(\\pi(j, u)\\) = highest scoring path to word \\(j\\) and label \\(u\\)</p> <p>\\(\\pi(0, \\text{START}) = 1\\) &amp; \\(\\pi(0, u) = 0\\)</p> <p>\\(\\pi(j+1, n) = \\max_{v \\in \\text{states}} (\\pi(j, v) \\times b_u(x_{j+1}) \\times a_{v, u})\\)</p> <p>\\(\\pi(n+1, \\text{STOP}) = \\max_{v \\in \\text{states}} (\\pi(n, v) \\times a_{v, \\text{STOP}})\\)</p> <p>store \\(v\\) as well so backtrack to find the actual path that gives the score.</p> <p>\\(\\mathcal{O}(n |\\mathcal{T}|^2)\\) complexity</p>"},{"location":"courses/ml/notes/w10/metrics/","title":"Metrics of Eval","text":""},{"location":"courses/ml/notes/w10/metrics/#accuracy","title":"Accuracy","text":"<p>Accuracy = \\(\\cfrac{TP + TF}{N}\\)</p>"},{"location":"courses/ml/notes/w10/metrics/#limitations","title":"Limitations","text":"<ol> <li>Class problem where # of class 0 = 9990 and # of class 1 = 10</li> <li>If everything is predicted to be class 0, accuracy is \\(9990/10000 = 0.999\\) =&gt; misleading!</li> </ol>"},{"location":"courses/ml/notes/w10/metrics/#cost-matrix","title":"Cost matrix","text":"<p>\\(c(i|j) =\\) cost of classifying \\(i\\) as \\(j\\)</p> <p>Cost = weighted accuracy from cost matrix</p>"},{"location":"courses/ml/notes/w10/metrics/#precision","title":"Precision","text":"<p>All correct positives over total positives</p> <p>Precision = \\(\\cfrac{TP}{TP + FP}\\)</p>"},{"location":"courses/ml/notes/w10/metrics/#recall","title":"Recall","text":"<p>All correct positives over all correct classifications</p> <p>Recall = \\(\\cfrac{TP}{TP + FN}\\)</p>"},{"location":"courses/ml/notes/w10/metrics/#f1-measure","title":"F1 Measure","text":"<p>Recall = \\(\\cfrac{2rp}{r + p} = \\cfrac{2 \\times TP}{2 \\times TP + FP + FN}\\)</p>"},{"location":"courses/ml/notes/w10/metrics/#methods-of-perf-eval","title":"Methods of perf eval","text":"<p>Depends on:</p> <ul> <li>Class distribution</li> <li>Cost of misclassification</li> <li> <p>Size of train/test sets</p> </li> <li> <p>Train: dataset used to train</p> </li> <li>Validation: dataset used to tune hyperparams</li> <li>Test: dataset used to test final model</li> </ul>"},{"location":"courses/ml/notes/w10/metrics/#methods-of-estimation","title":"Methods of Estimation","text":"<ul> <li>\u2154 train \u2153 test</li> <li>k-fold cross validation (average/majority of all the k runs) used to tune hyper params, choose model, validate significance of one model</li> <li>Leave one-out (LOO) cross validation</li> <li>Random subsampling - k fold cross validation but instead of contiguous split, choose randomly (w/o replacement) each time</li> </ul>"},{"location":"courses/ml/notes/w2/hinge_loss/","title":"Hinge Loss","text":""},{"location":"courses/ml/notes/w2/hinge_loss/#non-linearly-separable-case","title":"Non Linearly Separable Case","text":"<p>Empirical Risk</p> <p>\\(R_n (\\theta) = \\cfrac{1}{n}\\sum\\limits_{t=1}^{n} \\text{Loss}(y^{(t)} (\\theta^T x^{(t)}))\\)</p> <p>In our original linearly separable case, we chose</p> <p>\\(\\text{Loss}(y^{(t)} (\\theta^T x^{(t)})) = |y^{(t)} - y^{(t)} (\\theta^T x^{(t)})|/2\\)</p> <p>This is known as zero-one loss. We will change the metric of loss evaluation to hinge loss,</p> <p>\\(\\text{Loss}(y^{(t)} (\\theta^T x^{(t)})) = max\\{1 - y^{(t)} (\\theta^T x^{(t)}), 0\\}\\)</p> <p>here, \\(z = y^{(t)} (\\theta^T x^{(t)})\\) is called the agreement</p> <p>This forces the classifier's predictions to be more than just correct. Basically, in zero-one loss, the loss was binary, but now its a continuous range. This function happens to be convex, so we can carry out minimisation using gradient descent.</p>"},{"location":"courses/ml/notes/w2/hinge_loss/#sub-gradient-descent","title":"Sub Gradient Descent","text":"<p>\\(\\nabla_{\\theta} R_n(\\theta) = \\left[\\cfrac{\\partial R_n(\\theta)}{\\partial\\theta_1}, ..., \\cfrac{\\partial R_n(\\theta)}{\\partial\\theta_d}\\right]^T\\)</p> <p>\\(\\theta^{(k+1)} = \\theta^{(k)} - \\eta_k \\nabla_{\\theta} R_n (\\theta^{(k)})\\)</p> <p>\\(\\eta_k\\) is known as the step size or learning rate</p> <p>The function \\(R_n(\\theta)\\) is a piece-wise function - there are points where it is not differentiable, because we have multiple possible gradients (derivative from multiple directions are different). At these points, any one of the possible gradients can be chosen for our algorithm</p>"},{"location":"courses/ml/notes/w2/hinge_loss/#stochastic-sub-gradient-descent","title":"Stochastic Sub Gradient Descent","text":"<p>When there are many training examples, we can select one at random and perform a parameter update on the basis of the corresponding loss alone (similar to the perceptron rule). If we take small enough steps, these stochastic updates, in aggregate, move the parameters roughly in the same direction as the gradient.</p> <p>\\(\\theta^{(k+1)} = \\theta^{(k)} - \\eta_k \\nabla_{\\theta} R_n (\\theta^{(k)})\\)</p> <p>Here, the \\(R_n(\\theta^{(k)})\\) is just for one point, simply: \\(\\text{Loss}(y^{(t)}((\\theta^{(t)})^T x^{(t)}))\\)</p> <p>\\(\\nabla_{\\theta} R_n (\\theta^{(k)}) = \\nabla_{\\theta} \\text{Loss}(y^{(t)}((\\theta^{(t)})^T x^{(t)}))\\)</p> <p>\\(\\nabla_{\\theta} R_n (\\theta^{(k)}) = \\nabla_{\\theta} (1 - y^{(t)}((\\theta^{(t)})^T x^{(t)}))\\)</p> <p>\\(\\nabla_{\\theta} R_n (\\theta^{(k)}) = - y^{(t)} x^{(t)}\\)</p> <p>if \\(y^{(t)}((\\theta^{(t)})^T x^{(t)}) \\leq 1\\) then</p> <p>\\(\\theta^{(k+1)} = \\theta^{(k)} + \\eta_k y^{(t)} x^{(t)}\\)</p> <p>The differences from the perceptron algorithm are:</p> <ol> <li>The mistakes are penalised linearly, instead of in a binary fassion</li> <li>Here, the \\(\\eta_k\\) is decreased over the iterations, instead of being fixed</li> <li>Additionally, the \"mistake\", i.e. when the update is made is now defined in terms of \\(z \\leq 1\\) instead of \\(z &lt; 0\\)</li> <li>The training points are chosen at random, than cycling through them in order, to prevent the updates from oscillating</li> </ol> <p>\\(\\eta_k = \\cfrac{1}{k+1}\\) is a choice of \\(\\eta_k\\) that ensures that the SSGD converges. Any choice that satisfies \\(\\sum\\limits_{k=1}^{\\infty} \\eta_k^2 &lt; \\infty\\) and \\(\\sum\\limits_{k=1}^{\\infty} \\eta_k = \\infty\\) makes the algorithm converge</p>"},{"location":"courses/ml/notes/w2/regression/","title":"Regression","text":"<p>When choosing a model, the following things need to be considered:</p> <ol> <li>How are the errors measured? What is the criterion for choosing \\(\\hat{\\theta}\\) and \\(\\hat{\\theta_0}\\)</li> <li>What algorithm can be used to optimise the training criterion? How does the algorithm scale with dimension of feature vectors and size of training set?</li> <li>When the size of the dataset is not large enough in relation to the number of dimensions, there may be degrees of freedom (directions in the parameter space) that remain unconstrained. How do we set those DoF? This is a part of a bigger problem called regularisation. The question is how to softly constrain the set of functions \\(\\mathcal{F}\\) to achieve a better generalisation</li> </ol>"},{"location":"courses/ml/notes/w2/regression/#linear-regression","title":"Linear Regression","text":"<p>A linear regression function is a linear function of feature vectors:</p> <p>\\(f(x, \\theta, \\theta_0) = \\theta^T x + \\theta_0 = \\sum\\limits_{i=1}^{d} \\theta_i x_i + \\theta_0\\)</p> <p>For different choices of \\(\\theta \\in \\mathbb{R}^d, \\theta_0 \\in \\mathbb{R}\\) gives rise to a set of functions \\(\\mathcal{F}\\). The power of the regression algorithm lies in selecting appropriate feature vectors to use as input to these functions, since we have 100% control over how the feature vectors are constructed.</p> <p>Assuming a proper feature vector representation of the data has been found:</p> <p>\\(R_n (\\theta) = \\cfrac{1}{n}\\sum\\limits_{t=1}^{n} \\text{Loss}(y^{(t)} - \\theta^T x^{(t)})\\)</p> <p>Use square error as loss:</p> <p>\\(\\text{Loss}(y^{(t)} - \\theta^T x^{(t)}) = (y^{(t)} - \\theta^T x^{(t)})^2/2\\)</p> <ol> <li>Estimation Error: Small dataset leads to high estimation error (variance) and overfitting, where we model something linear as more complex</li> <li>Structural Error: Having a small set of functions \\(\\mathcal{F}\\) leads to high structural error (bias) and underfitting, where we model something more complex as something linear.</li> </ol> <p>In order to reduce structural error, we need to use a large set of functions \\(\\mathcal{F}\\) but this means that if the training dataset is noisy \\(S_n\\), it is harder to select the correct function \\(f\\) from a larger set \\(\\mathcal{F}\\). Thus striking the right balance between the two is an essential part of ML problems!</p> <p>The parameter \\(\\hat{\\theta}\\) can be viewed as a function of \\(S_n\\)</p>"},{"location":"courses/ml/notes/w2/regression/#optimising-least-squares-criterion-with-gradient-descent","title":"Optimising Least Squares Criterion With Gradient Descent","text":"<p>stochastic gradient descent may be used to minimise \\(R_n\\). In this case, \\(R_n(\\theta)\\) is differentiable everywhere.</p> <p>\\(\\nabla_{\\theta} (y^{(t)} - \\theta^T x^{(t)})^2/2 = (y^{(t)} - \\theta^T x^{(t)}) \\nabla_{\\theta} (y^{(t)} - \\theta^T x^{(t)})\\)</p> <p>\\(= - (y^{(t)} - \\theta^T x^{(t)}) x^{(t)}\\)</p> <p>Thus, the algo becomes</p> <ol> <li>Set \\(\\theta^{(0)} = 0\\)</li> <li>Pick \\(t \\in \\{1, 2, ..., n\\}\\)</li> <li>\\(\\theta^{(k+1)} = \\theta^{(k)} + \\eta_k (y^{(t)} - \\theta^T x^{(t)})x^{(t)}\\)</li> </ol> <p>Once we go through the entire dataset once, we call it ONE epoch</p> <p>Here, \\(\\eta_k = \\cfrac{1}{k+1}\\)</p>"},{"location":"courses/ml/notes/w2/regression/#closed-form-optimal-solution-for-least-squares-criterion","title":"Closed Form Optimal Solution For Least Squares Criterion","text":"<p>We can try to find a closed form solution by setting the gradient to 0 and solving for \\(\\hat{\\theta}\\)</p> \\[\\nabla_{\\theta} R_n(\\hat{\\theta}) = \\cfrac{1}{n}\\sum\\limits_{t=1}^{n} - (y^{(t)} - \\hat{\\theta}^T x^{(t)}) x^{(t)}\\] \\[= -\\cfrac{1}{n}\\sum\\limits_{t=1}^{n} y^{(t)} x^{(t)} + \\cfrac{1}{n}\\sum\\limits_{t=1}^{n} \\hat{\\theta}^T x^{(t)} x^{(t)}\\] \\[= -\\cfrac{1}{n}\\sum\\limits_{t=1}^{n} y^{(t)} x^{(t)} + \\cfrac{1}{n}\\sum\\limits_{t=1}^{n} x^{(t)} (x^{(t)})^T \\hat{\\theta}\\] <p>Remember, \\(y^{(t)}\\) is a scalar and \\(x^{(t)}\\) is a column vector</p> \\[= -\\cfrac{1}{n} \\begin{bmatrix}     x^{(1)} &amp; x^{(2)} &amp; ... &amp; x^{(n)} \\end{bmatrix} \\begin{bmatrix}     y^{(1)} \\\\     y^{(2)} \\\\     ... \\\\     y^{(n)} \\\\ \\end{bmatrix} + \\cfrac{1}{n} \\begin{bmatrix}     x^{(1)} &amp; x^{(2)} &amp; ... &amp; x^{(n)} \\end{bmatrix} \\begin{bmatrix}     (x^{(1)})^T \\\\     (x^{(2)})^T \\\\     ... \\\\     (x^{(n)})^T \\\\ \\end{bmatrix} \\hat{\\theta}\\] \\[= -b + A \\hat{\\theta}\\] <p>where \\(b = \\cfrac{1}{n}X \\vec{y}\\) and \\(A = \\cfrac{1}{n}X X^T\\)</p> <p>and \\(X\\) is a \\(d \\times n\\) matrix</p> \\[X = \\begin{bmatrix} x^{(1)} &amp; x^{(2)} &amp; ... &amp; x^{(n)} \\end{bmatrix}\\] <p>If A is invertible, we get \\(\\hat{\\theta} = A^{-1}b\\)</p> <p>Inverting \\(A\\) is about \\(\\mathcal{O}(d^3)\\) time complexity, hence when \\(d\\) is large, we opt for stochastic gradient descent.</p>"},{"location":"courses/ml/notes/w2/regression/#ridge-regression","title":"Ridge Regression","text":"<p>When \\(A\\) is not invertible, we say that the learning problem is ill posed. This usually happens when we don't have enough data points in comparison to the dimension of the data points. So how do we set the parameter \\(\\theta\\) in this case?</p> <p>The estimation criterion is modified by adding a regularisation term. The purpose of this term is to bias the parameters towards a default answer such as zero. The regularisation will resist moving the parameters away from zero, even if the data may weakly indicate otherwise. This helps ensure that the predictions generalise well. The intuition is that we want to go for the \"simplest\" answer when sufficient evidence is not present.</p> <p>Among the many different choices of regularisation terms, \\(||\\theta||^2/2\\) can be used, as it keeps the problem easily solvable.</p> <p>\\(J_{n, \\lambda} = \\cfrac{\\lambda ||\\theta||^2}{2} + R_n(\\theta)\\). Here, \\(\\lambda\\) is known as the regularisation parameter. It controls the \"trade off\" between keeping the parameters close to 0 and actually fitting them to the data. Using this as our objective function instead of \\(R_n(\\theta)\\) gives us ridge regression</p> <p>\\(\\theta^{(k+1)} = (1 - \\lambda \\eta_k) \\theta^{(k)} + \\eta_k (y^{(t)} - \\theta^T x^{(t)})x^{(t)}\\)</p> <p>A convex curve is obtained as we increas \\(\\lambda\\) from \\(0\\)</p>"},{"location":"courses/ml/notes/w3/kmeans/","title":"K Means","text":""},{"location":"courses/ml/notes/w3/kmeans/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>When there are no labels \\(y^{(i)}\\) to classify the data points accordingly, we use unsupervised learning. Instead of classification, the goal of unsupervised learning is to identify structures/patterns in the data.</p> <p>Input: \\(S_n = \\{ x^{(i)}, 1, ..., n \\}\\) where \\(x^{(i)} \\in \\mathbb{R}^d\\)</p> <p>integer \\(k\\)</p> <p>Output: Set of clusters \\(C_1, ..., C_k\\)</p> <p>cosine similarty:</p> <p>\\(cos(x, y) = \\cfrac{x . y}{||x|| \\; ||y||}\\)</p> <p>dissimilarity (pairwise euclidian distance)</p> <p>\\(dist(x, y) = ||x - y||^2\\)</p> <p>Sometimes, the L1/manhattan distance can be used:</p> <p>\\(dist(x, y) = ||x - y||_1\\)</p> <p>distortion within one cluster \\(C_j\\):</p> <p>\\(\\sum\\limits_{x^{(i)} \\in C_j}||x^{(i)} - z^{(j)}||^2\\)</p> <p>cost of the clustering:</p> <p>\\(\\text{cost}(C_1, ... C_k, z^{(1)}, ... z^{(k)}) = \\sum\\limits_{j = 1}^{k} \\sum\\limits_{x^{(i)} \\in C_j}||x^{(i)} - z^{(j)}||^2\\)</p> <p>\\(\\text{cost}(z^{(1)}, ... z^{(k)}) = \\min\\limits_{C_1, ..., C_k} \\text{cost}(C_1, ... C_k, z^{(1)}, ... z^{(k)})\\)</p> <p>\\(\\text{cost}(z^{(1)}, ... z^{(k)}) = \\min\\limits_{C_1, ..., C_k} \\sum\\limits_{j = 1}^{k} \\sum\\limits_{x^{(i)} \\in C_j}||x^{(i)} - z^{(j)}||^2\\)</p> <p>\\(\\text{cost}(z^{(1)}, ... z^{(k)}) = \\sum\\limits_{i = 1, ..., n} \\min\\limits_{j = 1, ..., k} ||x^{(i)} - z^{(j)}||^2\\)</p> <p>Basically, for every point in the dataset, choose the centroid from the clusters that is the closest to the point \\(i\\)</p>"},{"location":"courses/ml/notes/w3/kmeans/#k-means-algorithm","title":"K-Means Algorithm","text":"<p>Alternatively find the best clusters for centroids, then the best centroids for the clusters. Iterative algorithm:</p> <ol> <li>Randomly initialise \\(k\\) centroids</li> <li>Repeat until no further change in cost:</li> <li>\\(\\forall j = 1, ..., k, C_j = \\{ i |  x^{(i)} \\text{ is closest to } z^{(j)}\\}\\)</li> <li>\\(\\forall j = 1, ..., k, z^{(j)} = \\frac{1}{|C_j|} \\sum\\limits_{x^{(i)} \\in C_j} x^{(i)}\\) (cluster mean)</li> </ol> <p>Each iteration requires \\(\\mathcal{O}(kn)\\) steps</p>"},{"location":"courses/ml/notes/w3/kmeans/#convergence","title":"Convergence","text":"<p>Each iteration of K-Means either keeps the cost the same or necessarily lowers it. This is because in the first step, we are choosing members of the clusters based on fixed \\(z^{(i)}\\) points, and since this choosing is done by assigning points to the cluster of the closest centroid, it minimises our cost by definition. In the second iteration, a new centroid is chosen after fixing the clusters, which boils down to minimising the SSW of one cluster. The solution of this minimsation problem is the formula for the centroid given in step 2!</p>"},{"location":"courses/ml/notes/w3/kmeans/#k-mediods-algorithm","title":"K-Mediods Algorithm","text":"<p>This is the same as K-Means, but the main difference is that the cluster representative is not the cluster mean but one of the existing data points in the cluster, called an exemplar. This is useful in cases such as clustering news articles. Blending multiple news articles into a mean may not make a lot of sense, instead we'd want one concrete article to represent the entire cluster. We can also use other metrics than the euclidian 2norm for calculating the distance between two points:</p> <p>\\(\\text{cost}(C_1, ... C_k, z^{(1)}, ... z^{(k)}) = \\sum\\limits_{j = 1}^{k} \\sum\\limits_{x^{(i)} \\in C_j}d(x^{(i)} - z^{(j)})\\)</p> <p>The algorithm:</p> <ol> <li>Randomly initialise \\(k\\) exemplars</li> <li>Repeat until no further change in cost:</li> <li>\\(\\forall j = 1, ..., k, C_j = \\{ i |  x^{(i)} \\text{ is closest to } z^{(j)}\\}\\)</li> <li>\\(\\forall j = 1, ..., k, z^{(j)} = x^{(i)} \\in C_j | \\min\\limits_{x^{(i)} \\in C_j} d(x^{(i)}, z^{(j)})\\)</li> </ol>"},{"location":"courses/ml/notes/w3/kmeans/#practical-issues","title":"Practical Issues","text":"<p>These algorithms guarantee that we find a local min of the cost function, not necessarily the optimum. The quality of the clustering can greatly depend on the initialisation. Usually, the goal is to assign the starting centroids such that they are as far away from each other as possible.</p> <p>Elbow method is used to select the optimal value of \\(k\\). The minimum description length principle (casting clustering as a connection problem) or Gap statistics (characterising how much our cost would descrease if there are no additional cluster sections exists). We will consider the question of choosing \\(k\\) in a scenario where we use clustering to help semi supervised learning.</p>"},{"location":"courses/ml/notes/w3/kmeans/#k-fold-cross-validation","title":"K-Fold Cross Validation","text":"<p>In semi supervised learning problems, we have access to a small set of labeled data and a large set of unlabeled data. When there are few input points and the dimensionality of the feature vectors is high, a linear classifier would overfit, but we can use the unlabeled data to reduce the dimensionality of the feature vectors.</p> <p>Consider that our task involves labeling articles if they contain a specific topic, say 'ecology'. In the typical bag of words approach to map documents to feature vectors, the dimensionality of the vectors would be about the size of the english dictionary. However, we can take advantage of the unlabeled data by clustering them into semantically coherent groups. The clustering does not tell us which topics each arcticle contains, but it puts similar documents together, hopefully placing documents involving different topics in different clusters. If so, knowing which cluster a document belongs to can help us simplify its classification.</p> <p>The bag of words feature vector is replaced with a vector that contains the above clustering information. More precisely, given \\(k\\) clusters, a doc that belongs to cluster \\(j\\) can be repr-ed by a \\(k\\)-dim vector with the \\(j\\)th coord set to \\(1\\). All docs in the same cluster have the same feature vector this way! A better method is to use the relative distances of the document to the \\(k\\) clusters. In either case, we can obtain a low dim feature vector which can be used for topic classification. At least lower dimensionality means less chances of overfitting.</p> <p>How do we choose \\(k\\) in this case? We use cross validation from the datapoints which are labeled, with the \\(k\\) dimensional feature vectors - to get a sense of how well the prcess generalises. The value of \\(k\\) that minimises the cross validation error would be used.</p>"},{"location":"courses/ml/notes/w4/log_reg/","title":"Logistic Regression","text":"<p>Logistic regression is another linear model that outputs a probability (confidence score) between 0 and 1. The output is real and bounded. Logistic regression can be used to make predictions about something with a certain probability (instead of a binary classification), for lets say the probability diabetes occuring in a person depending on their BP, height, weight, age, etc.</p>"},{"location":"courses/ml/notes/w4/log_reg/#hypothesis-function","title":"Hypothesis Function","text":"<p>The output of logistic regression is given by pluggin in the original prediciton function into a sigmoid function:</p> <p>\\(h(x) = \\sigma(\\theta^T x + \\theta_0) = \\cfrac{\\exp(\\theta^T x + \\theta_0)}{1 + \\exp(\\theta^T x + \\theta_0)}\\)</p> <p>Here, \\(P(y | x) = \\begin{cases} h(x) &amp; y = 1 \\\\ 1 - h(x) &amp; y = -1\\end{cases}\\)</p> <p>due to a property of the sigmoid function where \\(\\sigma(-s) = 1 - \\sigma(x)\\) we can rewrite the probability as:</p> <p>\\(P(y | x) = \\sigma(y (\\theta^T x + \\theta_0))\\)</p>"},{"location":"courses/ml/notes/w4/log_reg/#maximum-likelyhood-estimation","title":"Maximum Likelyhood Estimation","text":"<p>The probability of predicting all the training examples in a set \\(S_n\\) are thus given by \\(p = \\prod\\limits_{i} P(y_i|x_i)\\) This is the function that we will be maximising.</p> <p>Maximising this is equivalent to maximising \\(\\cfrac{\\ln(p)}{n}\\). This is because \\(\\ln\\) is a monotonically increasing function:</p> <p>Maximise: \\(\\cfrac{1}{n} \\sum\\limits_{i = 1}^{n} \\ln(P(y_i | x_i))\\)</p> <p>Minimise: \\(J(\\theta) = \\cfrac{1}{n} \\sum\\limits_{i = 1}^{n} \\ln(1/P(y_i | x_i))\\)</p> <p>Minimise: \\(J(\\theta) = \\cfrac{1}{n} \\sum\\limits_{i = 1}^{n} \\ln(1 + \\exp(-y_i (\\theta^T x_i + \\theta_0 )))\\)</p> <p>The benifit of doing this is that deriving a sum is much more easy than deriving a product.</p> <p>We can use both regular gradient descent or stochastic gradient descent to find the minima of this cost.</p>"},{"location":"courses/ml/notes/w4/log_reg/#stochastic-gd","title":"Stochastic GD","text":"<p>\\(e_t(\\theta) = \\ln(1 + e^{\\displaystyle -y_t \\theta^T x_t})\\)</p> <p>\\(\\nabla_{\\theta} e_t(\\theta) = \\cfrac{1}{(1 + e^{\\displaystyle -y_t \\theta^T x_t})} \\times e^{\\displaystyle -y_t \\theta^T x_t} (-y_t \\theta^T x_t)\\)</p> <p>\\(\\nabla_{\\theta} e_t(\\theta) = \\cfrac{-y_t \\theta^T x_t}{(1 + e^{\\displaystyle y_t \\theta^T x_t})}\\)</p> <p>Then the weight update is:</p> <p>\\(\\theta_{t+1} = \\theta_t - \\eta \\nabla_{\\theta} e_t (\\theta)\\) </p>"},{"location":"courses/ml/notes/w4/log_reg/#prediction","title":"Prediction","text":"<p>Our prediction is \\(1\\) iff:</p> <ol> <li>\\(P(y = +1|x) \\geq 0.5\\)</li> <li>\\(P(y = +1|x) &gt; P(y = -1|x) \\implies P(y = +1|x)/P(y = -1|x) \\geq 1\\)</li> <li>\\(\\theta^T x \\geq 0\\) (by taking \\(\\ln\\) of the above inequality)</li> </ol>"},{"location":"courses/ml/notes/w4/svm/","title":"Support Vector Machines","text":"<p>When our training data is perfectly linearly separable, there are an infinite set of hyperplanes to choose a perfect classifier from. Which of these is the most \"optimal\" classifier? Assuming that real data is noiser than the training data, it would be sensible to say that an optimal classifier should:</p> <ol> <li>Classify all training data perfectly</li> <li>Be \"maximally removed\" from all the training data. (What would be better? a boundary that cuts the classification close for a lot of points or one that classifies them with sufficient distance?)</li> </ol>"},{"location":"courses/ml/notes/w4/svm/#maximum-margin-classification","title":"Maximum Margin Classification","text":"<p>We can measure the distance of each point from the hyperplane using:</p> <p>\\(\\gamma^{(t)}(\\theta, \\theta_0) = \\cfrac{y^{(t)}(\\theta^T x^{(t)} + \\theta_0)}{||\\theta||}\\)</p> <p>This is the geometric margin (of the point \\(t\\)), the functional margin is geometric margin times \\(||\\theta||\\)</p> <p>The geometric margin of the entire training set is \\(\\min\\limits_{t = 1, ..., n}\\gamma^{(t)}\\)</p> <p>the function \\(h(x) = \\theta^T x + \\theta_0\\) is known as the discriminant function</p> <p>So our problem of finding a \"maximally removed\" classifier is equivalent to maximising this geometric margin.</p> <p>Note that maximising this geometric margin is equivalent to minimising \\(||\\theta||\\) which is equivalent to minimising \\(\\cfrac{1}{2} ||\\theta||^2\\). Additionally, this minimisation problem is subject to the constraints that \\(y^{(t)}(\\theta^T x^{(t)} + \\theta_0) \\geq 1 \\; \\forall \\; t\\). The choice for the \\(1\\) on the right side of the constraint is arbitrary, any positive number would suffice (it implies that all points must be classified correctly)</p> <p>If we two points exactly on the two marins \\(\\theta^T x_+ + \\theta_0 = 1\\) and \\(\\theta^T x_- + \\theta_0 = -1\\), then, the difference vector between the two points can be obtained by: \\(x_+ - x_-\\). Now to get the width of the margin, we project it along the normal direction to the hyperplane:</p> <p>\\(\\cfrac{(x_+ - x_-) \\cdot \\theta}{||\\theta||}\\)</p> <p>\\(\\cfrac{(1-\\theta_0 - (-1-\\theta_0))}{||\\theta||}\\)</p> <p>\\(\\cfrac{(1-\\theta_0 +1+\\theta_0))}{||\\theta||}\\)</p> <p>\\(\\cfrac{2}{||\\theta||}\\)</p> <p>Once again, to maximise this (the width of the margin) we can minimise \\(||\\theta||\\), to which end we can:</p> <p>Primal Form: Minimise: \\(\\cfrac{||\\theta||^2}{2}\\), subject to \\(y^{(t)}(\\theta^T x^{(t)} + \\theta_0) \\geq 1\\)</p> <p>Enter KKT (dual):</p> <p>(Remember the complementary slackness condition and the dual feasibility condition imposed by KKT on the solution)</p> <p>\\(L = \\cfrac{||\\theta||^2}{2} - \\sum\\limits_{i = 1}^{n} \\alpha_i \\left(y_i (\\theta^T \\vec{x_i} + \\theta_0) - 1 \\right)\\)</p> <p>Convert to vector form:</p> <p>\\(L = \\cfrac{\\theta^T\\theta}{2} - \\alpha^T \\left(Y (X^T \\theta + \\theta_0 \\vec{1}) - \\vec{1}\\right)\\)</p> <p>here, \\(X\\) is a \\(d \\times n\\) matrix whose columns are the feature vectors, and \\(Y\\) is an \\(n \\times n\\) matrix where the diagonals are components of \\(\\vec{y}\\)</p> <p>\\(\\cfrac{\\partial L}{\\partial \\theta} = \\theta - X Y \\alpha = 0\\) (remember \\(Y^T = Y\\) in this case)</p> <p>\\(\\theta = X Y \\alpha \\;\\; ... \\;\\;\\;\\; (i)\\)</p> <p>non vector form: \\(\\vec{\\theta} = \\sum\\limits_{i} \\alpha_i y_i \\vec{x_i}\\)</p> <p>\\(\\cfrac{\\partial L}{\\partial \\theta_0} = \\alpha^T Y \\vec{1} = 0\\)</p> <p>\\(\\alpha^T Y \\vec{1} = 0 \\;\\ ... \\;\\;\\;\\; (ii)\\)</p> <p>non vector form: \\(\\sum\\limits_{i}\\alpha_i y_i  = 0\\)</p> <p>use \\((i)\\) and \\((ii)\\) in:</p> <p>\\(L = \\cfrac{\\theta^T\\theta}{2} - \\alpha^T \\left( Y (X^T \\theta + \\theta_0 \\vec{1}) - \\vec{1}\\right)\\)</p> <p>\\(L = \\cfrac{\\left( X Y \\alpha \\right)^T X Y \\alpha }{2} - \\alpha^T \\left( Y X^T X Y \\alpha - \\vec{1}\\right)\\)</p> <p>\\(L = \\cfrac{\\alpha^T Y X^T  X Y \\alpha }{2} - \\alpha^T \\left( Y X^T X Y \\alpha - \\vec{1}\\right)\\)</p> <p>\\(L = \\cfrac{\\alpha^T Y X^T  X Y \\alpha }{2} - \\alpha^T Y X^T X Y \\alpha + \\alpha^T\\vec{1}\\)</p> <p>\\(L = \\alpha^T\\vec{1} - \\cfrac{\\alpha^T Y X^T  X Y \\alpha }{2}\\)</p> <p>Non vector form:</p> <p>\\(L = \\sum\\limits_{i} \\alpha_i - \\cfrac{1}{2} \\sum\\limits_{i}\\sum\\limits_{j} \\alpha_i \\alpha_j y_i y_j \\vec{x_i}^T \\vec{x_j}\\)</p> <p>Since \\(\\theta\\) is a linear combination of some of the vectors \\(\\vec{x_i}\\) for which \\(\\alpha_i &gt; 0\\) and \\(y_i \\theta^T x_i = 1\\), they are called support vectors</p> <p>The rest are non support vectors since \\(\\alpha_i = 0\\) and \\(y_i \\theta^T x_i &gt; 1\\) and thus do not contribute to \\(\\theta\\)</p>"},{"location":"courses/ml/notes/w4/svm/#soft-margin-classification","title":"Soft Margin Classification","text":"<p>In the real world, data is often not nice and not linearly separable. In that case, a hard maximal margin classifier will not know what to do. Thus, to still be able to make classification work on non linearly separable data, we can allow some points (some outliers) to be misclassified by the classifier. This however should be reflected by some additional penality in our cost function. After all, we want to reduce misclassification as much as possible.</p> <p>To make this work, our original optimisation problem is modified in the following way:</p> <p>Primal Form: Minimise: \\(\\cfrac{\\lambda}{2}||\\theta||^2 + \\sum\\limits_{t}\\xi^{(t)}\\), subject to \\(y^{(t)}(\\theta^T x^{(t)} + \\theta_0) \\geq 1 - \\xi^{(t)}\\) and \\(\\xi^{(t)} \\geq 0\\)</p> <p>Here, \\(\\lambda\\) is a regularisation parameter. Increasing its value means we give more importance to minimising \\(||\\theta||\\) and lesser importance to actually satisfy the classification constraints, since if we increase \\(\\lambda\\), the \\(||\\theta||\\) term dominates the expression</p> <p>Enter KKT (dual):</p> <p>\\(L = \\cfrac{\\lambda}{2}||\\theta||^2 + \\sum\\limits_{i = 1}^{n} \\xi_i - \\sum\\limits_{i = 1}^{n} \\alpha_i \\left(y_i (\\theta^T \\vec{x_i} + \\theta_0) - 1 + \\xi_i \\right) - \\sum\\limits_{i = 1}^n \\mu_i \\xi_i\\)</p> <p>Convert to vector form:</p> <p>\\(L = \\cfrac{\\lambda}{2}\\theta^T\\theta + \\xi^T \\vec{1} - \\alpha^T \\left(Y (X^T \\theta + \\theta_0 \\vec{1}) - \\vec{1} + \\xi \\right) - \\mu^T \\xi\\)</p> <p>\\(\\cfrac{\\partial L}{\\partial \\theta} = \\lambda\\theta - X Y \\alpha = 0\\)</p> <p>\\(\\theta = \\cfrac{X Y \\alpha}{\\lambda} \\;\\; ... \\;\\;\\;\\; (i)\\)</p> <p>non vector form: \\(\\vec{\\theta} = \\cfrac{1}{\\lambda} \\sum\\limits_{i} \\alpha_i y_i \\vec{x_i}\\)</p> <p>\\(\\cfrac{\\partial L}{\\partial \\theta_0} = \\alpha^T Y \\vec{1} = 0\\)</p> <p>\\(\\alpha^T Y \\vec{1} = 0 \\;\\ ... \\;\\;\\;\\; (ii)\\)</p> <p>non vector form: \\(\\sum\\limits_{i}\\alpha_i y_i  = 0\\)</p> <p>\\(\\cfrac{\\partial L}{\\partial \\xi} = \\vec{1}^T - \\alpha^T - \\mu^T = 0\\)</p> <p>\\(\\alpha = \\vec{1} - \\mu \\;\\ ... \\;\\;\\;\\; (iii)\\)</p> <p>non vector form: \\(\\alpha_i = 1 - \\mu_i\\)</p> <p>Using these relations in the Lagrangian:</p> <p>\\(L = \\cfrac{\\alpha^T Y X^T X Y \\alpha}{2\\lambda} + \\xi^T \\vec{1} - \\alpha^T Y (\\cfrac{X^T X Y \\alpha}{\\lambda} + \\theta_0 \\vec{1}) + \\alpha^T \\vec{1} - \\alpha^T \\xi - \\mu^T \\xi\\)</p> <p>\\(L = \\cfrac{\\alpha^T Y X^T X Y \\alpha}{2\\lambda} + \\xi^T \\vec{1} - \\cfrac{\\alpha^T Y X^T X Y \\alpha}{\\lambda} + \\alpha^T \\vec{1} - (\\vec{1} - \\mu)^T \\xi - \\mu^T \\xi\\)</p> <p>\\(L = \\alpha^T \\vec{1} - \\cfrac{\\alpha^T Y X^T X Y \\alpha}{2\\lambda}\\)</p> <p>By KKT, we have:</p> <p>\\(\\alpha \\in (0, 1/\\lambda)\\) for SV and \\(\\alpha = 1/\\lambda\\) for margin violations</p>"},{"location":"courses/ml/notes/w4/svm/#the-kernel-method","title":"The Kernel Method","text":"<p>What if we have data that is linearly inseparable and there is no clear way to use soft margin classification effectively? Enter the kernel method.</p> <p>The motivation behind this method is that we want to map the feature vectors from their feature space into a higher dimension where they become linearly separable. This is useful because there is a theorem called Cover's Theorem which tells us that when lower dimensional data is mapped into a higher dimension, the probability of it becoming linearly separable is high.</p> <p>For example, consider a 2D vector \\(x = \\begin{bmatrix} x_1 &amp; x_2 \\end{bmatrix}^T\\) we can use a function \\(\\phi(x) = \\begin{bmatrix} x_1^2 &amp; x_2^2 &amp; \\sqrt{2} x_1 x_2 \\end{bmatrix}^T\\) to map it into 3D. Now we can use \\(\\phi(x)\\) in our dual problem instead of \\(x\\) since that becomes our input vector and find a linear classifier in 3D!</p> <p>There is however an issue with this approach that is not apparent with this small example. Consider if \\(x\\) had 1000 components, the equivalent transformation for it into higher dimenson may result in a vector that has \\(1000 + \\displaystyle\\binom{1000}{2} \\approx 500k\\) dimensions! This is not feasable because working with a vector which is that big is not feasable.</p> <p>There are however two things that help our case:</p> <ol> <li>The dual problem that we saw earlier only depends on the dot product of the input feature vectors, not on a single feature vector itself</li> <li>Thus, if the dot product of the mapped vectors after the kernel transformation can be represented in terms of the original vectors, we don't have to have added computational complexity due to the increased size of the mapped vectors! This is what a kernel function does!</li> </ol> <p>For the example kernel function defined above: \\(\\phi(x) = \\begin{bmatrix} x_1^2 &amp; x_2^2 &amp; \\sqrt{2} x_1 x_2 \\end{bmatrix}^T\\) we can see that:</p> <p>\\(\\phi(x)\\cdot\\phi(y) = x_1^2 y_1^2 + x_2^2 y_2^2 + 2 x_1 y_1 x_2 y_2 = (x_1 y_1 + x_2 y_2)^2 = (x_1 \\cdot y_2)^2\\)</p> <p>Additionally, even when we predict the labels using this technique, we do not have to go into a higher dimension because \\(\\theta^T x\\) can be expressed as \\(\\sum\\limits_{i}\\alpha_i y_i \\vec{x}_i^T x\\) (in vector form, \\(\\theta = X Y \\alpha\\) and \\(\\hat{y} = X^T \\theta = X^T X Y \\alpha\\)) where once again only dot products are involved</p> <p>Thus, a wisely chosen kernel function is super powerful because we are essentially able to leverage the linear classifiablility of data in higher dimensions without ever actually needing to go there!</p>"},{"location":"courses/ml/notes/w4/svm/#radial-basis-function","title":"Radial Basis Function","text":"<p>An INFINITE dimensional kernel where \\(K(x, y) = \\phi(x)\\cdot\\phi(y) = \\exp(-|| x - y ||^2/2)\\)</p>"},{"location":"courses/ml/notes/w4/svm/#more-kernels","title":"More Kernels","text":"<p>A function \\(K(x, y)\\) is a valid kernel function iff there exists a \\(\\phi(x)\\) such that \\(K(x, y) = \\phi(x)\\cdot\\phi(y)\\)</p> <p>We can build more kernel functions following these rules:</p> <ol> <li>\\(\\color{orange}K = 1\\) is a kernel function (\\(\\color{orange}\\phi(x) = 1\\))</li> <li>\\(\\color{yellow}f : \\mathbb{R}^d \\to \\mathbb{R}\\) and if \\(\\color{orange}K(x, y)\\) is a valid kernel function, then so is \\(\\color{lime}\\tilde{K}(x, y) \\color{white} = \\color{yellow} f(x) \\color{orange} K(x, y) \\color{yellow} f(y)\\) (\\(\\color{lime}\\tilde{\\phi}(x) \\color{white} = \\color{yellow} f(x) \\color{orange} \\phi(x)\\))</li> <li>if \\(\\color{orange} K_1\\) and \\(\\color{cyan} K_2\\) are valid kernels, then so are \\(\\color{orange} K_1 \\color{white} + \\color{cyan} K_2\\) \\(\\left( \\color{lime} \\phi(x) \\color{white} = \\begin{bmatrix} \\color{orange} \\phi_1(x) \\\\ \\color{cyan} \\phi_2(x) \\end{bmatrix}\\right)\\) and \\(\\color{orange}K_1 \\color{cyan} K_2\\) \\(\\left( \\color{lime} \\phi(x) \\color{white} = \\color{orange}\\phi_1(x) \\color{cyan}\\phi_2(x) \\color{white}\\right)\\)</li> </ol> <p>\\(K(x, y) = (x \\cdot y)^2 = (x \\cdot y)(x \\cdot y)\\) (3)</p> <p>\\(K(x, y) = x \\cdot y + (x \\cdot y)^2\\) (3)</p> <p>\\(K(x, y) = \\exp(x \\cdot y) = \\sum\\limits_{i = 0}^{1} \\cfrac{(x \\cdot y)^i}{i!}\\)</p> <p>\\(\\tilde{K}(x, y) = \\exp(-|| x - y ||^2/2) = (\\exp(-|| x ||^2/2) \\exp(x \\cdot y) \\exp(-|| y ||)^2/2)\\) (cosine rule)</p> <p>here, \\(f(x) = \\exp(-|| x ||^2/2)\\) and \\(K(x, y) = \\exp(x \\cdot y)\\)</p> <p>\\(K(x, y) = \\exp(-|| x - y ||^2/2)\\) Because this kernel has infinite dimensions, it will ALWAYS be able to classify data</p>"},{"location":"courses/ml/notes/w5/nn/","title":"Neural Networks","text":""},{"location":"courses/ml/notes/w5/nn/#from-logistic-regression-to-nns","title":"From Logistic Regression to NNs","text":"<p>\\(h(x) = \\sigma(\\theta^T x + \\theta_0) = \\cfrac{\\exp(\\theta^T x + \\theta_0)}{1 + \\exp(\\theta^T x + \\theta_0)}\\)</p> <p>Here, if \\(x\\) is a \\(d\\) dimensional vector, then we can imagine the input of \\(h\\) as being a layer composed of \\(d\\) \"neurons\" and the output layer being a single value \\(y\\).</p> <p> </p> <p>This resulting graph is an NN. Specifically, it is a single-layered feed-forward NN. The multipliers along the edges \\(\\theta_i\\) are called weights, and the \\(\\theta_0\\) the bias. The sum \\(z = \\theta^T x + \\theta_0\\). The output \\(y = \\sigma(z)\\) where \\(\\sigma\\) is the activation function.</p>"},{"location":"courses/ml/notes/w5/nn/#more-activation-functions","title":"More activation functions","text":"<ol> <li>\\(\\sigma(z) = \\cfrac{\\exp(z)}{1 + \\exp(z)}\\) range: \\((0, 1)\\)</li> <li>\\(\\tanh(z) = \\cfrac{e^z - e^{-z}}{e^z + e^{-z}}\\) range \\((-1, 1)\\)</li> <li>\\(\\text{ReLU}(z) = \\max(0, z)\\) (Rectifier Linear Unit)</li> <li>\\(\\text{LReLU}(z) = \\begin{cases} z &amp; z &gt; 0 \\\\ 0.01z &amp; z \\leq 0 \\end{cases}\\) (Leakey)</li> <li>\\(\\text{softplus}(z) = \\ln(1+e^z)\\)</li> <li>\\(\\text{ELU}(z) = \\begin{cases} z &amp; z \\geq 0 \\\\ a(e^z - 1) &amp; z &lt; 0 \\end{cases}\\) (Exponential Linear Unit)</li> </ol>"},{"location":"courses/ml/notes/w5/nn/#deep-nns","title":"Deep NNs","text":"<p>If an NN contains &gt; 1 hidden layers, then it is known as a deep NN.</p>"},{"location":"courses/ml/notes/w5/nn/#back-propagation","title":"Back Propagation","text":"<p>let the weight connecting neuron \\(i\\) in layer \\(k\\) to neuron \\(j\\) in the layer \\(k+1\\) be \\(w_{ijk}\\)</p> <p>The activation of \\(j\\) \\(a_{j(k+1)} = \\sigma\\left(\\sum\\limits_{i = 1}^{n_k} w_{ijk}a_{ik} + b_{j(k+1)}\\right) = \\sigma\\left(\\vec{w}_{jk}^T\\vec{a}_k + b_{j(k+1)}\\right)\\)</p> <p>Here, \\(b_{j(k+1)}\\) is the bias for neuron \\(j\\) in layer \\(k+1\\)</p> <p>\\(\\vec{a}_{k+1} = \\sigma\\left(W_k \\vec{a}_k + \\vec{b}_{k+1}\\right)\\)</p> <p>where \\(W_k = \\begin{bmatrix} \\vec{w}_{1k}^T &amp; ... &amp; \\vec{w}_{n_{k+1}k}^T \\end{bmatrix}^T\\)</p> <p>If we have a scalar cost function \\(C\\), we can take the \"gradient\" of \\(C\\) w.r.t. \\(W_k\\):</p> <p>\\(W_k^{(t+1)} = W_k^{(t)} - \\eta \\left(\\cfrac{\\partial C}{\\partial W_k}\\right)^T\\)</p> <p>let us have \\(k\\) layers (excluding the output layer) of neurons. the symbols \\(a, z, b\\) represent vectors, while \\(W\\) is a matrix.</p> <p>The activation vector layer \\(i\\) is \\(a_i = \\sigma(z_{i-1})\\) where \\(z_j = W_ja_j + b_j\\) (here, the bias can be considered the weight vector for a neuron in layer \\(j\\) which always has an activation of 1)</p> <p>the output layer \\(\\hat{y} = \\sigma(z_k)\\) and we have a scalar cost function \\(C\\)</p> <p>computing the derivatives for gradient descent:</p> <p>we define the \\(\\delta_{\\hat{y}} = \\cfrac{\\partial C}{\\partial \\hat{y}}\\)</p> <p>WRT weight matrix</p> <p>\\(\\cfrac{\\partial C}{\\partial W_k} = \\cfrac{\\partial C}{\\partial \\hat{y}} \\color{lime} \\cfrac{\\partial \\hat{y}}{\\partial z_k} \\color{yellow} \\cfrac{\\partial z_k}{\\partial W_k}\\)</p> <p>\\(\\color{lime} \\cfrac{\\partial \\hat{y}}{\\partial z_k} = (\\hat{y} \\odot (1 - \\hat{y}) 1^T) \\odot \\mathbb{I}\\)</p> <p>\\(\\color{yellow} \\cfrac{\\partial z_k}{\\partial W_k} = a_k\\) (Note: this is a 3<sup>rd</sup> order tensor in reality, but we can use this as a trick here due to some nice simplifications that happen with the tensor)</p> <p>\\(\\cfrac{\\partial C}{\\partial W_k} = \\color{yellow} a_k \\color{default} \\cfrac{\\partial C}{\\partial \\hat{y}} \\color{lime} (\\hat{y} \\odot (1 - \\hat{y}) 1^T) \\odot \\mathbb{I}\\)</p> <p>WRT biases</p> <p>\\(\\cfrac{\\partial C}{\\partial b_k} = \\cfrac{\\partial C}{\\partial \\hat{y}} \\color{lime} \\cfrac{\\partial \\hat{y}}{\\partial z_k} \\color{orange} \\cfrac{\\partial z_k}{\\partial b_k}\\)</p> <p>\\(\\color{orange} \\cfrac{\\partial z_k}{\\partial b_k} = \\mathbb{I}\\)</p> <p>\\(\\cfrac{\\partial C}{\\partial b_k} = \\cfrac{\\partial C}{\\partial \\hat{y}} \\color{lime} (\\hat{y} \\odot (1 - \\hat{y}) 1^T) \\odot \\mathbb{I}\\)</p> <p>\\(\\color{yellow} \\delta_k \\color{default} = \\cfrac{\\partial C}{\\partial \\hat{y}} \\color{lime} \\cfrac{\\partial \\hat{y}}{\\partial z_k} \\color{cyan} \\cfrac{\\partial z_k}{\\partial a_k}\\)</p> <p>\\(\\color{cyan} \\cfrac{\\partial z_k}{\\partial a_k} = W_k\\)</p> <p>\\(\\color{yellow} \\delta_k \\color{default} = \\cfrac{\\partial C}{\\partial \\hat{y}} \\color{lime} ((\\hat{y} \\odot (1 - \\hat{y}) 1^T) \\odot \\mathbb{I}) \\color{cyan} W_k\\)</p> <p>Going further back:</p> <p>WRT weights</p> <p>\\(\\cfrac{\\partial C}{\\partial W_{k-1}} = \\cfrac{\\partial C}{\\partial \\hat{y}} \\color{lime} \\cfrac{\\partial \\hat{y}}{\\partial z_k} \\color{cyan} \\cfrac{\\partial z_k}{\\partial a_k} \\color{violet} \\cfrac{\\partial a_k}{\\partial z_{k-1}} \\cfrac{\\partial z_{k-1}}{\\partial W_{k-1}}\\)</p> <p>\\(\\cfrac{\\partial C}{\\partial W_{k-1}} = \\color{yellow} \\delta_k \\color{violet} \\cfrac{\\partial a_k}{\\partial z_{k-1}} \\cfrac{\\partial z_{k-1}}{\\partial W_{k-1}}\\)</p> <p>\\(\\cfrac{\\partial C}{\\partial W_{k-1}} = \\color{violet} a_{k-1} \\color{yellow} \\delta_k \\color{violet} (a_k \\odot (1 - a_k) 1^T) \\odot \\mathbb{I}\\)</p> <p>WRT Bias</p> <p>\\(\\cfrac{\\partial C}{\\partial b_{k-1}} = \\cfrac{\\partial C}{\\partial \\hat{y}} \\color{lime} \\cfrac{\\partial \\hat{y}}{\\partial z_k} \\color{cyan} \\cfrac{\\partial z_k}{\\partial a_k} \\color{violet} \\cfrac{\\partial a_k}{\\partial z_{k-1}} \\color{pink} \\cfrac{\\partial z_{k-1}}{\\partial b_{k-1}}\\)</p> <p>\\(\\color{pink} \\cfrac{\\partial z_{k-1}}{\\partial b_{k-1}} = \\mathbb{I}\\)</p> <p>\\(\\cfrac{\\partial C}{\\partial b_{k-1}} = \\color{yellow} \\delta_k \\color{violet} (a_k \\odot (1 - a_k) 1^T) \\odot \\mathbb{I}\\)</p> <p>\\(\\color{yellow} \\delta_{k-1} \\color{default} = \\cfrac{\\partial C}{\\partial \\hat{y}} \\color{lime} \\cfrac{\\partial \\hat{y}}{\\partial z_k} \\color{cyan} \\cfrac{\\partial z_k}{\\partial a_k} \\color{violet} \\cfrac{\\partial a_k}{\\partial z_{k-1}} \\color{magenta} \\cfrac{\\partial z_{k-1}}{\\partial a_{k-1}}\\)</p> <p>\\(\\color{yellow} \\delta_{k-1} \\color{default} = \\color{yellow} \\delta_k \\color{violet} ((a_k \\odot (1 - a_k) 1^T) \\odot \\mathbb{I}) \\color{magenta} W_{k-1}\\)</p> <p>Now these form recursive formulae, yippe!</p>"},{"location":"courses/ml/notes/w6/gm/","title":"Generative Models","text":""},{"location":"courses/ml/notes/w6/gm/#generative-vs-discriminative-models","title":"Generative vs Discriminative Models","text":"<ol> <li>DMs learns the conditional pr dist</li> <li>GMs learns the joint pr dist</li> <li>DMs only distinguish</li> <li>GMs distinguish but also understand what the big picture is</li> <li>DMs focus on the decision boundary. They model boundary</li> <li>GMs Build prbabilistic model for each class. They model data itself</li> </ol>"},{"location":"courses/ml/notes/w6/gm/#gaussians","title":"Gaussians","text":"<p>\\(P(x; \\mu, \\sigma) = \\cfrac{1}{\\sqrt{2\\sigma^2\\pi}} e^{-(x-\\mu)^2/(2\\sigma^2)}\\)</p>"},{"location":"courses/ml/notes/w6/gm/#multinomials-naive-bayes","title":"Multinomials: Naive Bayes","text":"<p>bag of words approach</p> <p>Each word is associated with a probability of occurance.</p> <p>\\(\\sum_{w}\\theta_w = 1\\)</p> <p>\\(\\mathbb{P}(w) = \\theta_w\\)</p> <p>This distribution of probabilities can be called a model \\(\\theta\\)</p> <p>Given a document, we can then determine the probability that this model created that document:</p> <p>\\(\\mathbb{P}(D | \\theta) = \\prod\\limits_{w \\in D}\\theta_w^{n_D(w)}\\) where \\(n_D(w)\\) is the number of times the word is repeated in the document</p> <p>To model a document, we can maximise the likelyhood (MLE)</p> <p>\\(\\max\\limits_{\\theta} \\sum\\limits_{w \\in D}n_D(w)\\log\\left(\\theta_w\\right)\\)</p> <p>Some calculus later:</p> <p>\\(\\hat{\\theta}_w = \\cfrac{n_D(w)}{\\sum_{w \\in D} n_D(w)}\\)</p> <p>This model assumes that the features of one class of documents are independent (hence the name naive, this is generally not true for text documents at least)</p> <p>Lets assume that we have two classes of labeled docs \\(+\\) and \\(-\\). We can perform two MLE to obtain two models \\(\\theta^+\\) and \\(\\theta^-\\)</p> <p>Now we can ask:</p> <p>\\(\\mathbb{P}(y = + | D) = \\cfrac{\\mathbb{P}(D | y = +) \\mathbb{P}(y = +)}{\\mathbb{P}(D)}\\)</p> <p>where \\(\\mathbb{P}(D) = \\mathbb{P}(D | y = +) \\mathbb{P}(y = +) + \\mathbb{P}(D | y = -) \\mathbb{P}(y = -)\\)</p> <p>We can write the log likelyhood ratio as a discriminant function:</p> <p>\\(\\log \\cfrac{\\mathbb{P}(y = +|D)}{\\mathbb{P}(y = -|D)}\\)</p> <p>\\(\\log \\cfrac{\\mathbb{P}(D | y = +) \\mathbb{P}(y = +)}{\\mathbb{P}(D | y = -) \\mathbb{P}(y = -)}\\)</p> <p>\\(\\log \\prod\\limits_{w \\in b} (\\theta_w^+)^{n_D(w)} \\mathbb{P}(y = +) - \\log \\prod\\limits_{w \\in b} (\\theta_w^-)^{n_D(w)} \\mathbb{P}(y = -)\\)</p> <p>\\(\\sum\\limits_{w \\in b} n_D(w)\\log \\theta_w^+ \\mathbb{P}(y = +) - \\sum\\limits_{w \\in b} n_D(w)\\log \\theta_w^- \\mathbb{P}(y = -)\\)</p> <p>\\(\\sum\\limits_{w \\in D} n_D(w) \\log \\cfrac{\\theta^+_w \\mathbb{P}(y = +)}{\\theta^+_w \\mathbb{P}(y = -)}\\)</p> <p>\\(\\sum\\limits_{w \\in D} n_D(w) \\underbrace{\\log \\cfrac{\\theta^+_w}{\\theta^+_w}}_{\\theta_w} + \\underbrace{\\log \\cfrac{\\mathbb{P}(y = +)}{\\mathbb{P}(y = -)}}_{\\theta_0}\\)</p> <p>\\(\\sum\\limits_{w \\in D} n_D(w) \\theta_w + \\theta_0 = \\Phi(D)^T \\theta + \\theta_0\\)</p> <p>\\(\\Phi(D)\\) freq vector</p>"},{"location":"courses/ml/notes/w6/gm/#smoothing","title":"Smoothing","text":"<p>\\(\\Phi_s(D) = \\Phi(D) + \\lambda\\) where \\(\\lambda\\) is default values for words which can help account for unseen words</p>"},{"location":"courses/ml/notes/w6/mm/","title":"Mixture Models","text":""},{"location":"courses/ml/notes/w6/mm/#spherical-gaussian","title":"Spherical Gaussian","text":"<p>\\(P(x; \\mu, \\sigma) = \\cfrac{1}{(\\sqrt{2\\sigma^2\\pi})^d} e^{-(x-\\mu)^2/(2\\sigma^2)}\\) where \\(d\\) is dim of \\(x\\)</p> <p>from a sample:</p> <p>\\(\\hat{\\mu} = \\cfrac{1}{n}\\sum\\limits_{i = 1}^{n} x_i\\)</p> <p>\\(\\hat{\\sigma}^2 = \\cfrac{1}{dn}\\sum\\limits_{i = 1}^{n} ||x_i - \\hat{\\mu}||^2\\)</p>"},{"location":"courses/ml/notes/w6/mm/#mix-of-spherical-gaussians","title":"Mix of spherical gaussians","text":"<p>Assuming that there are \\(k\\) clusters, there will be \\(k\\) gaussians.</p> <p>\\(p_i\\) \\(i \\in \\{1, 2, ..., k\\}\\) - frequency of points expected to see in each cluster</p> <p>If all the parameters of our model are \\(\\theta\\) then:</p> <p>\\(P(x | \\theta) = \\sum\\limits_{i=1}^{k}p_i \\mathbb{P}(x ; \\hat{\\mu}_i, \\hat{\\sigma}_i^2)\\)</p>"},{"location":"courses/ml/notes/w6/mm/#estimating-mixtures-of-labeled-data","title":"Estimating Mixtures of Labeled Data","text":"<p>\\(\\delta (i|t) = \\begin{cases} 1 \\text{ if } x_t \\text{ in i} \\\\ 0\\end{cases}\\)</p> <p>The Max Likelyhood objective:</p> <p>\\(\\sum\\limits_{t = 1}^{n} \\sum\\limits_{i = 1}^{k} \\delta(i|t) \\log p_i \\mathbb{P}(x_t | \\hat{\\mu}_i, \\hat{\\sigma}_i^2)\\)</p> <p>\\(\\hat{n}_i = \\sum\\limits_{t = 1}^{n} \\delta(i|t)\\) (num points assigned to cluster \\(i\\))</p> <p>\\(p_i = \\cfrac{\\hat{n}_i}{n}\\)</p> <p>\\(\\hat{\\mu}_i = \\cfrac{1}{\\hat{n}_i}\\sum\\limits_{t = 1}^{n} \\delta(i|t) x_t\\)</p> <p>\\(\\hat{\\sigma}_i^2 = \\cfrac{1}{d \\hat{n}_i}\\sum\\limits_{t = 1}^{n} \\delta(i|t) ||x_t - \\hat{\\mu}||^2\\)</p>"},{"location":"courses/ml/notes/w6/mm/#estimating-mixtures-without-labels","title":"Estimating Mixtures Without Labels","text":"<p>Max:</p> <p>\\(\\sum\\limits_{t = 1}^{n} \\log \\sum\\limits_{i=1}^{k}p_i \\mathbb{P}(x ; \\hat{\\mu}_i, \\hat{\\sigma}_i^2)\\)</p> <p>A closed form solution cannot be obtained here</p> <p>initialise all \\(\\hat{\\sigma}_i^2 = \\hat{\\sigma}^{2} = \\cfrac{1}{d n}\\sum\\limits_{t = 1}^{n} ||x_t - \\hat{\\mu}||^2\\)</p> <p>The E Step:</p> <p>\\(w(i|t) = \\cfrac{p_i \\mathbb{P}(x_t ; \\hat{\\mu}_i, \\hat{\\sigma}_i^2)}{\\mathbb{P}(x_t | \\theta)} = \\cfrac{p_i \\mathbb{P}(x_t ; \\hat{\\mu}_i, \\hat{\\sigma}_i^2)}{\\sum\\limits_{j=1}^{k}p_j \\mathbb{P}(x_t ; \\hat{\\mu}_j, \\hat{\\sigma}_j^2)}\\)</p> <p>\\(w(i|t)\\) softly assigns each point to a cluster by a weight, this is similar to the labeled case where we could do a definite \\(0\\) or \\(1\\) assignment with \\(\\delta\\).</p> <p>The M Step:</p> <p>\\(\\hat{n}_i = \\sum\\limits_{t = 1}^{n} w(i|t)\\) (effecive num points assigned to cluster \\(i\\))</p> <p>\\(p_i = \\cfrac{\\hat{n}_i}{n}\\)</p> <p>\\(\\hat{\\mu}_i = \\cfrac{1}{\\hat{n}_i}\\sum\\limits_{t = 1}^{n} w(i|t) x_t\\)</p> <p>\\(\\hat{\\sigma}_i^2 = \\cfrac{1}{d \\hat{n}_i}\\sum\\limits_{t = 1}^{n} w(i|t) ||x_t - \\hat{\\mu}||^2\\)</p> <p>same properties of convergence as \\(k\\)-means</p>"},{"location":"courses/ml/notes/w8/cs/","title":"Cs","text":""},{"location":"courses/ml/notes/w8/cs/#perceptron","title":"Perceptron","text":"<p>\\(h(x, \\theta, \\theta_0) = sign(\\theta^T x + \\theta_0)\\)</p> <p>if \\(y^{(t)} \\neq h(x^{(t)}, \\theta^{(k)}, \\theta^{(k)}_0)\\) then</p> <p>\\(\\theta^{(k+1)} = \\theta^{(k)} + y^{(t)}x^{(t)}\\)</p> <p>\\(\\theta^{(k+1)}_0 = \\theta^{(k)}_0 + y^{(t)}\\)</p>"},{"location":"courses/ml/notes/w8/cs/#hinge-loss","title":"Hinge Loss","text":"<p>Empirical Risk</p> <p>\\(R_n (\\theta) = \\cfrac{1}{n}\\sum\\limits_{t=1}^{n} \\text{Loss}(y^{(t)} (\\theta^T x^{(t)}))\\)</p> <p>In our original linearly separable case, we chose</p> <p>\\(\\text{Loss}(y^{(t)} (\\theta^T x^{(t)})) = |y^{(t)} - y^{(t)} (\\theta^T x^{(t)})|/2\\)</p> <p>This is known as zero-one loss. We will change the metric of loss evaluation to hinge loss,</p> <p>\\(\\text{Loss}(y^{(t)} (\\theta^T x^{(t)})) = max\\{1 - y^{(t)} (\\theta^T x^{(t)}), 0\\}\\)</p> <p>here, \\(z = y^{(t)} (\\theta^T x^{(t)})\\) is called the agreement</p>"},{"location":"courses/ml/notes/w8/cs/#sub-gradient-descent","title":"Sub Gradient Descent","text":"<p>\\(\\nabla_{\\theta} R_n(\\theta) = \\left[\\cfrac{\\partial R_n(\\theta)}{\\partial\\theta_1}, ..., \\cfrac{\\partial R_n(\\theta)}{\\partial\\theta_d}\\right]^T\\)</p> <p>\\(\\theta^{(k+1)} = \\theta^{(k)} - \\eta_k \\nabla_{\\theta} R_n (\\theta^{(k)})\\)</p>"},{"location":"courses/ml/notes/w8/cs/#stochastic-gd","title":"Stochastic GD","text":"<p>\\(\\nabla_{\\theta} R_n (\\theta^{(k)}) = - y^{(t)} x^{(t)}\\)</p> <p>Loss from just one data point is considered ^</p> <p>if \\(y^{(t)}((\\theta^{(t)})^T x^{(t)}) \\leq 1\\) then</p> <p>\\(\\theta^{(k+1)} = \\theta^{(k)} + \\eta_k y^{(t)} x^{(t)}\\)</p> <ol> <li>The mistakes are penalised linearly, instead of in a binary fassion</li> <li>Here, the \\(\\eta_k\\) is decreased over the iterations, instead of being fixed</li> <li>Additionally, the \"mistake\", i.e. when the update is made is now defined in terms of \\(z \\leq 1\\) instead of \\(z &lt; 0\\)</li> <li>The training points are chosen at random, than cycling through them in order, to prevent the updates from oscillating</li> </ol> <p>\\(\\eta_k = \\cfrac{1}{k+1}\\) is a choice of \\(\\eta_k\\) that ensures that the SSGD converges. Any choice that satisfies \\(\\sum\\limits_{k=1}^{\\infty} \\eta_k^2 &lt; \\infty\\) and \\(\\sum\\limits_{k=1}^{\\infty} \\eta_k = \\infty\\) makes the algorithm converge</p>"},{"location":"courses/ml/notes/w8/cs/#k-means","title":"K Means","text":"<p>cosine similarty:</p> <p>\\(cos(x, y) = \\cfrac{x . y}{||x|| \\; ||y||}\\)</p> <p>dissimilarity (pairwise euclidian distance)</p> <p>\\(dist(x, y) = ||x - y||^2\\)</p> <p>Sometimes, the L1/manhattan distance can be used:</p> <p>\\(dist(x, y) = ||x - y||_1\\)</p> <p>distortion within one cluster \\(C_j\\):</p> <p>\\(\\sum\\limits_{x^{(i)} \\in C_j}||x^{(i)} - z^{(j)}||^2\\)</p> <p>cost of the clustering:</p> <p>\\(\\text{cost}(C_1, ... C_k, z^{(1)}, ... z^{(k)}) = \\sum\\limits_{j = 1}^{k} \\sum\\limits_{x^{(i)} \\in C_j}||x^{(i)} - z^{(j)}||^2\\)</p> <p>\\(\\text{cost}(z^{(1)}, ... z^{(k)}) = \\sum\\limits_{i = 1, ..., n} \\min\\limits_{j = 1, ..., k} ||x^{(i)} - z^{(j)}||^2\\)</p>"},{"location":"courses/ml/notes/w8/cs/#k-means-algorithm","title":"K-Means Algorithm","text":"<p>Alternatively find the best clusters for centroids, then the best centroids for the clusters. Iterative algorithm:</p> <ol> <li>Randomly initialise \\(k\\) centroids</li> <li>Repeat until no further change in cost:</li> <li>\\(\\forall j = 1, ..., k, C_j = \\{ i |  x^{(i)} \\text{ is closest to } z^{(j)}\\}\\)</li> <li>\\(\\forall j = 1, ..., k, z^{(j)} = \\frac{1}{|C_j|} \\sum\\limits_{x^{(i)} \\in C_j} x^{(i)}\\) (cluster mean)</li> <li>Variation in K-Mediods: \\(\\forall j = 1, ..., k, z^{(j)} = x^{(i)} \\in C_j | \\min\\limits_{x^{(i)} \\in C_j} d(x^{(i)}, z^{(j)})\\)</li> </ol> <p>Each iteration requires \\(\\mathcal{O}(kn)\\) steps</p>"},{"location":"courses/ml/notes/w8/cs/#convergence","title":"Convergence","text":"<p>Each iteration of K-Means either keeps the cost the same or necessarily lowers it. This is because in the first step, we are choosing members of the clusters based on fixed \\(z^{(i)}\\) points, and since this choosing is done by assigning points to the cluster of the closest centroid, it minimises our cost by definition. In the second iteration, a new centroid is chosen after fixing the clusters, which boils down to minimising the SSW of one cluster. The solution of this minimsation problem is the formula for the centroid given in step 2!</p>"},{"location":"courses/ml/notes/w8/cs/#practical-issues","title":"Practical Issues","text":"<ol> <li>Elbow method</li> <li>local min not global</li> </ol>"},{"location":"courses/ml/notes/w8/cs/#log-reg","title":"Log Reg","text":"<p>Here, \\(P(y | x) = \\begin{cases} h(x) &amp; y = 1 \\\\ 1 - h(x) &amp; y = -1\\end{cases}\\)</p> <p>due to a property of the sigmoid function where \\(\\sigma(-s) = 1 - \\sigma(x)\\) we can rewrite the probability as:</p> <p>\\(P(y | x) = \\sigma(y (\\theta^T x + \\theta_0))\\)</p> <p>Minimise: \\(J(\\theta) = \\cfrac{1}{n} \\sum\\limits_{i = 1}^{n} \\ln(1 + \\exp(-y_i (\\theta^T x_i + \\theta_0 )))\\)</p>"},{"location":"courses/ml/notes/w8/cs/#sgd","title":"SGD","text":"<p>\\(\\nabla_{\\theta} e_t(\\theta) = \\cfrac{-y_t \\theta^T x_t}{(1 + e^{\\displaystyle y_t \\theta^T x_t})}\\)</p> <p>Then the weight update is:</p> <p>\\(\\theta_{t+1} = \\theta_t - \\eta \\nabla_{\\theta} e_t (\\theta)\\) </p>"},{"location":"courses/ml/notes/w8/cs/#prediction","title":"Prediction","text":"<p>Our prediction is \\(1\\) iff:</p> <ol> <li>\\(P(y = +1|x) \\geq 0.5\\)</li> <li>\\(P(y = +1|x) &gt; P(y = -1|x) \\implies P(y = +1|x)/P(y = -1|x) \\geq 1\\)</li> <li>\\(\\theta^T x \\geq 0\\) (by taking \\(\\ln\\) of the above inequality)</li> </ol>"},{"location":"courses/ml/notes/w8/cs/#svm","title":"SVM","text":"<p>\\(L = \\cfrac{||\\theta||^2}{2} - \\sum\\limits_{i = 1}^{n} \\alpha_i \\left(y_i (\\theta^T \\vec{x_i} + \\theta_0) - 1 \\right)\\)</p> <p>non vector form: \\(\\vec{\\theta} = \\sum\\limits_{i} \\alpha_i y_i \\vec{x_i}\\)</p> <p>non vector form: \\(\\sum\\limits_{i}\\alpha_i y_i  = 0\\)</p> <p>Non vector form: \\(L = \\sum\\limits_{i} \\alpha_i - \\cfrac{1}{2} \\sum\\limits_{i}\\sum\\limits_{j} \\alpha_i \\alpha_j y_i y_j \\vec{x_i}^T \\vec{x_j}\\)</p> <p>Since \\(\\theta\\) is a linear combination of some of the vectors \\(\\vec{x_i}\\) for which \\(\\alpha_i &gt; 0\\) and \\(y_i \\theta^T x_i = 1\\), they are called support vectors</p> <p>The rest are non support vectors since \\(\\alpha_i = 0\\) and \\(y_i \\theta^T x_i &gt; 1\\) and thus do not contribute to \\(\\theta\\)</p> <p>\\(L = \\cfrac{\\lambda}{2}||\\theta||^2 + \\sum\\limits_{i = 1}^{n} \\xi_i - \\sum\\limits_{i = 1}^{n} \\alpha_i \\left(y_i (\\theta^T \\vec{x_i} + \\theta_0) - 1 + \\xi_i \\right) - \\sum\\limits_{i = 1}^n \\mu_i \\xi_i\\)</p> <p>or</p> <p>\\(L = \\cfrac{1}{2}\\mathbf{w}^T \\mathbf{w} + C\\sum\\limits_{i = 1}^{n} \\xi_i - \\sum\\limits_{i = 1}^{n} \\alpha_i \\left(y_i (\\mathbf{w}^T \\vec{x_i} + b) - 1 + \\xi_i \\right) - \\sum\\limits_{i = 1}^n \\mu_i \\xi_i\\)</p> <p>Along with their normal forms:</p> <ol> <li>\\(\\color{blue}\\mathbf{w} = \\sum\\limits_{i = 1}^{n} d_i \\alpha_i \\vec{x}_i\\)</li> <li>\\(\\color{blue}\\sum\\limits_{i = 1}^{n} \\alpha_i d_i = 0\\)</li> <li>\\(\\color{blue}\\alpha_i = C - \\mu_i\\)</li> </ol> <p>In the normal form, this is:</p> <p>\\(\\color{blue}L = \\sum\\limits_{i = 1}^{n} \\alpha_i - \\cfrac{1}{2}\\sum\\limits_{i = 1}^{n} \\sum\\limits_{j = 1}^{n} \\alpha_i d_i \\vec{x_i}^T \\vec{x_j} d_j \\alpha_j\\)</p> <p>lastly, from KKT, we have:</p> <ol> <li>Dual feasibility:</li> <li>\\(\\color{blue}\\alpha_i \\geq 0\\)</li> <li>\\(\\color{blue}\\mu_i \\geq 0\\)</li> </ol> <p>using these and \\(\\alpha_i = C - \\mu_i\\), we can conclude that \\(\\color{blue}\\alpha_i \\in [0, C]\\)</p> <ol> <li>The complementary slackness conditions:</li> <li>\\(\\color{blue} \\mu_i \\xi_i = 0\\)</li> <li>\\(\\color{blue} \\alpha_i (d_i(\\mathbf{w}^T x_i + b) - 1 + \\xi_i) = 0\\)</li> </ol> <p>\\(\\alpha \\in (0, 1/\\lambda)\\) for SV and \\(\\alpha = 1/\\lambda\\) for margin violations</p>"},{"location":"courses/ml/notes/w8/cs/#kernels","title":"Kernels","text":"<ol> <li>\\(\\color{orange}K = 1\\) is a kernel function (\\(\\color{orange}\\phi(x) = 1\\))</li> <li>\\(\\color{yellow}f : \\mathbb{R}^d \\to \\mathbb{R}\\) and if \\(\\color{orange}K(x, y)\\) is a valid kernel function, then so is \\(\\color{lime}\\tilde{K}(x, y) \\color{white} = \\color{yellow} f(x) \\color{orange} K(x, y) \\color{yellow} f(y)\\) (\\(\\color{lime}\\tilde{\\phi}(x) \\color{white} = \\color{yellow} f(x) \\color{orange} \\phi(x)\\))</li> <li>if \\(\\color{orange} K_1\\) and \\(\\color{cyan} K_2\\) are valid kernels, then so are \\(\\color{orange} K_1 \\color{white} + \\color{cyan} K_2\\) \\(\\left( \\color{lime} \\phi(x) \\color{white} = \\begin{bmatrix} \\color{orange} \\phi_1(x) \\\\ \\color{cyan} \\phi_2(x) \\end{bmatrix}\\right)\\) and \\(\\color{orange}K_1 \\color{cyan} K_2\\) \\(\\left( \\color{lime} \\phi(x) \\color{white} = \\color{orange}\\phi_1(x) \\color{cyan}\\phi_2(x) \\color{white}\\right)\\)</li> </ol> <p>An INFINITE dimensional kernel where \\(K(x, y) = \\phi(x)\\cdot\\phi(y) = \\exp(-|| x - y ||^2/2)\\)</p> <p>(this can be derived from power series and properties)</p>"},{"location":"courses/ml/notes/w8/cs/#nn","title":"NN","text":"<p>\\(\\vec{a}_{k+1} = \\sigma\\left(W_k \\vec{a}_k + \\vec{b}_{k+1}\\right)\\)</p> <p>\\(\\cfrac{\\partial C}{\\partial W_k} = \\color{yellow} a_k \\color{default} \\cfrac{\\partial C}{\\partial \\hat{y}} \\color{lime} (\\hat{y} \\odot (1 - \\hat{y}) 1^T) \\odot \\mathbb{I}\\)</p> <p>\\(\\cfrac{\\partial C}{\\partial b_k} = \\cfrac{\\partial C}{\\partial \\hat{y}} \\color{lime} (\\hat{y} \\odot (1 - \\hat{y}) 1^T) \\odot \\mathbb{I}\\)</p> <p>\\(\\color{yellow} \\delta_k \\color{default} = \\cfrac{\\partial C}{\\partial \\hat{y}} \\color{lime} ((\\hat{y} \\odot (1 - \\hat{y}) 1^T) \\odot \\mathbb{I}) \\color{cyan} W_k\\)</p> <p>\\(\\cfrac{\\partial C}{\\partial W_{k-1}} = \\color{violet} a_{k-1} \\color{yellow} \\delta_k \\color{violet} (a_k \\odot (1 - a_k) 1^T) \\odot \\mathbb{I}\\)</p> <p>\\(\\cfrac{\\partial C}{\\partial b_{k-1}} = \\color{yellow} \\delta_k \\color{violet} (a_k \\odot (1 - a_k) 1^T) \\odot \\mathbb{I}\\)</p> <p>\\(\\color{yellow} \\delta_{k-1} \\color{default} = \\color{yellow} \\delta_k \\color{violet} ((a_k \\odot (1 - a_k) 1^T) \\odot \\mathbb{I}) \\color{magenta} W_{k-1}\\)</p>"},{"location":"courses/ml/notes/w8/cs/#multinomials-naive-bayes","title":"Multinomials: Naive Bayes","text":"<p>bag of words approach, each word is associated with a probability of occurance.</p> <p>\\(\\sum_{w}\\theta_w = 1\\) &amp; \\(\\mathbb{P}(w) = \\theta_w\\)</p> <p>\\(\\mathbb{P}(D | \\theta) = \\prod\\limits_{w \\in D}\\theta_w^{n_D(w)}\\)</p> <p>\\(\\max\\limits_{\\theta} \\sum\\limits_{w \\in D}n_D(w)\\log\\left(\\theta_w\\right)\\)</p> <p>\\(\\hat{\\theta}_w = \\cfrac{n_D(w)}{\\sum_{w \\in D} n_D(w)}\\)</p> <p>\\(\\mathbb{P}(y = + | D) = \\cfrac{\\mathbb{P}(D | y = +) \\mathbb{P}(y = +)}{\\mathbb{P}(D)}\\)</p> <p>\\(\\log \\cfrac{\\mathbb{P}(y = +|D)}{\\mathbb{P}(y = -|D)}\\)</p> <p>\\(\\sum\\limits_{w \\in D} n_D(w) \\underbrace{\\log \\cfrac{\\theta^+_w}{\\theta^+_w}}_{\\theta_w} + \\underbrace{\\log \\cfrac{\\mathbb{P}(y = +)}{\\mathbb{P}(y = -)}}_{\\theta_0}\\)</p> <p>\\(\\Phi(D)\\) freq vector</p>"},{"location":"courses/ml/notes/w8/cs/#smoothing","title":"Smoothing","text":"<p>\\(\\Phi_s(D) = \\Phi(D) + \\lambda\\) where \\(\\lambda\\) is default values for words which can help account for unseen words</p>"},{"location":"courses/ml/notes/w8/cs/#spherical-gaussian","title":"Spherical Gaussian","text":"<p>\\(P(x; \\mu, \\sigma) = \\cfrac{1}{(\\sqrt{2\\sigma^2\\pi})^d} e^{-(x-\\mu)^2/(2\\sigma^2)}\\) where \\(d\\) is dim of \\(x\\)</p>"},{"location":"courses/ml/notes/w8/cs/#estimating-mixtures-without-labels","title":"Estimating Mixtures Without Labels","text":"<p>Max:</p> <p>\\(\\sum\\limits_{t = 1}^{n} \\log \\sum\\limits_{i=1}^{k}p_i \\mathbb{P}(x ; \\hat{\\mu}_i, \\hat{\\sigma}_i^2)\\)</p> <p>A closed form solution cannot be obtained here</p> <p>initialise all \\(\\hat{\\sigma}_i^2 = \\hat{\\sigma}^{2} = \\cfrac{1}{d n}\\sum\\limits_{t = 1}^{n} ||x_t - \\hat{\\mu}||^2\\)</p> <p>The E Step:</p> <p>\\(w(i|t) = \\cfrac{p_i \\mathbb{P}(x_t ; \\hat{\\mu}_i, \\hat{\\sigma}_i^2)}{\\mathbb{P}(x_t | \\theta)} = \\cfrac{p_i \\mathbb{P}(x_t ; \\hat{\\mu}_i, \\hat{\\sigma}_i^2)}{\\sum\\limits_{j=1}^{k}p_j \\mathbb{P}(x_t ; \\hat{\\mu}_j, \\hat{\\sigma}_j^2)}\\)</p> <p>\\(w(i|t)\\) softly assigns each point to a cluster by a weight, this is similar to the labeled case where we could do a definite \\(0\\) or \\(1\\) assignment with \\(\\delta\\).</p> <p>The M Step:</p> <p>\\(\\hat{n}_i = \\sum\\limits_{t = 1}^{n} w(i|t)\\) (effecive num points assigned to cluster \\(i\\))</p> <p>\\(p_i = \\cfrac{\\hat{n}_i}{n}\\)</p> <p>\\(\\hat{\\mu}_i = \\cfrac{1}{\\hat{n}_i}\\sum\\limits_{t = 1}^{n} w(i|t) x_t\\)</p> <p>\\(\\hat{\\sigma}_i^2 = \\cfrac{1}{d \\hat{n}_i}\\sum\\limits_{t = 1}^{n} w(i|t) ||x_t - \\hat{\\mu}||^2\\)</p> <p>same properties of convergence as \\(k\\)-means</p>"},{"location":"courses/ml/notes/w9/dec_tree/","title":"Decision Trees","text":"<ol> <li>Builds classification or regression models in the form of a tree structure</li> <li>Breaks down dataset into smaller subsets while associated dec tree is built</li> <li>decision nodes and leaf nodes</li> </ol>"},{"location":"courses/ml/notes/w9/dec_tree/#growing-a-tree","title":"Growing A Tree","text":"<ol> <li>Feature choice</li> <li>Conditions for splitting</li> <li>Stopping condition</li> <li>Pruning</li> </ol>"},{"location":"courses/ml/notes/w9/dec_tree/#decision-tree-induction","title":"Decision tree induction","text":"<ol> <li>Hunt's Algorithm</li> <li>CART</li> <li>ID3, C4.5</li> <li>SLIQ, SPRINT</li> </ol>"},{"location":"courses/ml/notes/w9/dec_tree/#hunts-algo","title":"Hunt's Algo","text":"<ol> <li>Grow recursively by partitioning training records successively into purer subset</li> <li>It is the basis of many existing decision tree induction algorithm</li> </ol> <p>Algo:</p> <ul> <li>Let \\(D_t\\) be the set of training records that reach a node \\(t\\)</li> <li>If \\(D_t\\) contains records that all belong to the same class \\(y_t\\), \\(t\\) is a leaf node labeled \\(y_t\\)</li> <li>If \\(D_t\\) is an empty set then \\(t\\) is a leaf node labeled \\(y_d\\) (default)</li> <li>If \\(D_t\\) contains records that belong to more than one class, use an attribute test to split the data into smaller subset, recurse</li> </ul>"},{"location":"courses/ml/notes/w9/dec_tree/#attribute-test","title":"Attribute Test","text":"<p>Greedy strat to split records based on optimising a certain metric. </p>"},{"location":"courses/ml/notes/w9/dec_tree/#nominalordinal-attributes","title":"Nominal/Ordinal Attributes","text":"<ol> <li>Multi-Way Split: As many splits as distinct values for \\(D_t\\)</li> <li>Binary Split: split into something and not something. Need to find optimal partitioning</li> </ol>"},{"location":"courses/ml/notes/w9/dec_tree/#continuous-attributes","title":"Continuous Attributes","text":"<ol> <li> <p>Descretisation:</p> <ul> <li>static - discretise once at the beginning</li> <li>dynamic - ranges by equal interval bucketing or equal freq bucketing or clustering</li> </ul> </li> <li> <p>Binary Decision <code>&lt;</code> and <code>&gt;=</code> Find optimal cutting point</p> </li> </ol>"},{"location":"courses/ml/notes/w9/dec_tree/#homogeneous-split","title":"Homogeneous Split","text":"<p>Greedy strat</p>"},{"location":"courses/ml/notes/w9/dec_tree/#measure-of-node-impurity","title":"Measure of node impurity","text":"<ol> <li> <p>Gini Index</p> <ul> <li>Probability of being class in X given a node \\(t\\)</li> <li>\\(\\text{GINI}(t) = 1 - \\sum\\limits_{j} p^2(j|t)\\) where \\(p(j|t)\\) is the relative freq of class \\(j\\) at node \\(t\\)</li> <li>Max: \\(1 - 1/n_c\\) when the records are equally distributed among all classes, implying least interesting info</li> <li>Min (0): When all records belong to one class, most interesting info</li> <li>CART, SLIQ, SPRINT</li> <li>When a node \\(t\\) is split into \\(k\\) parts,</li> <li>\\(\\text{GINI}_{split} = \\sum\\limits_{i} \\cfrac{n_i}{n} \\text{GINI}(i)\\) where \\(n_i\\) is the num records at child \\(i\\) and \\(n\\) is the num records at node \\(t\\)</li> </ul> </li> <li> <p>Entropy</p> <ul> <li>\\(\\text{Entropy}(t) = -\\sum\\limits_{j} p(j|t) \\ln p(j|t)\\) where \\(p(j|t)\\) is the relative freq of class \\(j\\) at node \\(t\\)</li> <li>Max (\\(\\ln n_c\\)) when records equally distributed, least interesting info</li> <li>Min (0) When all records belong to one class, most interesting info</li> <li>When a node \\(t\\) is split into \\(k\\) parts,</li> <li>\\(\\text{GAIN}_{split} = \\text{Entropy(t)} - \\sum\\limits_{i} \\cfrac{n_i}{n} \\text{Entropy}(i)\\) where \\(n_i\\) is the num records at child \\(i\\) and \\(n\\) is the num records at node \\(t\\)</li> <li>\\(\\text{SplitINFO} = - \\sum\\limits_{i} \\cfrac{n_i}{n} \\ln \\cfrac{n_i}{n}\\)</li> <li>\\(\\text{GainRATIO}_{split} = \\cfrac{\\text{GAIN}_{split}}{SplitINFO}\\) (higher entropy partitions are penalised)</li> </ul> </li> <li> <p>Misclassification Error</p> <ul> <li>\\(\\text{Error}(t) = 1 - \\max\\limits_{i} p(i|t)\\) where \\(p(j|t)\\) is the relative freq of class \\(j\\) at node \\(t\\)</li> <li>Max: \\(1 - 1/n_c\\) when the records are equally distributed among all classes, implying least interesting info</li> <li>Min (0): When all records belong to one class, most interesting info</li> <li>When a node \\(t\\) is split into \\(k\\) parts,</li> <li>\\(\\text{Error}_{split} = \\sum\\limits_{i} \\cfrac{n_i}{n} \\text{Error}(i)\\) where \\(n_i\\) is the num records at child \\(i\\) and \\(n\\) is the num records at node \\(t\\)</li> </ul> </li> </ol>"},{"location":"courses/ml/notes/w9/dec_tree/#stopping-criteria","title":"Stopping Criteria","text":"<ol> <li>Stop when all nodes under record have same label</li> <li>Stop if all attributes label are the same</li> <li>Early Termination</li> </ol>"},{"location":"courses/ml/notes/w9/dec_tree/#pros","title":"Pros","text":"<ol> <li>Simple to understand, interpret, visualise</li> <li>Categorical and Numerical data</li> <li>Extremely fast</li> <li>Accuracy is comparable to other techniques for simple datasets</li> <li>Non linear relations between variables do no affect perf</li> </ol>"},{"location":"courses/ml/notes/w9/dec_tree/#cons","title":"Cons","text":"<ol> <li>Prone to overfitting</li> <li>Unstable, small variation in data gives different tree</li> <li>Greedy algorithm doesn't guarantee the return of globally optimal decision tree</li> </ol>"},{"location":"courses/ml/notes/w9/dim_red/","title":"Dimensionality Reduction","text":""},{"location":"courses/ml/notes/w9/dim_red/#curse-of-dimensionality","title":"Curse of Dimensionality","text":"<ul> <li>When dimensionality increases, data becomes sparse, metrics like distance and density become less meaningful</li> </ul>"},{"location":"courses/ml/notes/w9/dim_red/#why-dimensionality-reduction","title":"Why Dimensionality Reduction","text":"<ul> <li>Avoid Curse of Dimensionality</li> <li>Reduce time &amp; memory used by ML algos</li> <li>Easier visualisation of data</li> <li>Remove irrelevant features</li> <li>Feature extraction/selection</li> </ul>"},{"location":"courses/ml/notes/w9/dim_red/#feature-selection","title":"Feature Selection","text":"<p>Filter out features that are obviously irrelevant to the prediction</p>"},{"location":"courses/ml/notes/w9/dim_red/#feature-extraction","title":"Feature Extraction","text":"<p>Combine multiple (height, weight -&gt; bmi) into one</p>"},{"location":"courses/ml/notes/w9/dim_red/#pca","title":"PCA","text":"<p>Unsupervised technique for extracting variance structure from high dimensional datasets</p> <p>An orthogonal projection or transformation of data into possibly lower dimensional space so that the variance of the projected data is max</p>"},{"location":"courses/ml/notes/w9/dim_red/#pca-by-svd","title":"PCA by SVD","text":"<ol> <li>Calculate mean of all the features. \\(\\bar{x}\\) </li> <li>Shift origin to \\(\\bar{x}\\).</li> <li>Find line of best fit through new origin (min perp dist to line or max proj dist from origin)</li> <li>PCA by SVD does the latter for computational reasons</li> <li>Max sum of squared distances of projected points from origin</li> <li>The line found is called PC1</li> <li>The slopes of PC1 tells you about the linear combination of two features from the dataset. This may tell us that one feature is more important than another!</li> <li>We normalise the vector made by the slope to get a unit vector. This is known as the singular vector or the eigen vector for PC1</li> <li>The sum of squared distances for the line of best fit is called the eigen value</li> <li>The square root of the eigen value is called the singular value</li> <li>PC2 is a perpendicular line to PC1 (PC3 is a perpendicular line to both PC2 and PC1, ...)</li> <li>Find the variance for both PC1 and PC2, and then variance_1/(variance_1+variance_2) tells us the \"weight\" of PC1 among all the PCs</li> <li>To find the PCx (transformation) we dot the feature vector with the corresponding eigen vector</li> <li>Once we do this, we can take the PCs that have the high % variation and drop the rest</li> </ol>"}]}