{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2022-07-13T18:23:30.587829Z",
     "iopub.execute_input": "2022-07-13T18:23:30.588337Z",
     "iopub.status.idle": "2022-07-13T18:23:30.605283Z",
     "shell.execute_reply.started": "2022-07-13T18:23:30.588293Z",
     "shell.execute_reply": "2022-07-13T18:23:30.603996Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Training Dataset"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_train = pd.read_csv(\"./train_tfidf_features.csv\")\n",
    "display(df_train)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-13T18:23:30.736651Z",
     "iopub.execute_input": "2022-07-13T18:23:30.737059Z",
     "iopub.status.idle": "2022-07-13T18:23:53.705365Z",
     "shell.execute_reply.started": "2022-07-13T18:23:30.737028Z",
     "shell.execute_reply": "2022-07-13T18:23:53.704076Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_train.describe()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-13T18:23:53.708000Z",
     "iopub.execute_input": "2022-07-13T18:23:53.708798Z",
     "iopub.status.idle": "2022-07-13T18:24:07.426532Z",
     "shell.execute_reply.started": "2022-07-13T18:23:53.708748Z",
     "shell.execute_reply": "2022-07-13T18:24:07.425212Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 1: Implement Logistic Regression\n",
    "Recalled that you have learned about Logistic Regression in your earlier class. Your task is to implement a Logistic Regression model from scratch. \\\n",
    "Note that you are NOT TO USE the sklearn logistic regression package or any other pre-defined logistic regression package for this task! \\\n",
    "Usage of any logistic regression packages will result in 0 marks for this task.\n",
    "\n",
    "## Key Task Deliverables\n",
    "1a. Code implementation of the Logistic Regression model. \\\n",
    "1b. Prediction made by your Logistic Regression on the Test set. Note that you are welcome to submit your predicted labels to Kaggle but you will need to submit the final prediction output in the final project submission. Please label the file as \"LogRed_Prediction.csv\"."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-- `sigmoid(z)`: A function that takes in a Real Number input and returns an output value between 0 and 1."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def sigmoid(z):\n",
    "    result = 1/(1 + np.exp(-z))\n",
    "#     print(\"sigmoid:\", result)\n",
    "    return result"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-13T18:24:07.427847Z",
     "iopub.execute_input": "2022-07-13T18:24:07.428185Z",
     "iopub.status.idle": "2022-07-13T18:24:07.433949Z",
     "shell.execute_reply.started": "2022-07-13T18:24:07.428156Z",
     "shell.execute_reply": "2022-07-13T18:24:07.432594Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "-- `loss(y, y_hat)`: A loss function that allows us to minimize and determine the optimal parameters. The function takes in the actual labels y and the predicted labels yhat, and returns the overall training loss. Note that you should be using the Log Loss function taught in class."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# def loss(y, y_hat):\n",
    "#         result = -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "# #         print(\"loss:\", result)\n",
    "#         return result\n",
    "\n",
    "def loss(y, y_hat):\n",
    "    loss = np.where(y == 1, np.log(y_hat), np.log(1 - y_hat).mean()\n",
    "    print(f\"{loss = }\")\n",
    "    return loss"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-13T18:24:07.437372Z",
     "iopub.execute_input": "2022-07-13T18:24:07.437858Z",
     "iopub.status.idle": "2022-07-13T18:24:07.448533Z",
     "shell.execute_reply.started": "2022-07-13T18:24:07.437809Z",
     "shell.execute_reply": "2022-07-13T18:24:07.447044Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "-- `gradients(X, y, y_hat)`: The Gradient Descent Algorithm to find the optimal values of our parameters. The function takes in the training feature X, actual labels y and the predicted labels yhat, and returns the partial derivative of the Loss function with respect to weights (w) and bias (db)."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def gradients(X, y, y_hat):\n",
    "    # m - number of training examples\n",
    "    m = np.shape(X)[0]\n",
    "    dw = (1 / m) * np.dot(X.T, (y_hat - y))\n",
    "    db = (1 / m) * np.sum((y_hat - y))\n",
    "#     print(\"dw:\", dw, \"db:\", db)\n",
    "    return dw, db"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-13T18:24:07.450115Z",
     "iopub.execute_input": "2022-07-13T18:24:07.450519Z",
     "iopub.status.idle": "2022-07-13T18:24:07.462369Z",
     "shell.execute_reply.started": "2022-07-13T18:24:07.450485Z",
     "shell.execute_reply": "2022-07-13T18:24:07.461363Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "-- `train(X, y, bs, epochs, lr)`: The training function for your model."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# @param X - features\n",
    "# @param y - labels\n",
    "# @param bs - batch size\n",
    "# @param epochs - number of iterations through dataset\n",
    "# @param lr - learning rate\n",
    "\n",
    "def train(X, y, bs, epochs, lr):\n",
    "    # n - number of training examples, d - number of features\n",
    "    n, d = np.shape(X)\n",
    "    \n",
    "    randomize = np.arange(n)\n",
    "    rng = np.random.default_rng(100)\n",
    "    \n",
    "    w = np.zeros((d, 1))\n",
    "    b = 0\n",
    "    \n",
    "    y = y.reshape(n, 1)\n",
    "    \n",
    "    old_losses = []\n",
    "    old_w = []\n",
    "    old_b = []\n",
    "    \n",
    "    old_w.append(w.copy())\n",
    "    old_b.append(b)\n",
    "    l = loss(y, sigmoid(np.dot(X, w) + b))\n",
    "    old_losses.append(l)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        limit = n // bs\n",
    "#         print(\"limit:\", limit)\n",
    "        for i in range(limit):\n",
    "            start = i * bs\n",
    "            end = start + bs\n",
    "#             print(\"epoch:\", epoch, \"start:\", start, \"end:\", end)\n",
    "\n",
    "#             X_batch = X[start:end]\n",
    "#             y_batch = y[start:end]\n",
    "            \n",
    "            rng.shuffle(randomize)\n",
    "            choice = randomize[start:end]\n",
    "#             print(choice)\n",
    "            X_batch = X[choice]\n",
    "            y_batch = y[choice]\n",
    "#             print(X_batch, y_batch)\n",
    "            \n",
    "            y_hat = sigmoid(np.dot(X_batch, w) + b)\n",
    "            \n",
    "            dw, db = gradients(X_batch, y_batch, y_hat)\n",
    "            \n",
    "            loss_old = loss(y, sigmoid(np.dot(X, w) + b))\n",
    "            \n",
    "            w_new = w.copy() - lr * dw\n",
    "            b_new = b - lr * db\n",
    "            loss_new = loss(y, sigmoid(np.dot(X, w_new) + b_new))\n",
    "            \n",
    "#             w = w_new\n",
    "#             b = b_new\n",
    "#             old_w.append(w)\n",
    "#             old_b.append(b)\n",
    "#             old_losses.append(loss_new)\n",
    "            \n",
    "            if (loss_new < loss_old):\n",
    "#                 print(loss_new, loss_old)\n",
    "                w = w_new\n",
    "                b = b_new\n",
    "                old_w.append(w)\n",
    "                old_b.append(b)\n",
    "                old_losses.append(loss_new)\n",
    "    \n",
    "#     print(\"old_w:\", old_w)\n",
    "#     print(\"old_b:\", old_b)\n",
    "#     print(\"old_losses:\", old_losses)\n",
    "\n",
    "    min_loss = min(old_losses)\n",
    "#     print(\"min_loss:\", min_loss)\n",
    "    \n",
    "    min_index = old_losses.index(min_loss)\n",
    "#     print(\"min_index:\", min_index)\n",
    "    \n",
    "    return old_w[min_index], old_b[min_index], old_losses\n",
    "            "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-13T18:24:07.464822Z",
     "iopub.execute_input": "2022-07-13T18:24:07.465240Z",
     "iopub.status.idle": "2022-07-13T18:24:07.481820Z",
     "shell.execute_reply.started": "2022-07-13T18:24:07.465193Z",
     "shell.execute_reply": "2022-07-13T18:24:07.480530Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "-- `predict(X)`: The prediction function where you can apply your validation and test sets."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def predict(X, w, b):\n",
    "    y_pred = sigmoid(np.dot(X, w) + b)\n",
    "    pred_labels = np.array([1 if i >= 0.5 else 0 for i in y_pred])\n",
    "    return pred_labels"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-13T18:24:07.483506Z",
     "iopub.execute_input": "2022-07-13T18:24:07.483955Z",
     "iopub.status.idle": "2022-07-13T18:24:07.503042Z",
     "shell.execute_reply.started": "2022-07-13T18:24:07.483904Z",
     "shell.execute_reply": "2022-07-13T18:24:07.501644Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performance Evaluation\n",
    "\n",
    "As per the grading rubric - \"Perfect Implementation of the Logistics Regression algorithm. Successfully trained the implemented model with the train set and achieved comparative performance compared to SKLearn Logistic Regression package\", we shall compare the performance of our model with the SKLearn Logistic Regression package.\n",
    "\n",
    "We shall first implement a function to evaluate the accuracy of our model."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def accuracy(y, y_hat):\n",
    "    accuracy = np.sum(y == y_hat) / np.shape(y)[0]\n",
    "    return accuracy"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-13T18:24:07.504961Z",
     "iopub.execute_input": "2022-07-13T18:24:07.505821Z",
     "iopub.status.idle": "2022-07-13T18:24:07.518061Z",
     "shell.execute_reply.started": "2022-07-13T18:24:07.505780Z",
     "shell.execute_reply": "2022-07-13T18:24:07.516671Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Due to the large number of features, we shall evaluate the model performance and compare it with the SKLearn Logistic Regression using only the first 100 columns."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X = df_train.iloc[:, 2:102].to_numpy()\n",
    "y = df_train.iloc[:,1].to_numpy()\n",
    "print(\"X:\", X.shape, \"y:\", y.shape)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-13T18:24:07.519821Z",
     "iopub.execute_input": "2022-07-13T18:24:07.520578Z",
     "iopub.status.idle": "2022-07-13T18:24:07.542547Z",
     "shell.execute_reply.started": "2022-07-13T18:24:07.520530Z",
     "shell.execute_reply": "2022-07-13T18:24:07.541038Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "w, b, l = train(X, y, bs = 100, epochs = 20, lr = 0.01)\n",
    "accuracy(y, predict(X, w, b))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-13T18:24:07.546999Z",
     "iopub.execute_input": "2022-07-13T18:24:07.548369Z",
     "iopub.status.idle": "2022-07-13T18:24:24.929699Z",
     "shell.execute_reply.started": "2022-07-13T18:24:07.548306Z",
     "shell.execute_reply": "2022-07-13T18:24:24.928425Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state = 100).fit(X, y)\n",
    "clf.predict(X)\n",
    "clf.score(X, y)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-13T18:24:24.931316Z",
     "iopub.execute_input": "2022-07-13T18:24:24.932560Z",
     "iopub.status.idle": "2022-07-13T18:24:25.073400Z",
     "shell.execute_reply.started": "2022-07-13T18:24:24.932508Z",
     "shell.execute_reply": "2022-07-13T18:24:25.072079Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=10000, noise=0.24)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-13T18:28:37.018317Z",
     "iopub.execute_input": "2022-07-13T18:28:37.018718Z",
     "iopub.status.idle": "2022-07-13T18:28:37.027201Z",
     "shell.execute_reply.started": "2022-07-13T18:28:37.018686Z",
     "shell.execute_reply": "2022-07-13T18:28:37.026212Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "w, b, l = train(X, y, bs = 100, epochs = 20, lr = 0.01)\n",
    "accuracy(y, predict(X, w, b))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-13T18:28:38.572486Z",
     "iopub.execute_input": "2022-07-13T18:28:38.573195Z",
     "iopub.status.idle": "2022-07-13T18:28:43.099844Z",
     "shell.execute_reply.started": "2022-07-13T18:28:38.573157Z",
     "shell.execute_reply": "2022-07-13T18:28:43.098452Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "clf = LogisticRegression(random_state = 100).fit(X, y)\n",
    "clf.predict(X)\n",
    "clf.score(X, y)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-13T18:28:46.037402Z",
     "iopub.execute_input": "2022-07-13T18:28:46.037854Z",
     "iopub.status.idle": "2022-07-13T18:28:46.076463Z",
     "shell.execute_reply.started": "2022-07-13T18:28:46.037816Z",
     "shell.execute_reply": "2022-07-13T18:28:46.075021Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the above score, we can deem that the performance of our Logistic Regression Model is comparable to that of SKLearn Logistic Regression Package."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exporting Prediction\n",
    "Prediction made by your Logistic Regression on the Test set. Note that you are welcome to submit your predicted labels to Kaggle but you will need to submit the final prediction output in the final project submission. Please label the file as \"LogRed_Prediction.csv\"."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}